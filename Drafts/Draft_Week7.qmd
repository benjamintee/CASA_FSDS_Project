---
date: last-modified
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Is Airbnb ‘out of control’ in London?

1.  How many Airbnb listings are there?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

2.  How does that compare with the total housing stock?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

3.  What is the overall occupancy rate of Airbnbs?

4.  How have these trends changed over time?

    -   in total

    -   in each MSOA / LSOA

### **1.1 Load Airbnb listings data and view structure of the dataset**

```{python}
import pandas as pd

# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'

# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
```

```{python}
# View data structure, column names and types 
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")

# View basic information about the dataset 
df.info(verbose = True)

# Generate a list of column names to identify variables of interest 
print(df.columns.to_list())
```

```{python}
# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_about', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 
    'license', 'reviews_per_month']
print(f"Cols contains {len(cols)} columns.")
```

```{python}
# Downloading the full data with columns of interest 
df = pd.read_csv(url, compression='gzip', low_memory=False, usecols=cols)
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")

df.info(verbose = True)
```

### **1.2 Basic cleaning of dataset**

First, we attempt to identify and remove rows with NA and NaN values. Fortunately, all the rows had a specific id and there were no NAs in the id.

```{python}
# Removing rows with NA in id 
df.drop(df[df.id.isna()].index.array, axis=0, inplace=True)
print(f"Data frame contains {df.shape[0]:,} rows.")

# Counting the NA data by columns
df.isnull().sum(axis=0).sort_values(ascending=False).head(10)

# Drop the two columns with the most number of NA data (license, host_about) 
df.drop(columns=['license','host_about'], inplace=True)

# Next, we look out for rows which may have multiple NA values
df.isnull().sum(axis=1).sort_values(ascending=False).head(10)

# Looking at the probability distribution of the rows with multiple NA values 
probs = df.isnull().sum(axis=1)
print(type(probs))       
probs.plot.hist(bins=30) 

# Dropping rows missing more than 8 values which may be problematic 
cutoff = 5 # Set cutoff for 5 
problematic_rows = df.loc[probs > cutoff]
df.drop(probs[probs > cutoff].index, inplace=True)
print(f"df now contains {df.shape[0]:,} rows, and we dropped {problematic_rows.shape[0]:,} rows")
```

Next, we want to assign the correct variable types to each column for efficiency in processing.

```{python}
# Fixing the host_is_superhost column and converting it to boolean
bools = ['host_is_superhost']
df.sample(5, random_state=43)[bools]

for b in bools:
    print(f"Converting {b}")
    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')

# Fixing the columns with dates in them, and converting it to dates 
dates = ['last_scraped','host_since','first_review','last_review']

print(f"Currently {dates[1]} is of type '{df[dates[1]].dtype}'", "\n")
df.sample(5, random_state=43)[dates]

for d in dates:
    print("Converting " + d)
    df[d] = pd.to_datetime(df[d])

# Fixing the columns that are supposed to be categories, convert to category 
cats = ['property_type','room_type']
for c in cats:
    print(f"Converting {c}")
    df[c] = df[c].astype('category')

# Fixing the column with price where there are odd symbols like $ and , 
money = ['price']
for m in money:
    print(f"Converting {m}")
    df[m] = df[m].str.replace('$','', regex=False).str.replace(',', '', regex = False).astype('float')

# Fixing integers 
ints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',
         'beds','minimum_nights','maximum_nights','availability_365']
for i in ints:
    print(f"Converting {i}")
    try:
        df[i] = df[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())
```

Finally, we validate the data to check and understand if the various columns are in order.

```{python}
df.info()
```

And we can export the cleaned and formatted file to folder

```{python}
from pathlib import Path
path = Path(f'Data/clean/{Path(url).name}') 
print(f"Writing to: {path}")

if not path.parent.exists(): 
    print(f"Creating {path.parent}")
    path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():  
    df.to_csv(path, index=False)
    print("Done.")
```

### **1.3 Quick data exploration** 

Answering some quick questions. How many unique listings were on Airbnb? How many unique hosts were there?

```{python}
# Counting the number of unique IDs
unique_ids = df['id'].nunique()
print(f"There were {unique_ids:,.0f} unique IDs in the listings.")

# Counting the numnber of unique hosts 
unique_hosts = df['host_id'].nunique()
print(f"There were {unique_hosts:,.0f} unique hosts.")

# Different property types and room types
unique_ptypes = df['property_type'].unique()
print(unique_ptypes)

unique_rtypes = df['room_type'].unique()
print(unique_rtypes)
```

{{< pagebreak >}}

# Briefing

{{< include _questions.qmd >}}

## References