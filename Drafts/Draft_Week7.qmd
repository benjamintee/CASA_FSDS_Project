---
date: last-modified
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Is Airbnb ‘out of control’ in London?

1.  How many Airbnb listings are there?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

2.  How does that compare with the total housing stock?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

3.  What is the overall occupancy rate of Airbnbs?

4.  How have these trends changed over time?

    -   in total

    -   in each MSOA / LSOA

### **1.1 Load Airbnb listings data and view structure**

```{python}
import pandas as pd

# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'

# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
```

```{python}
# View data structure, column names and types 
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")

# View basic information about the dataset 
df.info(verbose = True)

# Generate a list of column names to identify variables of interest 
print(df.columns.to_list())
```

```{python}
# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_about', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 
    'license', 'reviews_per_month']
print(f"Cols contains {len(cols)} columns.")
```

```{python}
# Downloading the full data with columns of interest 
df = pd.read_csv(url, compression='gzip', low_memory=False, usecols=cols)
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")
df.info(verbose = True)
```

### **1.2 Basic cleaning of listings dataset**

First, we attempt to identify and remove rows with NA and NaN values. Fortunately, all the rows had a specific id and there were no NAs in the id.

```{python}
# Removing rows with NA in id 
df.drop(df[df.id.isna()].index.array, axis=0, inplace=True)
print(f"Data frame contains {df.shape[0]:,} rows.")

# Counting the NA data by columns
df.isnull().sum(axis=0).sort_values(ascending=False).head(10)

# Drop the two columns with the most number of NA data (license, host_about) 
df.drop(columns=['license','host_about'], inplace=True)

# Next, we look out for rows which may have multiple NA values
df.isnull().sum(axis=1).sort_values(ascending=False).head(10)

# Looking at the probability distribution of the rows with multiple NA values 
probs = df.isnull().sum(axis=1)
print(type(probs))       
probs.plot.hist(bins=30) 

# Dropping rows missing more than 8 values which may be problematic 
cutoff = 5 # Set cutoff for 5 
problematic_rows = df.loc[probs > cutoff]
df.drop(probs[probs > cutoff].index, inplace=True)
print(f"df now contains {df.shape[0]:,} rows, and we dropped {problematic_rows.shape[0]:,} rows")
```

Next, we want to assign the correct variable types to each column for efficiency in processing.

```{python}
# Fixing the host_is_superhost column and converting it to boolean
bools = ['host_is_superhost']
df.sample(5, random_state=43)[bools]

for b in bools:
    print(f"Converting {b}")
    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')

# Fixing the columns with dates in them, and converting it to dates 
dates = ['last_scraped','host_since','first_review','last_review']

print(f"Currently {dates[1]} is of type '{df[dates[1]].dtype}'", "\n")
df.sample(5, random_state=43)[dates]

for d in dates:
    print("Converting " + d)
    df[d] = pd.to_datetime(df[d])

# Fixing the columns that are supposed to be categories, convert to category 
cats = ['property_type','room_type']
for c in cats:
    print(f"Converting {c}")
    df[c] = df[c].astype('category')

# Fixing the column with price where there are odd symbols like $ and , 
money = ['price']
for m in money:
    print(f"Converting {m}")
    df[m] = df[m].str.replace('$','', regex=False).str.replace(',', '', regex = False).astype('float')

# Fixing integers 
ints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',
         'beds','minimum_nights','maximum_nights','availability_365']
for i in ints:
    print(f"Converting {i}")
    try:
        df[i] = df[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())
```

Finally, we validate the data to check and understand if the various columns are in order.

```{python}
df.info()
```

Export the cleaned and formatted file to folder

```{python}
from pathlib import Path
path = Path(f'Data/clean/{Path(url).name}') 
print(f"Writing to: {path}")

if not path.parent.exists(): 
    print(f"Creating {path.parent}")
    path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():  
    df.to_csv(path, index=False)
    print("Done.")
```

### **1.3 Load and clean data on Housing Stock by MSOA and Borough**

We are interested in the number of domestic properties in London and how this has changed over time. We draw from the UK Valuation Office (VOA) which provides statistics on the stock of domestic properties by Council Tax Band. Thus far, this approach provides the most comprehensive estimate of the stock of local properties in London. We then filter for MSOAs within Greater London. 

```{python}
# Set the parameters to retrieve the downloaded data from LSOA 
host = 'https://raw.githubusercontent.com/benjamintee/'
years = [2020, 2021, 2022, 2023, 2024]

df_stock = pd.concat(
    [pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
     for y in years],
    ignore_index=True)

df_stock.info()

# Filter for LA data and ecodes that fall within Greater London 
df_stock_LA = df_stock[
    (df_stock['geography'] == 'LAUA') &
    (df_stock['ecode'].str.startswith('E090'))
]

# Filter and select MSOAs within Greater London 
"""
This block first extracts names of london boroughs, then filters the df_stock to select only the MSOAs with name strings that contain any of the london borough names, hence forming a dataframe of property stock by MSOA in Greater London only
"""

#list of london boroughs
london_boroughs = df_stock_LA.area_name.unique().tolist()

# Build regex pattern — escape special chars and join with |
pattern = "|".join([rf"\b{b.replace(' ', r'\s+')}\b" for b in london_boroughs])

# Filter for MSOA data and ecodes that fall within Greater London 
df_stock_MSOA = df_stock[
    (df_stock['geography'] == 'MSOA') &
    (df_stock['area_name'].str.contains(pattern, case=False, na=False, regex=True))
]

```

### **1.3 Quick exploration of listings dataset**

Answering some quick questions. How many unique listings were on Airbnb? How many unique hosts were there?

```{python}
# Counting the number of unique IDs
unique_ids = df['id'].nunique()
print(f"There were {unique_ids:,.0f} unique IDs in the listings.")

# Counting the numnber of unique hosts 
unique_hosts = df['host_id'].nunique()
print(f"There were {unique_hosts:,.0f} unique hosts.")

# Different property types and room types
unique_ptypes = df['property_type'].unique()
print(unique_ptypes)

unique_rtypes = df['room_type'].unique()
print(unique_rtypes)
```

### 1.4 Spatial Analysis of 2025 listings data 

Let's take a look at the spatial distribution of Airbnb listings across London. Where are Airbnb's located and how do the numbers differ by Boroughs and MSOAs?  

```{python}
import geopandas as gpd

# Load MSOA 2021 geopackage and BORO files from GitHub
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/'

BORO = gpd.read_file(f'{host}/{ddir}/LONDON_BOROUGH.gpkg').to_crs(epsg=27700)
MSOA = gpd.read_file(f'{host}/{ddir}/MSOA_DEC_2021_BOUNDARIES.gpkg').to_crs(epsg=27700)

# Clip MSOA Boundaries to those within London Broughs and only retain Polygons and MultiPolygons
LONDON = gpd.GeoDataFrame(geometry=[BORO.dissolve().geometry.iloc[0]], crs= '27700')
MSOA = gpd.clip(MSOA, LONDON)
MSOA = MSOA[MSOA.geometry.type.isin(["Polygon", "MultiPolygon"])]

# Undertake an inner spatial join to retain the MSOA that fall within Greater London Boundaries 
MSOA = gpd.sjoin(MSOA, BORO[['name', 'ons_inner', 'sub_2011', 'geometry']], how='inner',predicate='intersects')

# Retain columns of interest and rename for better readability 
MSOA = MSOA[['MSOA21CD', 'MSOA21NM', 'geometry', 'name', 'ons_inner', 'sub_2011']].rename(
    columns={
        'MSOA21CD': 'ecode', 
        'MSOA21NM': 'MSOA_name', 
        'name' : 'BORO_name'}).dissolve(by='ecode' , as_index=False)

# Converting listing pandas dataframe to a geopandas dataframe with CRS 4326, and then convert to CRS 27700 
gdf = gpd.GeoDataFrame(df,
      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326')).to_crs(epsg=27700)
```

Let's plot a chart! 

```{python}
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

fig, ax = plt.subplots(1, 1, figsize=(10, 8))

MSOA.plot(ax=ax, facecolor="none", edgecolor="lightgray", linewidth=0.7)
BORO.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=1.3)
gdf.plot(ax=ax, color="dodgerblue", alpha=0.7, markersize=0.1)

# Custom legend handle for Airbnb points
legend_elements = [
    Line2D([0], [0],
           marker='o',
           color='w',
           markerfacecolor='dodgerblue',
           markersize=8,
           linestyle='',
           label="Airbnb Listing (2025)")
]

ax.legend(handles=legend_elements,
          loc='upper right',
          frameon= True)

ax.set_title("Spatial Distribution of Airbnb Listings in London (2025)", fontsize=16, pad=15)
fig.text(0.5, 0.02, "Source: InsideAirbnb, data.gov.uk", ha='center', fontsize=12)
ax.axis("off")

plt.tight_layout()
plt.show()
```

How about the density of the listings relative to the total housing stock? For this we need to bring out the housing stock data. Given that the latest stock data relates to 2024, we shall use the 2024 data by MSOA. 

```{python}
import numpy as np

# Filter the total stock data for 2024
df_stock_MSOA2025 = df_stock_MSOA[df_stock_MSOA['Year'] == 2024]

# Merge in the data on the count of all_properties to MSOA by ecode
MSOA = MSOA.merge(df_stock_MSOA2025[['ecode', 'all_properties']], on='ecode', how='left')
MSOA = MSOA.rename(columns={'all_properties': 'allprop2024'})

# Count Airbnb listings inside each MSOA polygon
gdf_MSOA = gpd.sjoin(gdf, MSOA[['ecode', 'geometry']], how='left', predicate='within')
MSOA = MSOA.merge(gdf_MSOA.groupby('ecode').size().rename('listings2025'), on='ecode', how='left')
MSOA['listings2025'] = MSOA['listings2025'].fillna(0).astype(int)

# Count listings as a proportion of total housing stock
MSOA['listing_prop2025'] = (MSOA['listings2025'] / MSOA['allprop2024']) * 100
MSOA['listing_prop2025'] = MSOA['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)
```

```{python}
# Plot a histogram of the distribution of listing proportions by MSOA  
plt.figure(figsize=(8,6))
plt.hist(MSOA['listing_prop2025'], bins=40,  edgecolor='black', alpha=0.8)

plt.title("Distribution of Airbnb Listings as a Proportion of Housing Stock")
plt.xlabel("Listings as percentage of Housing Stock (%)")
plt.ylabel("Number of MSOAs")
plt.tight_layout()
plt.show()
```

```{python} 
fig, ax = plt.subplots(1, 1, figsize=(10, 8))

MSOA.plot(
    ax=ax,
    column='listing_prop2025',
    cmap='viridis',
    edgecolor="gray",
    linewidth=0.35,
    legend=True)

BORO.boundary.plot(ax=ax, edgecolor='black', linewidth= 1.0)

ax.set_title("Airbnb Listings as a Proportion of Housing Stock (2025)", fontsize=18, pad=15)
ax.axis("off")

cbar_ax = fig.axes[-1]           
cbar_ax.tick_params(labelsize=16)
cbar_ax.set_ylabel(
    "Listings as Percentage of Housing Stock",
    fontsize=16,
    rotation=90,   
    labelpad=15)

fig.text(0.03, 0.02, "Source: Listings data obtained from InsideAirbnb, \nHousing Stock retreived from Valuation Office Agency (VOA), \nAdministrative boundaries from data.gov.uk", ha='left', fontsize=12a)

plt.tight_layout()
plt.show()
```


### 1.5 Load Airbnb reviews data and view structure

```{python}
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-reviews.csv.gz'

# Download first 1,000 rows for EDA
df_reviews = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
```

```{python}
# View data structure, column names and types 
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")

# View basic information about the dataset 
df_reviews.info(verbose = True)

# Downloading the full data 
df_reviews = pd.read_csv(url, compression='gzip', low_memory=False)
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")
```

### 1.6 Cleaning of reviews data

```{python}
# Removing rows with NA in listing id, id, and reviewer id 
df_reviews.drop(df_reviews[df_reviews.id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.listing_id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.reviewer_id.isna()].index.array, axis=0, inplace=True)

print(f"Data frame contains {df_reviews.shape[0]:,} rows.")

# Counting the NA data by columns
df_reviews.isnull().sum(axis=0).sort_values(ascending=False).head(10)
```

```{python}
# Fixing the columns with dates in them, and converting it to dates 
df_reviews['date'] = pd.to_datetime(df_reviews['date'])

# Fixing integers 
ints  = ['id', 'listing_id', 'reviewer_id']
for i in ints:
    print(f"Converting {i}")
    try:
        df_reviews[i] = df_reviews[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df_reviews[i] = df_reviews[i].astype('float').astype(pd.UInt16Dtype())

# The comments column is excessively large and is beyond the scope of the project, so we are dropping it 
df_reviews.drop(columns=['comments'], inplace=True)
```

And, we can export the clean data for further analysis

```{python}
path = Path(f'Data/clean/{Path(url).name}') 
print(f"Writing to: {path}")

if not path.parent.exists(): 
    print(f"Creating {path.parent}")
    path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():  
    df_reviews.to_csv(path, index=False)
    print("Done.")
```

It appears that the reviews data is cumulative based on the listing_id.

```{python}
#delete df_stock to save memory
del(df_stock)
```

{{< pagebreak >}}

# Briefing

{{< include _questions.qmd >}}

## References