---
date: last-modified
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Is Airbnb ‘out of control’ in London?

1.  How many Airbnb listings are there?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

2.  How does that compare with the total housing stock?

    -   in total

    -   in each MSOA / LSOA

    -   profile of the listings (full flat / room)

3.  What is the overall occupancy rate of Airbnbs?

4.  How have these trends changed over time?

    -   in total

    -   in each MSOA / LSOA

### **1.1 Load Airbnb listings data and view structure** 

```{python}
import pandas as pd

# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'

# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
```

```{python}
# View data structure, column names and types 
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")

# View basic information about the dataset 
df.info(verbose = True)

# Generate a list of column names to identify variables of interest 
print(df.columns.to_list())
```

```{python}
# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_about', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 
    'license', 'reviews_per_month']
print(f"Cols contains {len(cols)} columns.")
```

```{python}
# Downloading the full data with columns of interest 
df = pd.read_csv(url, compression='gzip', low_memory=False, usecols=cols)
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")
df.info(verbose = True)
```

### **1.2 Basic cleaning of listings dataset**

First, we attempt to identify and remove rows with NA and NaN values. Fortunately, all the rows had a specific id and there were no NAs in the id.

```{python}
# Removing rows with NA in id 
df.drop(df[df.id.isna()].index.array, axis=0, inplace=True)
print(f"Data frame contains {df.shape[0]:,} rows.")

# Counting the NA data by columns
df.isnull().sum(axis=0).sort_values(ascending=False).head(10)

# Drop the two columns with the most number of NA data (license, host_about) 
df.drop(columns=['license','host_about'], inplace=True)

# Next, we look out for rows which may have multiple NA values
df.isnull().sum(axis=1).sort_values(ascending=False).head(10)

# Looking at the probability distribution of the rows with multiple NA values 
probs = df.isnull().sum(axis=1)
print(type(probs))       
probs.plot.hist(bins=30) 

# Dropping rows missing more than 8 values which may be problematic 
cutoff = 5 # Set cutoff for 5 
problematic_rows = df.loc[probs > cutoff]
df.drop(probs[probs > cutoff].index, inplace=True)
print(f"df now contains {df.shape[0]:,} rows, and we dropped {problematic_rows.shape[0]:,} rows")
```

Next, we want to assign the correct variable types to each column for efficiency in processing.

```{python}
# Fixing the host_is_superhost column and converting it to boolean
bools = ['host_is_superhost']
df.sample(5, random_state=43)[bools]

for b in bools:
    print(f"Converting {b}")
    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')

# Fixing the columns with dates in them, and converting it to dates 
dates = ['last_scraped','host_since','first_review','last_review']

print(f"Currently {dates[1]} is of type '{df[dates[1]].dtype}'", "\n")
df.sample(5, random_state=43)[dates]

for d in dates:
    print("Converting " + d)
    df[d] = pd.to_datetime(df[d])

# Fixing the columns that are supposed to be categories, convert to category 
cats = ['property_type','room_type']
for c in cats:
    print(f"Converting {c}")
    df[c] = df[c].astype('category')

# Fixing the column with price where there are odd symbols like $ and , 
money = ['price']
for m in money:
    print(f"Converting {m}")
    df[m] = df[m].str.replace('$','', regex=False).str.replace(',', '', regex = False).astype('float')

# Fixing integers 
ints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',
         'beds','minimum_nights','maximum_nights','availability_365']
for i in ints:
    print(f"Converting {i}")
    try:
        df[i] = df[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())
```

Finally, we validate the data to check and understand if the various columns are in order.

```{python}
df.info()
```

And we can export the cleaned and formatted file to folder

```{python}
from pathlib import Path
path = Path(f'Data/clean/{Path(url).name}') 
print(f"Writing to: {path}")

if not path.parent.exists(): 
    print(f"Creating {path.parent}")
    path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():  
    df.to_csv(path, index=False)
    print("Done.")
```

### **1.3 Quick exploration of listings dataset**

Answering some quick questions. How many unique listings were on Airbnb? How many unique hosts were there?

```{python}
# Counting the number of unique IDs
unique_ids = df['id'].nunique()
print(f"There were {unique_ids:,.0f} unique IDs in the listings.")

# Counting the numnber of unique hosts 
unique_hosts = df['host_id'].nunique()
print(f"There were {unique_hosts:,.0f} unique hosts.")

# Different property types and room types
unique_ptypes = df['property_type'].unique()
print(unique_ptypes)

unique_rtypes = df['room_type'].unique()
print(unique_rtypes)
```

### 1.4 Load Airbnb reviews data and view structure

```{python}
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-reviews.csv.gz'

# Download first 1,000 rows for EDA
df_reviews = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
```

```{python}
# View data structure, column names and types 
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")

# View basic information about the dataset 
df_reviews.info(verbose = True)

# Downloading the full data 
df_reviews = pd.read_csv(url, compression='gzip', low_memory=False)
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")
```

### 1.5 Cleaning of reviews data

```{python}
# Removing rows with NA in listing id, id, and reviewer id 
df_reviews.drop(df_reviews[df_reviews.id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.listing_id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.reviewer_id.isna()].index.array, axis=0, inplace=True)

print(f"Data frame contains {df_reviews.shape[0]:,} rows.")

# Counting the NA data by columns
df_reviews.isnull().sum(axis=0).sort_values(ascending=False).head(10)
```

```{python}
# Fixing the columns with dates in them, and converting it to dates 
df_reviews['date'] = pd.to_datetime(df_reviews['date'])

# Fixing integers 
ints  = ['id', 'listing_id', 'reviewer_id']
for i in ints:
    print(f"Converting {i}")
    try:
        df_reviews[i] = df_reviews[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        df_reviews[i] = df_reviews[i].astype('float').astype(pd.UInt16Dtype())

# The comments column is excessively large and is beyond the scope of the project, so we are dropping it 
df_reviews.drop(columns=['comments'], inplace=True)
```

And, we can export the clean data for further analysis

```{python}
path = Path(f'Data/clean/{Path(url).name}') 
print(f"Writing to: {path}")

if not path.parent.exists(): 
    print(f"Creating {path.parent}")
    path.parent.mkdir(parents=True, exist_ok=True)

if not path.exists():  
    df_reviews.to_csv(path, index=False)
    print("Done.")
```

It appears that the reviews data is cumulative based on the listing_id.

### 1.6 Load data from on the stock of domestic properties 

We are interested in the number of domestic properties in London and how this has changed over time. We draw from the UK Valuation Office (VOA) which provides statistics on the stock of domestic properties by Council Tax Band. Thus far, this approach provides the most comprehensive estimate of the stock of local properties in London. The data is split across 8 bands as follows:

| **Band** | **Value of property (at 1 April 1991)** |
|:---------|:----------------------------------------|
| A        | Up to £40,000                           |
| B        | £40,001 up to £52,000                   |
| C        | £52,001 up to £68,000                   |
| D        | £68,001 up to £88,000                   |
| E        | £88,001 up to £120,000                  |
| F        | £120,001 up to £160,000                 |
| G        | £160,001 up to £320,000                 |
| H        | £320,001 and above                      |

Specifically, we are interested in data at MSOA level and for areas within London, and reference Table CTSOP1.1 from 2020-2024.

```{python}
# Set the parameters to retrieve the downloaded data
host = 'https://raw.githubusercontent.com/benjamintee/'
years = ['2020', '2021', '2022', '2023', '2024']

df_stock = pd.concat(
    [pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
     for y in years],
    ignore_index=True)

df_stock.info()
```

```{python}
# Filter for MSOA data and ecodes that fall within London 
df_stock = df_stock[
    (df_stock['geography'] == 'LAUA') &
    (df_stock['ecode'].str.startswith('E090'))
]
df_stock = df_stock.reset_index(drop=True)
```

{{< pagebreak >}}

# Briefing

{{< include _questions.qmd >}}

## References