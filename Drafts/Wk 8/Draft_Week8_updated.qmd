---
date: last-modified
title: The Pythoneers' Group Project
execute:
  echo: false
  freeze: true
  eval: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

##  {.unnumbered .unlisted}

## Declaration of Authorship {.unnumbered .unlisted}

We, The Pythoneers, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 16 December 2025 (Tuesday)

| Names          | Student Numbers | Contact Email                |
|----------------|-----------------|------------------------------|
| Alice Southall | 25120600        | alice.southall.25\@ucl.ac.uk |
| Benjamin Tee   | 25049107        | benjamin.tee.25\@ucl.ac.uk   |
| Bosco Choi     | 22040212        | bosco.choi.22\@ucl.ac.uk     |
| Owen Hughes    | 25197128        | owen.hughes.25\@ucl.ac.uk    |
| Tong, C.Y      | 25244321        | c.tong.25\@ucl.ac.uk         |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

# Briefing on Airbnb Growth Trends & Policy Implications

## **Executive Summary**

This report provides an analysis of Airbnb activity in London in recent years (2021-2025), with a focus on 2025. Given time-sensitivity amidst the impending election, and a data-poor environment, we utilise appropriate data-driven approaches to provide insight into the following questions:

a.      Is Airbnb out of control in London?

b.      How many professional landlords are there?

c.      How many properties would be affected by the opposition’s proposal?

d.      What are the benefits and trade-offs with the opposition’s proposal?

e.      What are options to reframe the narrative to one about social mobility and housing opportunity?

### Key Findings

*(Insert findings in 2 column table)*

### Recommendations

*(Insert recommendations here)*

{{< pagebreak >}}

## **1. Is Airbnb ‘out of control’ in London?**

Opposition critics have suggested that the growth and spread of Airbnbs in London has accelerated, with professional landlords rapidly accumulating properties, flouting rental restrictions to make a profit. The increase of short-term lets and its potential to damage the housing supply in London is a concern that the Mayor's office had raised [previously](https://www.london.gov.uk/mayor-demands-licencing-scheme-prevent-short-term-lets-damaging-housing-supply), and is one of the reasons that London's 90-day limit was introduced. Indeed, the Mayor called for a registration system to be put in place in [2019](www.london.gov.uk/press-releases/mayoral/registration-system-for-short-term-letting-law). This chapter provides a systematic analysis of the situation. 

```{python}
#| echo: false
#| warning: false
# Load common libraries and packages
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.lines import Line2D
```

#### 1.1 What is the growth trend in Airbnb listings across London?

```{python}
#| echo: false
#| warning: false
#| output: false
### Download Airbnb listings data. Looking at the file structure in orca, 2024/2025 have one structure, 2022/2023 have one structure and 2021 has another structure. The following codes specify the urls. 
host = 'https://orca.casa.ucl.ac.uk'
city = 'London'

# Set up config file with specific patterns for each download year
config = {
    "2025": {"date": "20250615",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2024": {"date": "20240614",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2023": {"date": "2023-09-06", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2022": {"date": "2022-09-10", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2021": {"date": "2021-10",    "pattern": f"{host}/~jreades/data/{city}-{{date}}-listings.csv.gz"}
}

# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'estimated_occupancy_l365d', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 'reviews_per_month']

# Set up empty dictionary dfs 
dfs = {}

# Set up a loop to read in the data into the dictionary sequentially
for year, info in config.items():
    date    = info["date"]
    pattern = info["pattern"]
    url     = pattern.format(date=date)

    print("Loading:", year, url)

    # Read only the header (first row)
    header_cols = pd.read_csv(url, nrows=0).columns.tolist()

    # Keep only columns that actually exist in the file (Fix for 'estimated_occupancy_l365d' only in 2025)
    valid_cols = [c for c in cols if c in header_cols]
    
    dfs[year] = pd.read_csv(url, compression='gzip', low_memory=False, usecols= valid_cols)

    # Print shape of the dataframe just loaded
    print(f" → df_{year}: {dfs[year].shape[0]:,} rows × {dfs[year].shape[1]} columns\n")

print("Done!")
```

```{python}
#| echo: false
#| warning: false
#| output: false
### Cleaning the Airbnb listings data (2021-2025) 
# Evaluate the number of rows with NA in id and other columns 
na_summary = {}
for year, df in dfs.items():
    na_summary[year] = df.isna().sum()   

na_table = pd.DataFrame(na_summary).T  
na_table 

# Sensitivity Analysis: Count the number of rows with different thresholds of NA 
row_na_summary = {}
for year, df in dfs.items():

    na_per_row = df.isna().sum(axis=1)

    row_na_summary[year] = {
        "n1":  (na_per_row >= 1).sum(),
        "n2+": (na_per_row >= 2).sum(),
        "n3+": (na_per_row >= 3).sum(),
        "n4+": (na_per_row >= 4).sum(),
        "n5+": (na_per_row >= 5).sum(),
        "n6+": (na_per_row >= 6).sum(),
        "n7+": (na_per_row >= 7).sum(),
        "n8+": (na_per_row >= 8).sum(),
    }

# Convert to a DataFrame
na_summary = pd.DataFrame(row_na_summary).T
na_summary

# The na_summary shows that rows with 7 or more NAs comprise about 10,000 entries in 2024 and 2025. These are typically rows that do not have any reviews or do not have a price and data on bathrooms and bedrooms.  
```

```{python}
#| echo: false
#| warning: false
#| output: false
### Cleaning the data to remove NAs appropriately 
for year, df in dfs.items():

    # Removing rows with NA in id
    before = df.shape[0]
    df.drop(df[df['id'].isna()].index, inplace=True)

    # Dropping rows missing more than 6 values, which may be problematic - these rows tend to be those without reviews (4 columns) and three additional columns (e.g. bedrooms, bathrooms, price)
    cutoff = 6
    probs = df.isnull().sum(axis=1)
    problematic_rows = df.loc[probs > cutoff]
    df.drop(problematic_rows.index, inplace=True)

    # Print summary for reference
    after = df.shape[0]
    dropped = before - after

    print(f" → df_{year} now contains {after:,} rows. "
          f"(Dropped {dropped:,} rows due to missing data)")

    print("Done!")
```

```{python}
#| echo: false
#| warning: false
#| output: false
## We want to assign the correct variable types to each column for efficiency in processing 
# Columns to convert
bools = ['host_is_superhost']
dates = ['last_scraped','host_since','first_review','last_review']
cats  = ['property_type','room_type']
money = ['price']
ints  = ['id','host_id','host_listings_count','host_total_listings_count',
         'accommodates','beds','minimum_nights','maximum_nights',
         'availability_365', 'estimated_occupancy_l365d']

# Loop through the dataframes in dfs for each year and convert columns to specific formats 
for year, df in dfs.items():

    # Boolean columns
    for b in bools:
        if b in df.columns:
            print(f"  Converting {b} → bool")
            df[b] = df[b].replace({'f': False, 't': True}).infer_objects(copy=False).astype('bool')

    # Date columns
    for d in dates:
        if d in df.columns:
            print(f"  Converting {d} → datetime")
            df[d] = pd.to_datetime(df[d], errors='coerce')

    # Category columns
    for c in cats:
        if c in df.columns:
            print(f"  Converting {c} → category")
            df[c] = df[c].astype('category')

    # Money columns ($ → float)
    for m in money:
        if m in df.columns and df[m].dtype == object:
            print(f"  Cleaning {m} → float")
            df[m] = (
                df[m]
                .str.replace("$", "", regex=False)
                .str.replace(",", "", regex=False)
                .astype("float")
            )

    # Integer-like columns
    for i in ints:
        if i in df.columns:
            print(f"  Converting {i} → integer")
            try:
                df[i] = df[i].astype("float").astype("Int64")
            except ValueError:
                print(" Using UInt16 for compact storage")
                df[i] = df[i].astype("float").astype(pd.UInt16Dtype())

print("\nAll dataframes cleaned successfully!")
```

```{python}
#| echo: false
#| warning: false
#| output: false
listings_2021 = len(dfs["2021"])
listings_2025 = len(dfs["2025"])
listings_pct_change = (listings_2025 - listings_2021) / listings_2021 * 100
listings_2022 = len(dfs["2022"])
listings_2023 = len(dfs["2023"])
listings_pct_change2223 = (listings_2023 - listings_2022) / listings_2022 * 100
```

-   From 2021-2025, Airbnb listings across London[^1] **increased by `{python} f"{listings_pct_change:.0f}%"`** from **`{python} f"{listings_2021:,}"`** to **`{python} f"{listings_2025:,}"` listings**. [Figure 1.1]{.underline} summarizes the spatial distribution and growth trends[^2].

-   Overall growth was largely driven by a **sharp increase in listings from 2022-2023 (↑`{python} f"{listings_pct_change2223:.0f}%"`**), due to higher host registrations and marketing efforts. Growth rates over the last two years have moderated.

[^1]: Data compiled from Inside Airbnb ([http://insideairbnb.com](http://insideairbnb.com/)) which uses public information on the rental outlets available for booking at specific months in each year.  While data owners have exercised care with the processing cleaning and analysis, this dataset is the best publicly available estimate of Airbnb listings. We further process the data to remove listings with duplicate IDs, or more than six null fields, which typically reflects the fact that these entries have no reviews or incomplete information.  

[^2]: Given longitudinal analysis limitations, data does not include listings which dropped off from year to year.

```{python}
#| echo: false
#| warning: false
#| output: false
# Load MSOA 2021 geopackage and BORO files from GitHub
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/'

BORO = gpd.read_file(f'{host}/{ddir}/LONDON_BOROUGH.gpkg').to_crs(epsg=27700)
MSOA = gpd.read_file(f'{host}/{ddir}/MSOA_DEC_2021_BOUNDARIES.gpkg').to_crs(epsg=27700)

# Clip MSOA Boundaries to those within London Boroughs and only retain Polygons and MultiPolygons
LONDON = gpd.GeoDataFrame(geometry=[BORO.dissolve().geometry.iloc[0]], crs= '27700')
MSOA = gpd.clip(MSOA, LONDON)
MSOA = MSOA[MSOA.geometry.type.isin(["Polygon", "MultiPolygon"])]

# Undertake an inner spatial join to retain the MSOA that fall within Greater London Boundaries 
MSOA = gpd.sjoin(MSOA, BORO[['name', 'ons_inner', 'sub_2011', 'geometry']], how='inner',predicate='intersects')

# Retain columns of interest and rename for better readability 
MSOA = MSOA[['MSOA21CD', 'MSOA21NM', 'geometry', 'name', 'ons_inner', 'sub_2011']].rename(
    columns={
        'MSOA21CD': 'ecode', 
        'MSOA21NM': 'MSOA_name', 
        'name' : 'BORO_name'}).dissolve(by='ecode' , as_index=False)
```

```{python}
#| echo: false
#| warning: false
### Visualising the growth in number of listings (2021-2025) in a simple chart 
# First, we convert each dataframe in dfs into a gdfs with crs EPSG:27700
gdfs = {}
for year, df in dfs.items():
    gdfs[year] = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs='EPSG:4326'
    ).to_crs(epsg=27700)

# Next, we count the number of listings in each Borough for each year
boro_counts = {}

for year, gdf in gdfs.items():

    # Spatial join: attach Borough name to each point
    join = gpd.sjoin(
        gdf,
        BORO[['name', 'geometry']],
        how='left',
        predicate='within'
    )

    # Group by Borough name from the join
    counts = join.groupby('name_right').size()
    counts = counts.rename('listings')        
    counts.index.name = 'name'                

    # Merge onto Borough GeoDataFrame
    bc = BORO.merge(
        counts,
        left_on='name',
        right_index=True,
        how='left'
    )

    # Replace missing Boroughs with 0 listings
    bc['listings'] = bc['listings'].fillna(0).astype(int)
    boro_counts[year] = bc

years = ["2021", "2022", "2023", "2024", "2025"]

# Set a consistent colour scale 
vmin = min(boro_counts[y]['listings'].min() for y in years)
vmax = max(boro_counts[y]['listings'].max() for y in years)

# Define the scale for mapping
sm = plt.cm.ScalarMappable(
    cmap="viridis",
    norm=plt.Normalize(vmin=vmin, vmax=vmax)
)
sm._A = []

# Set out the plot for Airbnb listings from 2021–2025
fig, axes = plt.subplots(2, 3, figsize=(8, 5), sharex=True, sharey=True)
axes = axes.ravel()

# Plot each map (5 years → fill first 5 axes)
for ax, year in zip(axes[:5], years):
    
    boro_counts[year].plot(
        ax=ax,
        column="listings",
        cmap="viridis",
        edgecolor="black",
        linewidth=0.5,
        vmin=vmin, vmax=vmax,
        legend=False
    )
    
    ax.set_title(
        f"{year}\n{int(boro_counts[year]['listings'].sum()):,} listings",
        fontsize=13,
        pad=5
    )
    ax.axis("off")
    ax.set_aspect("equal")

# Set Colourbar 
axes[5].axis("off")
cax = axes[5].inset_axes([0.15, 0.25, 0.67, 0.10])
cbar = fig.colorbar(
    sm,
    cax=cax,
    orientation="horizontal"
)
cbar.set_label("Number of Listings", fontsize=12)
cbar.ax.tick_params(labelsize=10)    

# Add main title
fig.suptitle(
    "Figure 1.1: Airbnb Listings by London Borough (2021–2025)",
    fontsize=14,
    y= 0.98
)

# Caption
fig.text(
    0.5, -0.02,
    "Source: Listings data from Insideairbnb.com, Borough boundaries from data.london.gov.uk",
    ha="center",
    fontsize=11
)

# Reduce whitespace between rows 
plt.subplots_adjust(
    top= 0.88,
    bottom= 0.01,
    hspace=0.05,  
    wspace=0.02
)
plt.show()

### ChatGPT5.1 was used to refine the plot layout and the inclusion of the colourbar in the figure, and to size the positioning of the chart. 
```

#### 1.2 Which areas saw the fastest growth in Airbnb listings?

```{python}
#| echo: false
#| warning: false
### Increase in Airbnb listings by Borough 
# Combine all Borough counts into one table
df_boros = pd.DataFrame({
    year: boro_counts[year].set_index("name")["listings"]
    for year in ["2021", "2022", "2023", "2024", "2025"]
})
df_boros.head()

# Compile data for number of listings in 2021 and 2025 and percentage change in listings 
df_boros["x_2025"] = df_boros["2025"]
df_boros["abs_change"] = df_boros["2025"] - df_boros["2021"]
df_boros["pct_change"] = (df_boros["abs_change"] / df_boros["2021"].replace(0, pd.NA)) * 100

# Select top 5 Boroughs by absolute increase in listings
top5 = df_boros.nlargest(5, "abs_change")

# Parse out references for key findings 
top_boro = top5["abs_change"].idxmax()
top_boro_pct_change = top5.loc[top_boro, "pct_change"].round(0)
top_boro_2025 = top5.loc[top_boro, "2025"]
```

-   From 2021-2025, central areas of London generally saw larger increases. The **`{python} f"Borough of {top_boro}"`** saw the **largest increase in listings `{python} f"Borough of {top_boro_pct_change}%"`** (to reach **`{python} f"Borough of {top_boro_2025:,.}%"`** **listings in 2025** ([Figure 1.2]{.underline}).

```{python}
#| echo: false
#| warning: false
# Plot figure to show top 5 Boroughs with largest increase in listings. 
fig, ax = plt.subplots(figsize=(8, 6))

boros = top5.index
boros_display = [
    "Kensington\nand Chelsea" if b == "Kensington and Chelsea" else b
    for b in boros
]
x = np.arange(len(boros))
width = 0.35 

# Bars for 2021 and 2025
ax.bar(x - width/2, top5["2021"], width, label="2021", color="lightgray", edgecolor="black")
ax.bar(x + width/2, top5["2025"], width, label="2025", color="skyblue", edgecolor="black")

# Add percentage labels above the 2025 bars
for i, boro in enumerate(boros):
    abs_inc = top5.loc[boro, "abs_change"]
    pct = top5.loc[boro, "pct_change"]
    
    ax.text(
        x[i] + width/2,
        top5.loc[boro, "2025"] + 150,     # slightly above the bar
        f"+{abs_inc:,}\n ({pct:.1f}%)",
        ha="right",
        va="bottom",
        fontsize=14
    )

# Formatting
ax.set_title("Figure 1.2: Top 5 Boroughs with largest increase in Listings (2021-2025)\n(% increase in parentheses)", fontsize=14, pad=15)
ax.set_ylabel("Number of Listings", fontsize=12)
ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f"{int(x):,}"))
ax.tick_params(axis='y', labelsize=12)
ax.set_ylim(0, 12000)
ax.grid(axis="y", linestyle="--", alpha=0.4)

ax.set_xticks(x)
ax.set_xticklabels(boros_display, fontsize=12)
ax.legend(fontsize=12)

# Include Caption at bottom 
fig.text(
    0.5, -0.02,
    "Source: Listings data from Insideairbnb.com, Borough boundaries from data.london.gov.uk",
    ha="center",
    fontsize=11)

plt.tight_layout()
plt.show()
```

#### 1.3 How does this compare to changes in the overall housing stock?

```{python}
#| echo: false
#| warning: false
#| output: false
### Download data from Valuation Office Agency on Housing Stock 
# Set the parameters to retrieve the downloaded data from LSOA 
host = 'https://raw.githubusercontent.com/benjamintee/'
years = [2020, 2021, 2022, 2023, 2024]

df_stock = pd.concat(
    [pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
     for y in years],
    ignore_index=True)

df_stock.info()

# Filter for LA data and ecodes that fall within Greater London 
df_stock_LA = df_stock[
    (df_stock['geography'] == 'LAUA') &
    (df_stock['ecode'].str.startswith('E090'))
]

# Filter and select MSOAs within Greater London 
"""
This block first extracts names of London Boroughs, then filters the df_stock to select only the MSOAs with name strings that contain any of the London Borough names, hence forming a dataframe of property stock by MSOA in Greater London only
"""

#list of london Boroughs
london_boroughs = df_stock_LA.area_name.unique().tolist()

# Build regex pattern — escape special chars and join with |
pattern = "|".join([rf"\b{b.replace(' ', r'\s+')}\b" for b in london_boroughs])

# Filter for MSOA data and ecodes that fall within Greater London 
df_stock_MSOA = df_stock[
    (df_stock['geography'] == 'MSOA') &
    (df_stock['area_name'].str.contains(pattern, case=False, na=False, regex=True))
]
```

```{python}
#| echo: false
#| warning: false
# Create a stock_table detailing the total number of properties in London each year 
stock_table = (
    df_stock_MSOA
    .groupby("Year")["all_properties"]
    .sum()
    .sort_index())

pct_change = stock_table.pct_change() * 100

summary_stock = pd.DataFrame({
    "total_stock": stock_table,
    "pct_change_yoy": pct_change})

stock_2020 = summary_stock.loc[2020, "total_stock"]
stock_2024 = summary_stock.loc[2024, "total_stock"]
stock_growth = (summary_stock.loc[2024, "total_stock"] - summary_stock.loc[2020, "total_stock"]) / summary_stock.loc[2020, "total_stock"]*100
```

-   From 2020-2024, total housing stock in London only increased from **`{python} f"{stock_2020 / 1000000:.2f}M"`** to **`{python} f"{stock_2024 / 1000000:.2f}M"`** ([Figure 1.3]{.underline}, data for 2025 unavailable), with a **much slower growth rate of around `{python} f"{stock_growth:.1f}%"`, over the last five years**. Amidst robust demand and a tight housing market, the faster increase in properties for short-term rental relative to housing stock could have exacerbated pressures on housing prices and rents ([Todd, Musah & Cheshire (2022)](https://discovery.ucl.ac.uk/id/eprint/10124650/7/Musah_23998083211001836.pdf)).

```{python}
#| echo: false
#| warning: false
### Plot a line chart to show the increase in housing stock in London 
fig, ax = plt.subplots(figsize=(8,4))

ax.plot(
    stock_table.index,
    stock_table.values,
    marker="o",
    markersize=10,
    linewidth=3,
    color="skyblue"
)

# Title + Axis labels 
ax.set_title("Figure 1.3: Total Housing Stock in London (2020–2024)", fontsize=14, pad=15)
ax.set_xlabel("Year", fontsize=12)
ax.set_ylabel("Total Number of Properties (Millions)", fontsize=12)

# Total stock labels ABOVE each point 
for year, value in stock_table.items():
    ax.text(
        year, value + 25000,
        f"{value/1_000_000:.2f}M",
        ha="center",
        va="bottom",
        fontsize=12
    )

#  Y-axis formatting 
ax.set_ylim(3500000, stock_table.max() * 1.06)
ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f"{x/1_000_000:.2f}M"))

#  X-axis whole numbers only 
ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

# Tick label sizes
ax.tick_params(axis="x", labelsize=12)
ax.tick_params(axis="y", labelsize=12)

fig.text(
    0.01, -0.05,
    "Source: UK Valuation Office Agency properties by Council Tax band as at end March.\n"
    "As data for 2025 is not yet available, we used data over the last five years as a suitable proxy.",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| warning: false
### Change in stock and listings at the Borough level
df_stock_LA_wide = (
    df_stock_LA
    .pivot_table(
        index=["area_name", "ecode"],
        columns="Year",
        values="all_properties")
    .reset_index()).rename(columns={"area_name": "name"})

df_stock_LA_wide["stock_change_2020_2024"] = df_stock_LA_wide[2024] - df_stock_LA_wide[2020]
df_stock_LA_wide["pct_change_2020_2024"] = (df_stock_LA_wide["stock_change_2020_2024"] / df_stock_LA_wide[2020]) * 100

df_listings_LA_wide = df_boros.reset_index().rename(columns={"index": "name", "abs_change": "listings_change"})

df_change_LA = df_stock_LA_wide.merge(
    df_listings_LA_wide,
    on="name",
    how="left")

df_change_LA = df_change_LA[["name", "stock_change_2020_2024", "listings_change"]]
df_change_LA["listings_stock_perc"] = df_change_LA["listings_change"] / df_change_LA["stock_change_2020_2024"] 
```

-   **Inner London Boroughs (orange bars in [Figure 1.4]{.underline}) saw a higher ratio of increases in Airbnb listings compared to increases in housing stock**. Kensington and Chelsea experienced listing growth that was 5 times that of housing stock growth and faced the largest impact of listings growth.  

```{python}
#| echo: false
#| warning: false
### Visualising the change in listings as a percentage of change in housing stock 
df_bar = df_change_LA[["name", "listings_stock_perc"]].sort_values(
    "listings_stock_perc",
    ascending=True)

inner_london = {
    "Camden", "City of London", "Greenwich", "Hackney", "Hammersmith and Fulham",
    "Islington", "Kensington and Chelsea", "Lambeth", "Lewisham",
    "Southwark", "Tower Hamlets", "Wandsworth", "Westminster"}

df_bar["area_type"] = df_bar["name"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London")

color_map = {
    "Inner London": "tomato",
    "Outer London": "skyblue"}

colors = df_bar["area_type"].map(color_map)

fig, ax = plt.subplots(figsize=(8, 6))

ax.bar(
    df_bar["name"],
    df_bar["listings_stock_perc"],
    color=colors)

# Axis labels & title 
ax.set_xticks(range(len(df_bar)))
ax.set_xticklabels(df_bar["name"], rotation= 90, ha="right", fontsize=10)
ax.set_ylabel("Ratio of increase in listings to housing stock", fontsize=11)
ax.tick_params(axis="y", labelsize=12)
ax.set_title(
    "Figure 1.4: Ratio of Change in Airbnb Listings to Change in Housing Stock \nby Boroughs (last five years)",
    fontsize=14,
    pad=15)

# Legend title 
handles = [
    plt.Rectangle((0,0),1,1, color="tomato"),
    plt.Rectangle((0,0),1,1, color="skyblue")
]
ax.legend(handles, ["Inner London Boroughs", "Outer London Boroughs"], fontsize=12)

# Add caption
plt.figtext(
    0.01, -0.08,
    "Source: Listings data from Insideairbnb.com, \nBorough boundaries from data.london.gov.uk, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock in 2025 was unavailable, changes were compared over the last five years\n"
    "(i.e. Airbnb listings (2021–2025), housing stock (2020–2024)).",
    ha="left",
    fontsize=11)

plt.tight_layout()
plt.show()
```

#### 1.4 What is the situation at the Borough- and MSOA-levels in 2025? Are issues widespread or localised?

```{python}
#| echo: false
#| warning: false
### Aggregation of listings to Boroughs by count
gdf_BORO = gpd.sjoin(gdf, BORO[['gss_code', 'geometry']], how='left', predicate='within')
BORO = BORO.merge(gdf_BORO.groupby('gss_code').size().rename('listings2025'), on='gss_code', how='left')
BORO['listings2025'] = BORO['listings2025'].fillna(0).astype(int)

# Filter the total stock data for 2024
df_stock_BORO2025 = df_stock_LA[df_stock_LA['Year'] == 2024]

# Rename BORO columns
BORO = BORO.rename(columns={'gss_code': 'ecode'}).dissolve(by='ecode', as_index=False)

# Merge in the data on the count of all_properties to Borough by gss_code
BORO = BORO.merge(df_stock_BORO2025[['ecode','all_properties']], on='ecode', how='left')
BORO = BORO.rename(columns={'all_properties': 'allprop2024'})

# Count listings as a proportion of total housing stock
BORO['listing_prop2025'] = (BORO['listings2025'] / BORO['allprop2024']) * 100
BORO['listing_prop2025'] = BORO['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)
```

-   At the Borough-level ([Figure 1.5]{.underline}), the **proportion of Airbnb listings compared to the overall housing stock ranged from 0.2% to 5.8%**, with Inner London Boroughs having a higher share of listings relative to housing stock.  

```{python}
#| echo: false
#| warning: false
### Visualising the percentage of listings of housing stock in 2025

# Prepare the dataframe
df_bar = BORO[["name", "listing_prop2025"]].copy()

# Calculate overall average
overall_mean = df_bar["listing_prop2025"].mean()

# Create Inner/Outer labels
df_bar["area_type"] = df_bar["name"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London")

# Sort by the measure
df_bar = df_bar.sort_values("listing_prop2025", ascending=True)

# Colour map
color_map = {
    "Inner London": "tomato",
    "Outer London": "skyblue"}
colors = df_bar["area_type"].map(color_map)

# Plot the figure 
fig, ax = plt.subplots(figsize=(8, 6))

ax.bar(
    df_bar["name"],
    df_bar["listing_prop2025"],
    color=colors)

# Add average line 
ax.axhline(
    overall_mean,
    color="grey",
    linestyle="--",
    linewidth=1.2,
    label=f"Average: {overall_mean:.2f}%")

# Label average line
ax.text(
    x= -0.1,              
    y=overall_mean + overall_mean*0.02,  
    s=f"Average: {overall_mean:.2f}%",
    fontsize=12,
    ha="left",
    va="bottom")

# Axis labels and title
ax.set_xticks(range(len(df_bar)))
ax.set_xticklabels(df_bar["name"], rotation=90, ha="right", fontsize=12)
ax.tick_params(axis="y", labelsize=12)
ax.set_ylabel("Listings as percentage of Housing Stock (%)", fontsize=12)
ax.set_title(
    "Figure 1.5: Airbnb Listings as a Proportion of Housing Stock by Boroughs (2025)",
    fontsize=14,
    pad=15)

# Legend
handles = [
    plt.Rectangle((0,0),1,1, color="tomato"),
    plt.Rectangle((0,0),1,1, color="skyblue"),
]
ax.legend(handles, ["Inner London", "Outer London"], fontsize=14)

# Caption Text
plt.figtext(
    0.01, -0.05,
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used.",
    ha="left",
    fontsize=11)

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| warning: false
### Aggregation of listings to MSOA level 
# Filter the total stock data for 2024
df_stock_MSOA2025 = df_stock_MSOA[df_stock_MSOA['Year'] == 2024]

# Merge in the data on the count of all_properties to MSOA by ecode
MSOA = MSOA.merge(df_stock_MSOA2025[['ecode', 'all_properties']], on='ecode', how='left')
MSOA = MSOA.rename(columns={'all_properties': 'allprop2024'})

# Count Airbnb listings inside each MSOA polygon
gdf_MSOA = gpd.sjoin(gdf, MSOA[['ecode', 'geometry']], how='left', predicate='within')
MSOA = MSOA.merge(gdf_MSOA.groupby('ecode').size().rename('listings2025'), on='ecode', how='left')
MSOA['listings2025'] = MSOA['listings2025'].fillna(0).astype(int)

# Count listings as a proportion of total housing stock
MSOA['listing_prop2025'] = (MSOA['listings2025'] / MSOA['allprop2024']) * 100
MSOA['listing_prop2025'] = MSOA['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)
```

-   At the MSOA-level, it becomes clearer that there are pockets of concentration of listings relative to housing stock in these Boroughs, with **up to 1 in 8 homes (12.5%)** listed for Airbnb ([Figure 1.6]{.underline}). This high proportion could be driving negative sentiments in some parts of London that Airbnb could be out of control.

```{python}
#| echo: false
#| warning: false
### Visualising the percentage of listings of housing stock in 2025 using Jenks breaks
import mapclassify
from matplotlib.patches import Patch

# Jenks classification
classifier = mapclassify.NaturalBreaks(MSOA['listing_prop2025'], k=5)
MSOA['jenks_class'] = classifier.yb  # 0..k-1

# Prepare class boundaries for labels
bin_edges = [round(b, 1) for b in classifier.bins]  
labels = []
prev = round(MSOA['listing_prop2025'].min(), 1)

for b in bin_edges:
    labels.append(f"{prev} – {round(b, 1)}")
    prev = round(b, 1)

# Assign colours
k = len(bin_edges)
cmap = plt.cm.viridis
colors = [cmap(i / (k-1)) for i in range(k)]

# Create figure
fig, ax = plt.subplots(1, 1, figsize=(8, 6))

# Plot each class separately for categorical legend
for i, color in enumerate(colors):
    MSOA[MSOA['jenks_class'] == i].plot(
        ax=ax,
        color=color,
        edgecolor="gray",
        linewidth=0.35)

# Plot Borough boundaries on top
BORO.boundary.plot(ax=ax, edgecolor='black', linewidth=1.0)

# Create custom legend with class boundaries
legend_handles = [Patch(facecolor=color, edgecolor='gray', label=label)
                  for color, label in zip(colors, labels)]
ax.legend(
    handles=legend_handles,
    title="Listings as Proportion\n of Housing Stock (%)",
    title_fontsize=12,
    fontsize=12,
    loc="upper right")

# Titles and annotations
ax.set_title("Figure 1.6: Airbnb Listings as a Proportion of Housing Stock (MSOA, 2025)", fontsize=14, pad=15)
ax.axis("off")
fig.text(
    0.03, -0.02,
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used.",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()
```

#### 1.5 How many listings potentially breach the current 90-day limit for rentals?

-   Since 2015 *(AS note: I think 2015: www.london.gov.uk/press-releases/mayoral/registration-system-for-short-term-letting-law)*, London planning rules state that *entire homes* let on platforms such as Airbnb should not be used for short-term letting for more than **90 nights per calendar year**, without planning permission obtained. This prevents permanent housing from being converted into de facto holiday accommodation.

```{python}
#| echo: false
#| warning: false
## Count the number of entire homes/apartments that were let out for more than 90 days in 2025
# Number of Listings that were entire homes/apartments 
df_2025 = dfs['2025'] 
entire_homes_2025 = df_2025[df_2025["room_type"] == "Entire home/apt"]
entire_homes_2025_n = len(entire_homes_2025)
entire_homes_2025_per = round(entire_homes_2025_n / len(df_2025) * 100, 0)

# Number of entire homes that had estimated occupancy > 90 days 
entire_homes_2025_90 = entire_homes_2025[entire_homes_2025['estimated_occupancy_l365d'] > 90]
entire_homes_2025_90_n = len(entire_homes_2025_90)
entire_homes_2025_90_per = round(entire_homes_2025_90_n / entire_homes_2025_n * 100, 2)
```

-   In 2025, **`{python} f"{entire_homes_2025_n:,}"` or `{python} f"{entire_homes_2025_per}%"` of all listings** were for "entire homes / apartments". Based on occupancy estimates[^3] , **`{python} f"{entire_homes_2025_90_n:,}"`** entire homes / apartments were used for more than 90 nights per year. This represents **more than 2 in 10 of all entire homes listings**, and **around 1 in 8 of all listings in London** that were possibly in breach of this rule.

-   Notwithstanding the sensitivity of the assumptions in Inside Airbnb's methodology[^4], we expect this estimate to be a conservative one, given reported **likelihood of multiple listings of the same property**, which some owners have used to circumvent the 90 day rule[^5].

[^3]: Occupancy estimates are obtained from Inside Airbnb data. First, a review rate is used to compute reviews to estimated bookings. Thereafter, the average length of stay for London was multiplied by the estimated bookings for each listing giving the occupancy rate (out of 365 days). To estimate potential breaches, we look at the subset of listings that are "entire homes / apartments", and the occupancy rate, which estimates how many nights the listing was booked in the last 365 days, providing a useful proxy for annual use, where raw daily calendar data are unavailable.

[^4]: The occupancy model assumed (a) a review rate of 50% and (b) average length of stay of 3.1 days for London. This is conservative given that Airbnb previously cited an average review rate of 78%, and, community forums have cited a range of 33% and 85% for various properties.

[^5]: BBC journalists found about 1,300 listings had reused identical images - such as the same furniture, rooms and decor - from other supposedly unique listing. <https://www.bbc.co.uk/news/articles/cvg96rz9061o>

```{python}
#| echo: false
#| warning: false
data = entire_homes_2025["estimated_occupancy_l365d"]

# Define bins every 30 days up to 271
bins = np.arange(0, 271, 30)
thresholds = [90, 120, 150, 180]

# Summary table data
summary_rows = []
for t in thresholds:
    count_t = (data > t).sum()
    pct_t   = round(count_t / entire_homes_2025_n * 100)
    summary_rows.append([t, count_t, pct_t])

summary_df = pd.DataFrame(summary_rows, columns=["Days", "Count", "% total"])

# Plot histogram with summary table at top 
from matplotlib.ticker import FuncFormatter
fig, ax = plt.subplots(figsize=(8, 5))

counts, edges, patches = ax.hist(
    data,
    bins=bins,
    edgecolor="black",
    color="skyblue",
    alpha=0.8
)

# Colour bins > 90 days
for i, patch in enumerate(patches):
    mid = (edges[i] + edges[i+1]) / 2
    if mid > 90:
        patch.set_facecolor("tomato")

# Add vertical line at 90 days
ax.axvline(90, color="black", linestyle="--", linewidth=1.2)

# Titles and labels
ax.set_title(
    "Figure 1.7: Distribution of Estimated Occupancy (Entire Homes/Apt) in 2025",
    fontsize=14,
    pad=10
)
ax.set_xlabel("Estimated Occupancy (days per year)", fontsize=12)
ax.tick_params(axis="y", labelsize=12)
ax.set_ylabel("Number of Listings - Entire Homes / Apts", fontsize=12)
ax.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"{int(x):,}"))

fig.text(
    0.03, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

# X-axis ticks
ax.set_xticks([30, 60, 90, 120, 150, 180, 210, 240, 270])

# Summary Table 
table = plt.table(
    cellText=summary_df.values,
    colLabels=summary_df.columns,
    loc='upper right',
    cellLoc='center',
    colColours=["lightgray"] * 3,
    bbox=[0.68, 0.45, 0.31, 0.35] 
)
table.set_fontsize(11)
table.scale(1.0, 1.3)

# Legend for coloured bars 
legend_handles = [
    plt.Rectangle((0, 0), 1, 1, color="skyblue", label="Up to 90 days"),
    plt.Rectangle((0, 0), 1, 1, color="tomato", label="More than 90 days")
]
ax.legend(handles=legend_handles, fontsize=12, loc="upper right")

# Grid lines
ax.grid(axis='y', linestyle='--', alpha=0.4)

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| warning: false
## Map out the average percentages of entire homes / apartments being let out for more than 90 days by Borough 
# Convert `entire_homes_2025` to GeoDataFrame
entire_homes_2025_gdf = gpd.GeoDataFrame(
    entire_homes_2025,
    geometry=gpd.points_from_xy(entire_homes_2025.longitude, entire_homes_2025.latitude),
    crs='EPSG:4326'
    ).to_crs(epsg=27700) 

# Spatially join and include the Borough name 
entire_homes_2025_mapped = gpd.sjoin(entire_homes_2025_gdf, bc[['name', 'geometry']], how='left', predicate='within')

# Summarise spatial findings 
summary = (
    entire_homes_2025_mapped
    .groupby('name_right')
    .agg(
        total_listings=('estimated_occupancy_l365d', 'count'),
        high_occupancy=('estimated_occupancy_l365d', lambda x: (x > 90).sum())
    )
    .assign(
        pct_high_occupancy=lambda df: (df['high_occupancy'] / df['total_listings'] * 100).round(1)
    )
    .sort_values(by='pct_high_occupancy', ascending=False)
)

top3 = summary.head(3)
top3_names = top3.index.tolist()
top3_pcts = top3['pct_high_occupancy'].tolist()
```

The spatial pattern is consistent with earlier trends. Boroughs in inner London show the **highest rates of intense use** (i.e \>90 days).

-   **`{python} f"{top3_names[0]}: {top3_pcts[0]:.0f}%"`** of entire home listings have occupancy exceeding 90 nights"

-   **`{python} f"{top3_names[1]}: {top3_pcts[1]:.0f}%"`**

-   **`{python} f"{top3_names[2]}: {top3_pcts[2]:.0f}%"`**

#### 1.6 Taken together, do these trends suggest a problem and what is the impact on communities?

*\[AS note: just to flag that previous Mayoral press releases may be a good source of high level commentary about the impact, and be quite helpful in the context of 'advising' the Mayor now. e.g. https://www.london.gov.uk/mayor-demands-licencing-scheme-prevent-short-term-lets-damaging-housing-supply\]*

*\[AS note 2: we could perhaps refer to measures taken in other jurisdictions here and the reasons (basically competition for housing) but we should also talk about the positives i.e. money in Londoners' pockets for renting out their spare rooms/tourist income etc. https://www.bbc.co.uk/travel/article/20240701-what-does-a-world-without-airbnb-look-like; https://hbr.org/2024/02/what-does-banning-short-term-rentals-really-accomplish \]*

## **2. How many professional landlords are there?**

#### 2.1 What is the distribution of Airbnb types in London?

```{python}
#| echo: false
#| warning: false
# Compute the number of properties in each category in 2025 
type_summary_2025 = (
    df_2025["room_type"]
    .value_counts()
    .to_frame("count")
    .assign(percentage=lambda x: (x["count"] / x["count"].sum() * 100).round(1)))

# Plot a simple bar chart to show the distribution
# Function to add value labels on top of bars from Geeks for Geeks. 
def add_labels(x, y, pct):
    for i in range(len(x)):
        label = f"{y[i]:,}\n({pct[i]}%)"
        plt.text(
            i, y[i],label, ha='center', va='bottom',fontsize=12,
            bbox=dict(facecolor='skyblue', alpha=0.8, edgecolor='none'))

x = type_summary_2025.index.tolist()
y = type_summary_2025["count"].tolist()
pct = type_summary_2025["percentage"].tolist()

plt.figure(figsize=(8, 5))
plt.bar(x,y, color = "salmon")
add_labels(x,y, pct)

# Title and axis labels
plt.title("Figure 2.1 Distribution of Airbnb Types in London (2025)", fontsize=14, pad=15)
plt.xlabel("Room Types", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.ylim(0, 70000)
plt.gca().yaxis.set_major_formatter(
    ticker.FuncFormatter(lambda x, pos: f"{int(x):,}"))

plt.subplots_adjust(top=0.88)
plt.show()
```

#### 2.2 What is the difference between a professional landlord and a commercial landlord? Does this matter for analysis?

*\[I wrote the below before I realised that Owen had dont an analysis of this point in his separate doc. I'm leaving it in as some of it may be helpful. I think the below has the possible benefit that it makes the point re business rates, which we may want to come back to as part of our recommendations, and also refers to the details of the question.\]*

The opposition proposes that "professional" landlords' properties should be subject to increased council tax. Leaving aside for the moment the question of how that proposal might be implemented and administered *\[see below...- could refer to recent Budget's proposals on Council Tax for "mansions"\]*, a separate issue is how the opposition would determine which Airbnb landlords would be classed as "professional".

Pending receiving clarity from the opposition on that point, in this report a "professional" landlord is defined as one:

-   whose income from their Airbnb properties is *\[greater than the median UK/London? income\]*. This is on the basis that the plain English meaning of the word "professional" is as a counterpart to "amateur". In this context, "professional" suggests that the landlord could be relying on Airbnb for their living; that is, to provide the income that they would otherwise be earning through employment.

-   whose properties are residential, rather than commercial. This is based on, first, the opposition's reference to Council Tax (which is not paid by commercial landlords; instead they pay business rates) and, second, the context for the opposition's proposal: the letting of three "homes" (including one council-owned property) on Airbnb - the likely argument on the opposition's part being that letting these homes on Airbnb means that they are not available either for private rental (or council tenants) or sale to buyers on the property market.

```{python}
#AS note: leaving code block here for now

```

#### 2.3 How many hosts have listings for two or more entire properties (homes / apartments)? 

```{python}
#| echo: false
#| warning: false
# Counting the number of entire homes / apartments for each host id 
host_sizes = (
    entire_homes_2025
    .groupby("host_id")["id"]
    .count()
    .reset_index(name="listing_count"))

# Merging the total number of entire homes back to the listings dataframe
df = entire_homes_2025.merge(host_sizes, on="host_id", how="left")

# Given the wide and long-tailed distribution, we visualise them in discrete buckets instead
# Using 1, 2, 10, 100, 100+ as discrete categories for a sense of scale 
bins = [0, 1, 2, 10, 100, float("inf")]
labels = ["1", "2", "3-10", "11-100", "100+"]

df["host_size_group"] = pd.cut(
    df["listing_count"],
    bins=bins,
    labels=labels,
    include_lowest=True
)
# Further check for each bucket of listings, how many were let out for 90 or more days
df["occupancy_group"] = df["estimated_occupancy_l365d"].apply(
    lambda x: "<90 days" if x < 90 else "≥90 days")

# Compute % of listings in each bucket
table = df.groupby(["host_size_group", "occupancy_group"]).size().unstack(fill_value=0)
dist = table / table.sum().sum() * 100
labels = ["1", "2", "3-10", "11-100", "100+"]
dist = dist.reindex(labels)
print(dist)

# Computing number of hosts for each category 
host_count = (
    host_sizes
    .assign(host_size_group=pd.cut(
        host_sizes["listing_count"],
        bins=bins,
        labels=labels,
        include_lowest=True))
    .groupby("host_size_group")["host_id"]
    .nunique()
    .reindex(labels))

# Plot two panel figure with bar chart and bubble plot for hosts
fig, (ax1, ax2) = plt.subplots(
    ncols=2,
    figsize=(10, 5),
    gridspec_kw={'width_ratios': [4, 1]})

colors = ["tomato", "skyblue"]

# Horizontal bar chart (Left)
ax1.barh(dist.index, dist["<90 days"], color=colors[0], label="< 90 days")
ax1.barh(dist.index, dist["≥90 days"],
         left=dist["<90 days"], color=colors[1], label="≥ 90 days")

# Add % labels
for i, (low, high) in enumerate(zip(dist["<90 days"], dist["≥90 days"])):
    ax1.text(low / 2, i, f"{low:.0f}%", va="center", ha="center", fontsize=12)
    ax1.text(low + high / 2, i, f"{high:.0f}%", va="center", ha="center", fontsize=12)

# Titles and labels  
fig.suptitle("Figure 2.2 Distribution of Listings by Host Portfolio Size and Estimated Occupancy (2025)",
              fontsize=14)
ax1.set_xlabel("Percentage of Entire Home / Apartment Listings (%)", fontsize=12)
ax1.set_ylabel("Number of listings by unique host", fontsize=12)
ax1.set_xlim(0, 50.5)
ax1.legend(title="Occupancy Category", loc="upper right", fontsize=12)
ax1.tick_params(labelsize=12)

# Bubble chart (Right)
y_pos = np.arange(len(dist.index))
bubble_sizes = host_count.values
scaled_sizes = bubble_sizes / bubble_sizes.max() * 2000  

ax2.scatter(
    [0] * len(host_count),   
    y_pos,
    s=scaled_sizes,
    alpha=0.6,
    color="gray",
    edgecolor="black")

# Format bubble axis to match primary y-axis
ax2.set_yticks(y_pos)
ax2.set_yticklabels([])     # hide labels on bubble axis
ax2.set_xticks([])
ax2.set_xlim(-0.2, 0.6)
ax2.set_ylim(ax1.get_ylim())   # ensures identical vertical span
ax2.set_title("Number of Hosts", fontsize=12, pad=10)

# Add numeric labels beside bubbles
for y, val in zip(y_pos, bubble_sizes):
    ax2.text(0.2, y, f"{val:,}", va="center", fontsize=11)

# Figure Caption
fig.text(
    0.03, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| warning: false
# Are there really hosts with more than 100+ listings? Who are they? 
df_100plus = df[df["host_size_group"] == "100+"].copy()

# Investigating if these are hosts with multiple listings to evade the 90 day rule
same_location_same_host = (
    df_100plus.groupby(["host_id","latitude", "longitude"])
      .size()
      .reset_index(name="count")
      .query("count > 1")
      .sort_values("count", ascending=False)
)

#calculate numbers for hosts with 1, more than 1 and 100 or more entire homes on Airbnb
df_only1 = df[df["host_size_group"] == "1"].copy()
df_morethan1 = df[df["host_size_group"] != "1"].copy()
df_5ormore = df[df["listing_count"] >= 5].copy() #for calculations based on a 5+ property threshold


propmorethan1 = len(df_morethan1)/len(df) * 100
print(propmorethan1)
prop100plus = len(df_100plus)/len(df) * 100
print(prop100plus)
```

An analysis of the number of entire homes / apartments listed by each host[^6] shows that a **small number of hosts are responsible for a disproportionately large number of listings (Figure 2.2)**. Previous studies have found that such "multi-listing hosts" are significant drivers of increase in rent and Airbnb's profit in London[^7].

[^6]: We assume that hosts either [own]{.underline} the entire home / apartment that they list, or have [written permission]{.underline} from owners to list on Airbnb.

[^7]: Todd et al. and Cox and Slee BT: Need to cite this properly and reference the key finding.

-   **`{python} len(df_only1)`** hosts own only 1 entire property. **`{python} len(df_morethan1)`** hosts own two or more properties.

-   **`{python} len(df_100plus)`** hosts own over 100 properties. This represents **`{python} prop100plus`** of the total number of entire homes available.

#### 2.4 What is the average income from these properties, and how does it compare with median / average incomes in London?

*\[From Owen's week7 doc; incorporate and expand further to include his results:\]* The London Datastore reported that the mean income in London for 2022-2023 was £59,000 and the median was £34,100. *\[expand source?\]*

```{python}
#[Expand to include relevant code/calculations]
```

#### 2.5 What is the profile of these hosts ?

```{python}
#use 'host_location' column in the dfs to try to compare whether hosts who own > 1 property are more likely to be based outside the UK. Note that there are NA values in some host_location rows; these have been excluded (method for doing so queried with CoPilot) (but the numbers of these are noted below).

only1propUK = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na = False)] #20,538 hosts
only1propUKNAs = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na = True)] #25,798 hosts
only1propUKNAcount = len(only1propUKNAs) - len(only1propUK) #5,260 hosts not giving locations
only1propUK_per = (len(only1propUK)/len(df_only1))*100 #75.6% of hosts with only 1 property are UK-based (not counting NAs)

propmorethan1UK = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na = False)] #20,457 hosts
propmorethan1UKNAs = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na = True)] #29,333 hosts
morethan1propUKNAcount = len(propmorethan1UKNAs) - len(propmorethan1UK) #8,876 hosts not giving locations
propmorethan1UK_per = (len(propmorethan1UK)/len(df_morethan1))*100 #67.2% of hosts with more than 1 property are UK-based (not counting NAs)

prop100plusUK = df_100plus[df_100plus.host_location.str.contains(r'United Kingdom', regex= False, na = False)] #2,003 hosts
prop100plusUKNA = df_100plus[df_100plus.host_location.str.contains(r'United Kingdom', regex= False, na = True)] #2,282 hosts
prop100plusUKNAcount = len(prop100plusUKNA) - len(prop100plusUK)#279 hosts not giving location
prop100plusUK_per = (len(prop100plusUK)/len(df_100plus))*100 #72.8% of hosts with more than 100 properties are UK-based (not including NAs)

#investigate the hosts that own more than 100 properties to establish where they are based, how many properties each of them own and whether it can be established that they are individuals or companies
df_100plus_grouped = df_100plus.groupby(['host_id', 'host_name', 'host_location']).size().reset_index(name = "number_of_properties").sort_values("number_of_properties", ascending=False) 

df_100plus_grouped_len = len(df_100plus_grouped) #13 hosts
df_100plus_grouped['number_of_properties'].sum() #total of 2,472 properties
```

A large majority of hosts renting out entire rooms/apts in London, whether they rent out one or more than 100, are based in the UK.

However, `{python}df_100plus_grouped_len` owners between them own `{python}df_100plus_grouped['number_of_properties'].sum()` entire homes/apartments that they let on Airbnb. The host with the most properties is a company based in Dubai; three other host names suggest that they are companies rather than individuals. *\[AS note: possible link to pros/cons of Council Tax approach vs business rates approach: these hosts may be paying a huge amount of tax already if they are properly registered for business rates already - this could be entirely lawful (and be compliant with planning permission too).\]*

## **3. How many properties would be affected by the opposition’s proposal?**

The opposition is proposing that "professional" landlords be required to register their properties and pay higher Council Tax rates.

On the basis set out above that any individual owning two or more properties is a "professional" landlord, this would mean that `{python}len(propmorethan1UK)` properties would need to be registered.

If, instead, the threshold is set at \\5 properties, then `{python}len(df_5ormore)` would require registration.

## **4. What are the likely pros and cons of the opposition’s proposal (for the Mayor, for residents, and for the city)?**

## **5. Can the story be reframed as a positive one about social mobility or housing opportunity?**

## References