import pandas as pd
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'
# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
# View data structure, column names and types
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")
# View basic information about the dataset
df.info(verbose = True)
# Generate a list of column names to identify variables of interest
print(df.columns.to_list())
# Subsetting columns of interest
cols = ['id', 'listing_url', 'last_scraped', 'name',
'description', 'host_id', 'host_name', 'host_since',
'host_location', 'host_about', 'host_is_superhost',
'host_listings_count', 'host_total_listings_count',
'host_verifications', 'latitude', 'longitude',
'property_type', 'room_type', 'accommodates',
'bathrooms', 'bathrooms_text', 'bedrooms', 'beds',
'amenities', 'price', 'minimum_nights', 'maximum_nights',
'availability_365', 'number_of_reviews',
'first_review', 'last_review', 'review_scores_rating',
'license', 'reviews_per_month']
print(f"Cols contains {len(cols)} columns.")
# Downloading the full data with columns of interest
df = pd.read_csv(url, compression='gzip', low_memory=False, usecols=cols)
print(f"Data frame is {df.shape[0]:,} rows x {df.shape[1]} columns")
df.info(verbose = True)
# Removing rows with NA in id
df.drop(df[df.id.isna()].index.array, axis=0, inplace=True)
print(f"Data frame contains {df.shape[0]:,} rows.")
# Counting the NA data by columns
df.isnull().sum(axis=0).sort_values(ascending=False).head(10)
# Drop the two columns with the most number of NA data (license, host_about)
df.drop(columns=['license','host_about'], inplace=True)
# Next, we look out for rows which may have multiple NA values
df.isnull().sum(axis=1).sort_values(ascending=False).head(10)
# Looking at the probability distribution of the rows with multiple NA values
probs = df.isnull().sum(axis=1)
print(type(probs))
probs.plot.hist(bins=30)
# Dropping rows missing more than 8 values which may be problematic
cutoff = 5 # Set cutoff for 5
problematic_rows = df.loc[probs > cutoff]
df.drop(probs[probs > cutoff].index, inplace=True)
print(f"df now contains {df.shape[0]:,} rows, and we dropped {problematic_rows.shape[0]:,} rows")
# Fixing the host_is_superhost column and converting it to boolean
bools = ['host_is_superhost']
df.sample(5, random_state=43)[bools]
for b in bools:
print(f"Converting {b}")
df[b] = df[b].replace({'f':False, 't':True}).astype('bool')
# Fixing the columns with dates in them, and converting it to dates
dates = ['last_scraped','host_since','first_review','last_review']
print(f"Currently {dates[1]} is of type '{df[dates[1]].dtype}'", "\n")
df.sample(5, random_state=43)[dates]
for d in dates:
print("Converting " + d)
df[d] = pd.to_datetime(df[d])
# Fixing the columns that are supposed to be categories, convert to category
cats = ['property_type','room_type']
for c in cats:
print(f"Converting {c}")
df[c] = df[c].astype('category')
# Fixing the column with price where there are odd symbols like $ and ,
money = ['price']
for m in money:
print(f"Converting {m}")
df[m] = df[m].str.replace('$','', regex=False).str.replace(',', '', regex = False).astype('float')
# Fixing integers
ints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',
'beds','minimum_nights','maximum_nights','availability_365']
for i in ints:
print(f"Converting {i}")
try:
df[i] = df[i].astype('float').astype('int')
except ValueError as e:
print("  - !!!Converting to unsigned 16-bit integer!!!")
df[i] = df[i].astype('float').astype(pd.UInt16Dtype())
df.info()
from pathlib import Path
path = Path(f'Data/clean/{Path(url).name}')
print(f"Writing to: {path}")
if not path.parent.exists():
print(f"Creating {path.parent}")
path.parent.mkdir(parents=True, exist_ok=True)
if not path.exists():
df.to_csv(path, index=False)
print("Done.")
# Counting the number of unique IDs
unique_ids = df['id'].nunique()
print(f"There were {unique_ids:,.0f} unique IDs in the listings.")
# Counting the numnber of unique hosts
unique_hosts = df['host_id'].nunique()
print(f"There were {unique_hosts:,.0f} unique hosts.")
# Different property types and room types
unique_ptypes = df['property_type'].unique()
print(unique_ptypes)
unique_rtypes = df['room_type'].unique()
print(unique_rtypes)
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-reviews.csv.gz'
# Download first 1,000 rows for EDA
df_reviews = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
# View data structure, column names and types
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")
# View basic information about the dataset
df_reviews.info(verbose = True)
# Downloading the full data
df_reviews = pd.read_csv(url, compression='gzip', low_memory=False)
print(f"Data frame is {df_reviews.shape[0]:,} rows x {df_reviews.shape[1]} columns")
# Removing rows with NA in listing id, id, and reviewer id
df_reviews.drop(df_reviews[df_reviews.id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.listing_id.isna()].index.array, axis=0, inplace=True)
df_reviews.drop(df_reviews[df_reviews.reviewer_id.isna()].index.array, axis=0, inplace=True)
print(f"Data frame contains {df_reviews.shape[0]:,} rows.")
# Counting the NA data by columns
df_reviews.isnull().sum(axis=0).sort_values(ascending=False).head(10)
# Fixing the columns with dates in them, and converting it to dates
df_reviews['date'] = pd.to_datetime(df_reviews['date'])
# Fixing integers
ints  = ['id', 'listing_id', 'reviewer_id']
for i in ints:
print(f"Converting {i}")
try:
df_reviews[i] = df_reviews[i].astype('float').astype('int')
except ValueError as e:
print("  - !!!Converting to unsigned 16-bit integer!!!")
df_reviews[i] = df_reviews[i].astype('float').astype(pd.UInt16Dtype())
# The comments column is excessively large and is beyond the scope of the project, so we are dropping it
df_reviews.drop(columns=['comments'], inplace=True)
path = Path(f'Data/clean/{Path(url).name}')
print(f"Writing to: {path}")
if not path.parent.exists():
print(f"Creating {path.parent}")
path.parent.mkdir(parents=True, exist_ok=True)
if not path.exists():
df_reviews.to_csv(path, index=False)
print("Done.")
# Set the parameters to retrieve the downloaded data
host = 'https://raw.githubusercontent.com/benjamintee/'
years = ['2020', '2021', '2022', '2023', '2024']
df_stock = pd.concat(
[pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
for y in years],
ignore_index=True)
df_stock.info()
# Filter for LA data and ecodes that fall within Greater London
df_stock_LA = df_stock[
(df_stock['geography'] == 'LAUA') &
(df_stock['ecode'].str.startswith('E090'))
]
#df_stock = df_stock.reset_index(drop=True)
"""
This block first extracts names of london boroughs, then filters the df_stock to select only the MSOAs with name strings that contain any of the london borough names, hence forming a dataframe of property stock by MSOA in Greater London only
"""
#list of london boroughs
london_boroughs = df_stock_LA.area_name.unique().tolist()
# Build regex pattern â€” escape special chars and join with |
pattern = "|".join([rf"\b{b.replace(' ', r'\s+')}\b" for b in london_boroughs])
# Filter for MSOA data and ecodes that fall within Greater London
df_stock_MSOA = df_stock[
(df_stock['geography'] == 'MSOA') &
(df_stock['area_name'].str.contains(pattern, case=False, na=False, regex=True))
]
#delete df_stock to save memory
del(df_stock)
import pandas as pd
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'
# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
pip install pandas
!pip install pandas
import pandas as pd
# Set filepath and read csv data using pandas
ymd  = '20250615'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'
# Download first 1,000 rows for EDA
df = pd.read_csv(url, compression='gzip', nrows = 1000, low_memory=False)
library("reticulate")
library(reticulate)
py_install("pandas", pip = TRUE)
reticulate::repl_python()
