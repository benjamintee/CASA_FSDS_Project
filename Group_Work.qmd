---
date: last-modified
title: The Pythoneers' Group Project
bibliography: fsds_group_work.bib
csl: harvard-cite-them-right.csl
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, The Pythoneers, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 16 December 2025 (Tuesday)

| Names          | Student Numbers | Contact Email                |
|----------------|-----------------|------------------------------|
| Alice Southall | 25120600        | alice.southall.25\@ucl.ac.uk |
| Benjamin Tee   | 25049107        | benjamin.tee.25\@ucl.ac.uk   |
| Bosco Choi     | 22040212        | bosco.choi.22\@ucl.ac.uk     |
| Owen Hughes    | 25197128        | owen.hughes.25\@ucl.ac.uk    |
| Tong, C.Y      | 25244321        | c.tong.25\@ucl.ac.uk         |

Full code and data can be found at the attached (<https://github.com/benjamintee/CASA_FSDS_Project>)

Web-friendly HTML version can be found at the following (<https://benjamintee.github.io/CASA_FSDS_Project/Group_Work.html>)

## Priorities for Feedback

(Include write-up here on areas for feedback)

{{< pagebreak >}}

# Finding a balance: Airbnb's growth, impact and benefits for London

## Executive Summary 

The opposition's proposal is likely to gain some traction due to public awareness of recent protests and measures taken against Airbnb in other holiday destinations[^1] , and a perception (due to repeated promises by more than one UK government to build more housing) that there is a shortage of housing in the UK [^2]. The discovery of the Mayor's aide's housing circumstances has stirred media interest, feeding into the debate about the current unbalanced housing market and suspicions of political hypocrisy[^3].

[^1]: [Spain clamps down on Airbnb as tourism backlash returns for summer (BBC, 2025)](https://www.bbc.co.uk/news/articles/c3wdd8lg581o)

[^2]: [Housing targets increased to get Britain building again (Gov.uk, 2024)](https://www.gov.uk/government/news/housing-targets-increased-to-get-britain-building-again)

[^3]: [Homelessness Minister Rushanara Ali quits over rent increase claims (BBC, 2025)](https://www.bbc.co.uk/news/articles/clyd3l2x2n8o)

While studies have investigated the potentially undesirable consequences for areas that host large proportions of Airbnb properties, such as increased crime rates and anti-social behaviour, the opposition's proposal focuses on the impact of Airbnb on housing stock, and this report does likewise.

This report draws from data from Inside Airbnb[^4], to evaluate the scale and nature of Airbnb's penetration into the London property market, examining the implications and recommendations for action. We also discuss limitations of the data at the end of the report.

[^4]: Inside Airbnb obtains public information on rentals available for booking at specific months through web-scraping which could contain errors or discrepancies. We further process the data to remove listings with duplicate IDs, or more than six null fields, which typically reflects the fact that entries have no reviews or incomplete information.   

In summary, we conclude that:

1.  London's Airbnb listings have expanded faster than housing supply, particularly in inner boroughs[^5], heightening market pressures and public concern.
2.  Many entire-home listings exceed the 90-day limit, with potential regulatory breaches concentrated among multi-listing "professional" landlords.
3.  The opposition's proposed council tax surcharge is a blunt and politically difficult tool. Effective alternatives include landlord registration, enforcement of business rates for commercial operators or a tourist tax, which targets problematic hosts without disadvantaging ordinary residents who rely on modest hosting income.

[^5]: We refer to the definitions of inner London / outer London boroughs as per [London Plan 2021](https://www.london.gov.uk/programmes-strategies/planning/london-plan/the-london-plan-2021-online/annex-2-inner-and-outer-london-boroughs).

```{python}
#| lst-label: Load common libraries and packages
#| echo: false
#| warning: false
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.lines import Line2D
from pathlib import Path
import requests
import mapclassify
import math
from matplotlib.patches import Patch
from matplotlib.ticker import FuncFormatter
```

```{python}
#| lst-label: Download BibTeX and csl files into the same folder if it has not been downloaded yet
#| echo: false
#| warning: false
#| output: false
# Name of the BibTeX and csl files
bib_name = ["fsds_group_work.bib", "harvard-cite-them-right.csl"]

# Folder where the .qmd file lives
qmd_folder = Path(".").resolve()  

# Raw GitHub URL
url_bib = "https://raw.githubusercontent.com/BoscoChoi/CASA_FSDS_Project/refs/heads/main/fsds_group_work.bib"

for bib in bib_name:
    # Full path for the BibTeX/csl file
    bib_path = qmd_folder / bib

    # Download only if missing
    if not bib_path.exists():
        print(f"{bib_name} not found. Downloading to {bib_path} ...")
        response = requests.get(url_bib)
        response.raise_for_status()
        bib_path.write_bytes(response.content)
        print("Download complete!")
    else:
        print(f"{bib_name} already exists at {bib_path}")
```

```{python}
#| lst-label: Download Airbnb listings data
#| echo: false
#| warning: false
#| output: false
# Looking at the file structure in orca, 2024/2025 have one structure, 2022/2023 have one structure and 2021 has another structure. The following codes specify the urls. 
host = 'https://orca.casa.ucl.ac.uk'
city = 'London'

# Set up config file with specific patterns for each download year
config = {
    "2025": {"date": "20250615",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2024": {"date": "20240614",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2023": {"date": "2023-09-06", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2022": {"date": "2022-09-10", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2021": {"date": "2021-10",    "pattern": f"{host}/~jreades/data/{city}-{{date}}-listings.csv.gz"}
}

# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'estimated_occupancy_l365d', 'estimated_revenue_l365d', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 'reviews_per_month']

# Set up empty dictionary dfs 
dfs = {}

# Set up a loop to read in the data into the dictionary sequentially
for year, info in config.items():
    date    = info["date"]
    pattern = info["pattern"]
    url     = pattern.format(date=date)

    print("Loading:", year, url)

    # Read only the header (first row)
    header_cols = pd.read_csv(url, nrows=0).columns.tolist()

    # Keep only columns that actually exist in the file (Fix for 'estimated_occupancy_l365d' and 'estimated_revenue_l365d' which are only available for 2025)
    valid_cols = [c for c in cols if c in header_cols]
    
    dfs[year] = pd.read_csv(url, compression='gzip', low_memory=False, usecols= valid_cols)

    # Print shape of the dataframe just loaded
    print(f" → df_{year}: {dfs[year].shape[0]:,} rows × {dfs[year].shape[1]} columns\n")

print("Done!")
```

```{python}
#| lst-label: Cleaning the Airbnb listings data (2021-2025) 
#| echo: false
#| warning: false
#| output: false
# Evaluate the number of rows with NA in id and other columns 
na_summary = {}
for year, df in dfs.items():
    na_summary[year] = df.isna().sum()    

na_table = pd.DataFrame(na_summary).T  
na_table 

# Sensitivity Analysis: Count the number of rows with different thresholds of NA 
row_na_summary = {}
for year, df in dfs.items():

    na_per_row = df.isna().sum(axis=1)

    row_na_summary[year] = {
        "n1":  (na_per_row >= 1).sum(),
        "n2+": (na_per_row >= 2).sum(),
        "n3+": (na_per_row >= 3).sum(),
        "n4+": (na_per_row >= 4).sum(),
        "n5+": (na_per_row >= 5).sum(),
        "n6+": (na_per_row >= 6).sum(),
        "n7+": (na_per_row >= 7).sum(),
        "n8+": (na_per_row >= 8).sum(),
    }

# Convert to a DataFrame
na_summary = pd.DataFrame(row_na_summary).T
na_summary

# The na_summary shows that rows with more than six NAs comprise about 10,000 entries in 2024 and 2025. These are typically rows that do not have any reviews or do not have a price and data on bathrooms and bedrooms. 
```

```{python}
#| lst-label: Cleaning the data to remove NAs appropriately 
#| echo: false
#| warning: false
#| output: false
for year, df in dfs.items():

    # Removing rows with NA in id
    before = df.shape[0]
    df.drop(df[df['id'].isna()].index, inplace=True)

    # Dropping rows missing more than six NA values, which may be problematic - these rows tend to be those without reviews (4 columns) and three additional columns (e.g. bedrooms, bathrooms, price)
    cutoff = 6
    probs = df.isnull().sum(axis=1)
    problematic_rows = df.loc[probs > cutoff]
    df.drop(problematic_rows.index, inplace=True)

    # Print summary for reference
    after = df.shape[0]
    dropped = before - after

    print(f" → df_{year} now contains {after:,} rows. "
          f"(Dropped {dropped:,} rows due to missing data)")

    print("Done!")
```

```{python}
#| lst-label: Assign correct variable types to each column for efficiency in processing 
#| echo: false
#| warning: false
#| output: false
# Columns to convert
bools = ['host_is_superhost']
dates = ['last_scraped','host_since','first_review','last_review']
cats  = ['property_type','room_type']
money = ['price', 'estimated_revenue_l365d']
ints  = ['id','host_id','host_listings_count','host_total_listings_count',
         'accommodates','beds','minimum_nights','maximum_nights',
         'availability_365', 'estimated_occupancy_l365d']

# Loop through the dataframes in dfs for each year and convert columns to specific formats 
for year, df in dfs.items():

    # Boolean columns
    for b in bools:
        if b in df.columns:
            print(f"  Converting {b} → bool")
            df[b] = df[b].replace({'f': False, 't': True}).astype('bool')

    # Date columns
    for d in dates:
        if d in df.columns:
            print(f"  Converting {d} → datetime")
            df[d] = pd.to_datetime(df[d], errors='coerce')

    # Category columns
    for c in cats:
        if c in df.columns:
            print(f"  Converting {c} → category")
            df[c] = df[c].astype('category')

    # Money columns ($ → float)
    for m in money:
        if m in df.columns and df[m].dtype == object:
            print(f"  Cleaning {m} → float")
            df[m] = (
                df[m]
                .str.replace("$", "", regex=False)
                .str.replace(",", "", regex=False)
                .astype("float")
            )

    # Integer-like columns
    for i in ints:
        if i in df.columns:
            print(f"  Converting {i} → integer")
            try:
                df[i] = df[i].astype("float").astype("Int64")
            except ValueError:
                print(" Using UInt16 for compact storage")
                df[i] = df[i].astype("float").astype(pd.UInt16Dtype())

print("\nAll dataframes cleaned successfully!")
```

```{python}
#| lst-label: Load MSOA 2021 geopackage and BORO files from GitHub
#| echo: false
#| warning: false
#| output: false
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/'

BORO = gpd.read_file(f'{host}/{ddir}/LONDON_BOROUGH.gpkg').to_crs(epsg=27700)
MSOA = gpd.read_file(f'{host}/{ddir}/MSOA_DEC_2021_BOUNDARIES.gpkg').to_crs(epsg=27700)

# Clip MSOA Boundaries to those within London Boroughs and only retain Polygons and MultiPolygons
LONDON = gpd.GeoDataFrame(geometry=[BORO.dissolve().geometry.iloc[0]], crs= '27700')
MSOA = gpd.clip(MSOA, LONDON)
MSOA = MSOA[MSOA.geometry.type.isin(["Polygon", "MultiPolygon"])]

# Undertake an inner spatial join to retain the MSOA that fall within Greater London Boundaries 
MSOA = gpd.sjoin(MSOA, BORO[['name', 'ons_inner', 'sub_2011', 'geometry']], how='inner',predicate='intersects')

# Retain columns of interest and rename for better readability 
MSOA = MSOA[['MSOA21CD', 'MSOA21NM', 'geometry', 'name', 'ons_inner', 'sub_2011']].rename(
    columns={
        'MSOA21CD': 'ecode', 
        'MSOA21NM': 'MSOA_name', 
        'name' : 'BORO_name'}).dissolve(by='ecode' , as_index=False)
```

{{< pagebreak >}}

## 1. Is Airbnb ‘out of control’ in London?

The opposition are behind the times: the increase in short-term lets and its potential to damage the housing supply is a concern that the Mayor's office had already raised and taken concrete steps to tackle.

Since 2015, London landlords letting their property for more than 90 days per year have been required to obtain planning permission for a change of use. This planning permission requirement seeks to prevent permanent housing from being converted into de facto holiday accommodation. Airbnb and other equivalent platforms are therefore under far more "control" in London than in many European cities. More recently, the Mayor has proposed both a landlord registration system and a licensing system.

We consider below what the data reveal about

a.  The growth of Airbnbs relative to London housing stock; and

b.  Whether landlords are flouting the planning permission requirement.

#### [How many Airbnbs are there in London?]{.underline}

```{python}
#| lst-label: Key statistics for growth trend in listings 2021-2025
#| echo: false
#| warning: false
#| output: false
listings_2021 = len(dfs["2021"])
listings_2025 = len(dfs["2025"])
listings_pct_change = (listings_2025 - listings_2021) / listings_2021 * 100
listings_2022 = len(dfs["2022"])
listings_2023 = len(dfs["2023"])
listings_pct_change2223 = (listings_2023 - listings_2022) / listings_2022 * 100
```

Since 2021, London has seen a notable increase in Airbnb listings, rising from **`{python} f"{listings_2021:,}"`** to **`{python} f"{listings_2025:,}"`** listings**,** a **`{python} f"{listings_pct_change:.0f}%"`** increase ([Figure 1]{.underline}).

```{python}
#| lst-label: Increase in Airbnb listings by Borough 2021-2025
#| echo: false
#| warning: false
# First, we convert each dataframe in dfs into a gdfs with crs EPSG:27700
gdfs = {}
for year, df in dfs.items():
    gdfs[year] = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs='EPSG:4326'
    ).to_crs(epsg=27700)

# Next, we count the number of listings in each Borough for each year
boro_counts = {}

for year, gdf in gdfs.items():

    # Spatial join: attach Borough name to each point
    join = gpd.sjoin(
        gdf,
        BORO[['name', 'geometry']],
        how='left',
        predicate='within'
    )

    # Group by Borough name from the join
    counts = join.groupby('name_right').size()
    counts = counts.rename('listings')        
    counts.index.name = 'name'                

    # Merge onto Borough GeoDataFrame
    bc = BORO.merge(
        counts,
        left_on='name',
        right_index=True,
        how='left'
    )

    # Replace missing Boroughs with 0 listings
    bc['listings'] = bc['listings'].fillna(0).astype(int)
    boro_counts[year] = bc

# Combine all Borough counts into one table
df_boros = pd.DataFrame({
    year: boro_counts[year].set_index("name")["listings"]
    for year in ["2021", "2022", "2023", "2024", "2025"]
})
```

```{python}
#| lst-label: Generate Figure 1
#| echo: false
#| warning: false
years = ["2021", "2022", "2023", "2024", "2025"]

# Set a consistent colour scale 
vmin = min(boro_counts[y]['listings'].min() for y in years)
vmax = max(boro_counts[y]['listings'].max() for y in years)

# Define the scale for mapping
sm = plt.cm.ScalarMappable(
    cmap="viridis",
    norm=plt.Normalize(vmin=vmin, vmax=vmax)
)
sm._A = []

# Set out the plot for Airbnb listings from 2021–2025
fig, axes = plt.subplots(2, 3, figsize=(8, 5.5), sharex=True, sharey=True)
axes = axes.ravel()

# Plot each map (5 years → fill first 5 axes)
for ax, year in zip(axes[:5], years):
    
    boro_counts[year].plot(
        ax=ax,
        column="listings",
        cmap="viridis",
        edgecolor="black",
        linewidth=0.5,
        vmin=vmin, vmax=vmax,
        legend=False
    )
    
    ax.set_title(
        f"{year}\n{int(boro_counts[year]['listings'].sum()):,} listings",
        fontsize=13,
        pad=5
    )
    ax.axis("off")
    ax.set_aspect("equal")

# Set Colourbar 
axes[5].axis("off")
cax = axes[5].inset_axes([0.15, 0.25, 0.67, 0.10])
cbar = fig.colorbar(
    sm,
    cax=cax,
    orientation="horizontal"
)
cbar.set_label("Number of Listings", fontsize=12)
cbar.ax.tick_params(labelsize=10)    

# Add main title
fig.suptitle(
    f"Figure 1: Airbnb listings increased by {listings_pct_change:.0f}% over the last five years",
    x= 0.01, y = 0.98,
    fontsize=13, 
    fontweight="bold",
    ha="left")

# Caption
fig.text(
    0.01, -0.02,
    "Source: Listings data from Insideairbnb.com, Borough boundaries from data.london.gov.uk",
    ha="left",
    fontsize=11)

# Reduce whitespace between rows 
plt.subplots_adjust(
    top= 0.88,
    bottom= 0.01,
    hspace=0.05,  
    wspace=0.02)

plt.tight_layout()
plt.show()
### ChatGPT5.1 was used to refine the plot layout and the inclusion of the colourbar in the figure, and to size the positioning of the chart.
```

```{python}
#| lst-label: Generate growth rates for top 5 boroughs
#| echo: false
#| warning: false
# Compile data for number of listings in 2021 and 2025 and percentage change in listings 
df_boros["x_2025"] = df_boros["2025"]
df_boros["abs_change"] = df_boros["2025"] - df_boros["2021"]
df_boros["pct_change"] = (df_boros["abs_change"] / df_boros["2021"].replace(0, pd.NA)) * 100

# Select top 5 Boroughs by absolute increase in listings
top5 = df_boros.nlargest(5, "abs_change")

# Extracting top 5 boros for summary 
top5_boros = top5.index.tolist()
top5_abs   = top5["abs_change"].tolist()
top5_pct   = top5["pct_change"].round(0).tolist()
top5_2025  = top5["2025"].tolist()
```

Most of the increase occurred between 2022-2023 **`{python} f"({listings_pct_change2223:.0f}%)"`**, with new listings concentrated in inner London:

-   Specifically, **`{python} f"{top5_boros[0]}"`**, **`{python} f"{top5_boros[1]}"`**, **`{python} f"{top5_boros[2]}"`**, **`{python} f"{top5_boros[3]}"`** and **`{python} f"{top5_boros[4]}"`** saw the highest increase in listings.

-   **`{python} f"{top5_boros[0]}"`** alone added **`{python} f"{top5_abs[0]:,}"`** listings, constituting a **`{python} f"{top5_pct[0]:.0f}%"`** increase within the borough.

```{python}
#| lst-label: Download data from Valuation Office Agency on Housing Stock 
#| echo: false
#| warning: false
#| output: false

# How does this compare to changes in the overall housing stock?
# Set the parameters to retrieve the housing stock data by LSOA from github repo
host = 'https://raw.githubusercontent.com/benjamintee/'
years = [2020, 2021, 2022, 2023, 2024]

df_stock = pd.concat([pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
     for y in years],
    ignore_index=True)

df_stock.info()

# Filter for LA data and ecodes that fall within Greater London 
df_stock_LA = df_stock[
    (df_stock['geography'] == 'LAUA') &
    (df_stock['ecode'].str.startswith('E090'))
]

# Filter and select MSOAs within Greater London 
"""
This block first extracts names of London Boroughs, then filters the df_stock to select only the MSOAs with name strings that contain any of the London Borough names, hence forming a dataframe of property stock by MSOA in Greater London only
"""

#list of London Boroughs
london_boroughs = df_stock_LA.area_name.unique().tolist()

# Build regex pattern — escape special chars and join with |
pattern = "|".join([rf"\b{b.replace(' ', r'\s+')}\b" for b in london_boroughs])

# Filter for MSOA data and ecodes that fall within Greater London 
df_stock_MSOA = df_stock[
    (df_stock['geography'] == 'MSOA') &
    (df_stock['area_name'].str.contains(pattern, case=False, na=False, regex=True))
]
```

```{python}
#| lst-label: Create a stock_table detailing the total number of properties in London each year 
#| echo: false
#| warning: false
stock_table = (
    df_stock_MSOA
    .groupby("Year")["all_properties"]
    .sum()
    .sort_index())

pct_change = stock_table.pct_change() * 100

summary_stock = pd.DataFrame({
    "total_stock": stock_table,
    "pct_change_yoy": pct_change})

stock_2020 = summary_stock.loc[2020, "total_stock"]
stock_2024 = summary_stock.loc[2024, "total_stock"]
stock_growth = (summary_stock.loc[2024, "total_stock"] - summary_stock.loc[2020, "total_stock"]) / summary_stock.loc[2020, "total_stock"]*100
```

```{python}
#| lst-label: Change in stock and listings at the Borough level
#| echo: false
#| warning: false
df_stock_LA_wide = (
    df_stock_LA
    .pivot_table(
        index=["area_name", "ecode"],
        columns="Year",
        values="all_properties")
    .reset_index()).rename(columns={"area_name": "name"})

df_stock_LA_wide["stock_change_2020_2024"] = df_stock_LA_wide[2024] - df_stock_LA_wide[2020]
df_stock_LA_wide["pct_change_2020_2024"] = (df_stock_LA_wide["stock_change_2020_2024"] / df_stock_LA_wide[2020]) * 100

df_listings_LA_wide = df_boros.reset_index().rename(columns={"index": "name", "abs_change": "listings_change"})

df_change_LA = df_stock_LA_wide.merge(
    df_listings_LA_wide,
    on="name",
    how="left")

# Dataframe of change in listings, stock and listing as percentage of housing stock
df_change_LA = df_change_LA[["name", "stock_change_2020_2024", "listings_change"]]
df_change_LA["listings_stock_perc"] = df_change_LA["listings_change"] / df_change_LA["stock_change_2020_2024"]

df_change_LA_sorted = df_change_LA.sort_values("listings_stock_perc", ascending=False)
largest_ratio_name = df_change_LA_sorted.iloc[0]["name"]
largest_ratio_value = round(df_change_LA_sorted.iloc[0]["listings_stock_perc"])
```

In contrast, between 2020 and 2024, London's housing stock grew from **`{python} f"{stock_2020 / 1000000:.2f}M"`** to **`{python} f"{stock_2024 / 1000000:.2f}M"`** properties: a relatively smaller **`{python} f"{stock_growth:.1f}%"`** increase. **`{python} f"{largest_ratio_name}"`** saw **`{python} f"{largest_ratio_value} times"`** more new Airbnb listings than new housing stock.

```{python}
#| lst-label: Plot change in listings as percentage of change in housing stock
#| echo: false
#| warning: false
df_bar = df_change_LA[["name", "listings_stock_perc"]].sort_values(
    "listings_stock_perc", ascending= False)

inner_london = {
    "Camden", "City of London", "Greenwich", "Hackney", "Hammersmith and Fulham",
    "Islington", "Kensington and Chelsea", "Lambeth", "Lewisham",
    "Southwark", "Tower Hamlets", "Wandsworth", "Westminster"}

df_bar["area_type"] = df_bar["name"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London")

color_map = {
    "Inner London": "tomato",
    "Outer London": "skyblue"}

colors = df_bar["area_type"].map(color_map)

fig, ax = plt.subplots(figsize=(8, 5))

ax.bar(
    df_bar["name"],
    df_bar["listings_stock_perc"],
    color=colors)

ax.axhline(
    y=1,
    color="black",
    linestyle="dotted",
    linewidth=1.2,
    alpha=0.8)

# Add label above the dotted line
ax.text(
    x=len(df_bar) - 0.1,   
    y=1.02,                
    s="increase in listings > increease in housing stock",
    ha="right",
    va="bottom",
    fontsize=11,
    color="black")

# Axis labels & title 
ax.set_xticks(range(len(df_bar)))
ax.set_xticklabels(df_bar["name"], rotation= 90, ha="right", fontsize=10)
ax.set_ylabel("Ratio of increase in listings \nto housing stock", fontsize=10)
ax.tick_params(axis="y", labelsize=12)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

# Legend 
handles = [
    plt.Rectangle((0,0),1,1, color="tomato"),
    plt.Rectangle((0,0),1,1, color="skyblue")
]
legend = ax.legend(handles, ["Inner London", "Outer London"], fontsize=12)
legend.get_frame().set_linewidth(0)
legend.get_frame().set_facecolor("none")

# Figure title 
fig.suptitle("Figure 2: Inner London Boroughs saw higher increases in listings \nrelative to housing stock",
    x= 0.02, y = 0.98,
    fontsize=13, 
    fontweight="bold",
    ha="left")

# Figure caption
plt.figtext(
    0.01, -0.10,
    "Source: Listings data from Insideairbnb.com, \nBorough boundaries from data.london.gov.uk, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock in 2025 was unavailable, changes were compared over the last five years",
    ha="left",
    fontsize=11)

plt.tight_layout()
plt.show()
```

#### [What is the situation in 2025?]{.underline}

```{python}
#| lst-label: Aggregation of listings to Boroughs by count
#| echo: false
#| warning: false
gdf_BORO = gpd.sjoin(gdf, BORO[['gss_code', 'geometry']], how='left', predicate='within')
BORO = BORO.merge(gdf_BORO.groupby('gss_code').size().rename('listings2025'), on='gss_code', how='left')
BORO['listings2025'] = BORO['listings2025'].fillna(0).astype(int)

# Filter the total stock data for 2024
df_stock_BORO2025 = df_stock_LA[df_stock_LA['Year'] == 2024]

# Rename BORO columns
BORO = BORO.rename(columns={'gss_code': 'ecode'}).dissolve(by='ecode', as_index=False)

# Merge in the data on the count of all_properties to Borough by gss_code
BORO = BORO.merge(df_stock_BORO2025[['ecode','all_properties']], on='ecode', how='left')
BORO = BORO.rename(columns={'all_properties': 'allprop2024'})

# Count listings as a proportion of total housing stock
BORO['listing_prop2025'] = (BORO['listings2025'] / BORO['allprop2024']) * 100
BORO['listing_prop2025'] = BORO['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)

# Average listing percent of stock across Boroughs
BORO_mean = round(BORO["listing_prop2025"].mean(),2)

# Number of Boroughs exceeding average 
BORO_above_mean_n = (BORO["listing_prop2025"] > BORO_mean).sum()

# Number of Inner London Boroughs greater than mean 
BORO_above_mean = BORO.loc[BORO["listing_prop2025"] > BORO_mean, "name"].tolist()
BORO_inner_above_mean_n = sum(b in inner_london for b in BORO_above_mean)

# Top Two Boros 
BORO_sorted = BORO.sort_values("listing_prop2025", ascending=False)
top1_listingprop_name  = BORO_sorted.iloc[0]["name"]
top1_listingprop_value = BORO_sorted.iloc[0]["listing_prop2025"]
top2_listingprop_name  = BORO_sorted.iloc[1]["name"]
top2_listingprop_value = BORO_sorted.iloc[1]["listing_prop2025"]
```

Across all London Boroughs in 2025, the average number of listings as a percentage of housing stock was **`{python} f"{BORO_mean:.2f}%"`**, though **`{python} f"{BORO_above_mean_n} Boroughs"`** exceeded this average. These **`{python} f"{BORO_above_mean_n} Boroughs"`** were in inner London. Close to **`{python} f"{top1_listingprop_value:.0f}%"`** of total housing stock in **`{python} f"{top1_listingprop_name}",`** and **`{python} f"{top2_listingprop_name}"`** were listed as an Airbnb ([Figure 3]{.underline}). Up to **1 in 8 homes** in some MSOAs in inner London were listed on Airbnb ([Figure 4]{.underline}).

The growth of Airbnb properties therefore far outstrips the rate of growth of new housing available, particularly in inner London boroughs.

In a tight housing market, this exacerbates pressure on housing prices and rents, driving a perception that Airbnb could be “out of control” in parts of London. 

```{python}
#| lst-label: Figure 3 - Visualising the percentage of listings of housing stock in 2025
#| echo: false
#| warning: false
# Prepare the dataframe
df_bar = BORO[["name", "listing_prop2025"]].copy()

# Calculate overall average
overall_mean = df_bar["listing_prop2025"].mean()

# Create Inner/Outer labels
df_bar["area_type"] = df_bar["name"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London")

# Sort by the measure
df_bar = df_bar.sort_values("listing_prop2025", ascending= False)

# Colour map
color_map = {
    "Inner London": "tomato",
    "Outer London": "skyblue"}
colors = df_bar["area_type"].map(color_map)

# Plot the figure 
fig, ax = plt.subplots(figsize=(8, 5))

ax.bar(
    df_bar["name"],
    df_bar["listing_prop2025"],
    color=colors)

# Add average line 
ax.axhline(
    overall_mean,
    color="grey",
    linestyle="--",
    linewidth=1.2,
    label=f"Average: {overall_mean:.2f}%")

# Label average line
ax.text(
    x=len(df_bar) - 0.5,           
    y=overall_mean + overall_mean*0.02,  
    s=f"Average: {overall_mean:.2f}%",
    fontsize=12,
    ha="right",
    va="bottom")

# Axis labels and title
ax.set_xticks(range(len(df_bar)))
ax.set_xticklabels(df_bar["name"], rotation=90, ha="right", fontsize=10)
ax.tick_params(axis="y", labelsize=12)
ax.set_ylabel("Listings as % of Housing Stock", fontsize=12)

ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

# Figure title 
fig.suptitle("Figure 3: Airbnb listings made up a larger proportion of housing stock \nin inner London Boroughs than in outer London Boroughs in 2025",
    x= 0.02, y = 1.00,
    fontsize=13, 
    fontweight="bold",
    ha="left")

# Legend, customise labels and set borders and fill to transparent
handles = [
    plt.Rectangle((0,0),1,1, color="tomato"),
    plt.Rectangle((0,0),1,1, color="skyblue"),
]
legend = ax.legend(handles, ["Inner London", "Outer London"], fontsize=13)
legend.get_frame().set_linewidth(0)
legend.get_frame().set_facecolor("none")

# Caption Text
plt.figtext(
    0.01, -0.05,
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used.",
    ha="left",
    fontsize=11)

plt.tight_layout()
plt.show()
```

```{python}
#| lst-label: Aggregation of listings to MSOA level 
#| echo: false
#| warning: false
# Filter the total stock data for 2024
df_stock_MSOA2025 = df_stock_MSOA[df_stock_MSOA['Year'] == 2024]

# Merge in the data on the count of all_properties to MSOA by ecode
MSOA = MSOA.merge(df_stock_MSOA2025[['ecode', 'all_properties']], on='ecode', how='left')
MSOA = MSOA.rename(columns={'all_properties': 'allprop2024'})

# Count Airbnb listings inside each MSOA polygon
gdf_MSOA = gpd.sjoin(gdfs["2025"], MSOA[['ecode', 'geometry']], how='left', predicate='within')
MSOA = MSOA.merge(gdf_MSOA.groupby('ecode').size().rename('listings2025'), on='ecode', how='left')
MSOA['listings2025'] = MSOA['listings2025'].fillna(0).astype(int)

# Count listings as a proportion of total housing stock
MSOA['listing_prop2025'] = (MSOA['listings2025'] / MSOA['allprop2024']) * 100
MSOA['listing_prop2025'] = MSOA['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)
```

```{python}
#| lst-label: Figure 4 - Airbnb Listings as a Proportion of Housing Stock (MSOA, 2025)", Jenks breaks
#| echo: false
#| warning: false
def MSOA_map_plotter (column:str, Legend_title:str, Map_title:str, Notes:str):

    # Jenks classification
    classifier = mapclassify.NaturalBreaks(MSOA[column], k=5)
    MSOA[f"Jenks_{column}"] = classifier.yb  # 0..k-1

    # Prepare class boundaries for labels
    bin_edges = [round(b, 1) for b in classifier.bins]  
    labels = []
    prev = round(MSOA[column].min(), 1)

    for b in bin_edges:
        labels.append(f"{prev} – {round(b, 1)}")
        prev = round(b, 1)

    # Assign colours
    k = len(bin_edges)
    cmap = plt.cm.viridis
    colors = [cmap(i / (k-1)) for i in range(k)]

    # Create figure
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))

    # Plot each class separately for categorical legend
    for i, color in enumerate(colors):
        MSOA[MSOA[f"Jenks_{column}"] == i].plot(
            ax=ax,
            color=color,
            edgecolor="gray",
            linewidth=0.35)

    # Plot Borough boundaries on top
    BORO.boundary.plot(ax=ax, edgecolor='black', linewidth=1.0)

    # Create custom legend with class boundaries
    legend_handles = [Patch(facecolor=color, edgecolor='gray', label=label)
                      for color, label in zip(colors, labels)]
    legend = ax.legend(
        handles=legend_handles,
        title=Legend_title,
        title_fontsize=12,
        fontsize=12,
        loc="upper left",
        bbox_to_anchor=(0.85,0.4))

    legend.get_frame().set_linewidth(0)
    legend.get_frame().set_facecolor("none")

    # Titles and annotations
    ax.set_title(Map_title,
                 x= 0.01, y = 0.98,
                 fontsize=13, 
                 fontweight="bold",
                 pad=15, 
                 ha="left")
    ax.axis("off")
    fig.text(
        0.01, -0.02,
        Notes,
        ha="left",
        fontsize=11)

    plt.tight_layout()
    plt.show()

# Create Figure 4 to show distribution of listings as proportion of housing stock 
MSOA_map_plotter (
    'listing_prop2025',
    "Listings as Proportion\n of Housing Stock (%)",
    "Figure 4: At MSOA-level, up to 1 in 8 homes in inner London \nwere listed on Airbnb",
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\nNote: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used.")
```

#### [Are hosts flouting the regulations?]{.underline}

```{python}
#| lst-label: Compute the number of properties and % in each category in 2025 
#| echo: false
#| warning: false
df_2025 = dfs['2025'] 
type_summary_2025 = (
    df_2025["room_type"]
    .value_counts()
    .to_frame("count")
    .assign(percentage=lambda x: (x["count"] / x["count"].sum() * 100).round(1)))

# Number of Listings that were entire homes/apartments 
entire_homes_2025 = df_2025[df_2025["room_type"] == "Entire home/apt"]
entire_homes_2025_n = len(entire_homes_2025)
entire_homes_2025_per = round(entire_homes_2025_n / len(df_2025) * 100, 0)

# Number of entire homes that had estimated occupancy > 90 days 
entire_homes_2025_90 = entire_homes_2025[entire_homes_2025['estimated_occupancy_l365d'] > 90]
entire_homes_2025_90_n = len(entire_homes_2025_90)
entire_homes_2025_90_per = round(entire_homes_2025_90_n / entire_homes_2025_n * 100, 2)
```

```{python}
#| lst-label: Average percentages of entire homes / apartments let out for more than 90 days by Borough 
#| echo: false
#| warning: false
# Convert `entire_homes_2025` to GeoDataFrame
entire_homes_2025_gdf = gpd.GeoDataFrame(
    entire_homes_2025,
    geometry=gpd.points_from_xy(entire_homes_2025.longitude, entire_homes_2025.latitude),
    crs='EPSG:4326'
    ).to_crs(epsg=27700) 

# Spatially join and include the Borough name 
entire_homes_2025_mapped = gpd.sjoin(entire_homes_2025_gdf, bc[['name', 'gss_code', 'geometry']], how='left', predicate='within')

# Summarise spatial findings 
summary = (
    entire_homes_2025_mapped
    .groupby('name_right')
    .agg(
        total_listings=('estimated_occupancy_l365d', 'count'),
        high_occupancy=('estimated_occupancy_l365d', lambda x: (x > 90).sum())
    )
    .assign(
        pct_high_occupancy=lambda df: (df['high_occupancy'] / df['total_listings'] * 100).round(1)
    )
    .sort_values(by='pct_high_occupancy', ascending=False)
)

top3 = summary.head(3)
top3_names = top3.index.tolist()
top3_pcts = top3['pct_high_occupancy'].tolist()
```

Based on occupancy estimates[^6] ,**`{python} f"{entire_homes_2025_90_n:,} entire homes / apartments"`** were used for 90 or more nights per year ("frequent lets"). This represented **more than 1 in 5 of all entire homes listings**, and **1 in 8 of all listings in London**.

[^6]: Occupancy estimates obtained from Inside Airbnb. First, a review rate is used to compute reviews to estimated bookings. Thereafter, the average length of stay for London was multiplied by the estimated bookings for each listing giving the occupancy rate (out of 365 days). To estimate potential breaches, we look at the subset of listings that are "entire homes / apartments", and the occupancy rate, which estimates how many nights the listing was booked in the last 365 days, providing a useful proxy for annual use, where raw daily calendar data are unavailable.

Again, inner London boroughs showed the highest rates of intense use. **`{python} f"{top3_names[0]}, {top3_names[1]} and {top3_names[2]}"`** had **`{python} f"{top3_pcts[0]:.0f}%, {top3_pcts[1]:.0f}% and {top3_pcts[2]:.0f}%"`** of entire home listings as frequent lets respectively.

Notwithstanding the sensitivity of assumptions[^7], we expect this to be a conservative estimate, given the reported likelihood of multiple listings of the same property, which some owners have used to circumvent the 90 day rule[^8].

[^7]: The occupancy model assumed (a) a review rate of 50% and (b) average length of stay of 3.1 days for London. This is conservative given that Airbnb previously cited an average review rate of 78%, and, community forums have cited a range of 33% and 85% for various properties.

[^8]: [Landlords exposed flouting law on Airbnb (BBC, 2025)](https://www.bbc.co.uk/news/articles/cvg96rz9061o)

One action that could be taken in response to the opposition's claims is to investigate how many of these listings are compliant with the planning permission requirement, and step up enforcement action against hosts in breach.

## 2. Taking back "control"?

The opposition proposes that "professional" landlords' properties should be subject to registration and increased council tax. As above, the Mayor's office has already proposed a registration system for landlords, and this should be made clear in any response to the opposition.

The opposition has not defined "professional" landlord nor provided details on how its proposal would be implemented. We attempt to define and scope this to aid a proper response.

#### [What is a professional landlord?]{.underline}

Studies suggest that a company or individual that owns and operates 2 or more entire homes can be classed as a commercial or professional landlord. Such "multi-listing hosts" are significant drivers of increase in rent and of Airbnb's profit in London [@todd2021; @cox2016].

```{python}
#| lst-label: Calculate statistics for hosts 
#| echo: false
#| warning: false
host_sizes = (
    entire_homes_2025
    .groupby("host_id")["id"]
    .count()
    .reset_index(name="listing_count"))

# Merging the total number of entire homes back to the listings dataframe
df = entire_homes_2025.merge(host_sizes, on="host_id", how="left")

# Given the wide and long-tailed distribution, we visualise them in discrete buckets instead
# Using 1, 2, 10, 100, 100+ as discrete categories for a sense of scale 
bins = [0, 1, 2, 10, 100, float("inf")]
labels = ["1", "2", "3-10", "11-100", "100+"]

df["host_size_group"] = pd.cut(
    df["listing_count"],
    bins=bins,
    labels=labels,
    include_lowest=True
)
# Further check for each bucket of listings, how many were let out for 90 or more days
df["occupancy_group"] = df["estimated_occupancy_l365d"].apply(
    lambda x: "<90 days" if x < 90 else "≥90 days")

# Compute % of listings in each bucket
table = df.groupby(["host_size_group", "occupancy_group"]).size().unstack(fill_value=0)
dist = table / table.sum().sum() * 100
labels = ["1", "2", "3-10", "11-100", "100+"]
dist = dist.reindex(labels)

# Computing number of hosts for each category 
host_count = (
    host_sizes
    .assign(host_size_group=pd.cut(
        host_sizes["listing_count"],
        bins=bins,
        labels=labels,
        include_lowest=True))
    .groupby("host_size_group")["host_id"]
    .nunique()
    .reindex(labels))

# Compute the % of hosts in each category 
host_pct = host_count / host_count.sum() * 100

# Computing the numnber of hosts in each group and percentage 
df_only1 = df[df["host_size_group"] == "1"].copy()
df_only1_perc = round(len(df_only1) / len(df) *100, 0)
df_only1_hosts = df_only1["host_id"].nunique()
df_only1_hosts_perc = round(df_only1_hosts / df["host_id"].nunique() * 100, 0)

df_morethan1 = df[df["host_size_group"] != "1"].copy()
df_morethan1_perc = round(len(df_morethan1) / len(df) *100, 0)
df_morethan1_hosts = df_morethan1["host_id"].nunique()
df_morethan1_hosts_perc = round(df_morethan1_hosts / df["host_id"].nunique() * 100, 0)

df_5ormore = df[df["listing_count"] >= 5].copy() #for calculations based on a 5+ property threshold
df_5ormore_perc = round(len(df_5ormore) / len(df) *100)
df_5ormore_hosts = df_morethan1["host_id"].nunique()
df_5ormore_hosts_perc = round(df_5ormore_hosts / df["host_id"].nunique() * 100, 0)

df_morethan100 = df[df["host_size_group"] == "100+"].copy()
df_morethan100_perc = round(len(df_morethan100) / len(df) *100, 0)
df_morethan100_hosts = df_morethan100["host_id"].nunique()
df_morethan100_hosts_perc = round(df_morethan100_hosts / df["host_id"].nunique() * 100, 2)

df_morethan1_90more = df[
    (df["host_size_group"] != "1") &
    (df["occupancy_group"] == "≥90 days")
].copy()
df_morethan1_90more_perc = round(len(df_morethan1_90more) / len(df) * 100, 2)
df_morethan1_90more_hosts = df_morethan1_90more["host_id"].nunique()
df_morethan1_90more_hosts_perc = round(df_morethan1_90more_hosts / df["host_id"].nunique() * 100, 2)

df_only1_90more = df[
    (df["host_size_group"] == "1") &
    (df["occupancy_group"] == "≥90 days")
].copy()
df_only1_90more_perc = round(len(df_only1_90more) / len(df) * 100, 2)
df_only1_90more_hosts = df_only1_90more["host_id"].nunique()
df_only1_90more_hosts_perc = round(df_only1_90more_hosts / df["host_id"].nunique() * 100, 2)
```

The data show that a **small number of hosts are responsible for a disproportionately large number of listings** ([Figure 5)]{.underline}.

-   **`{python} f"{df_only1_hosts:,} ({df_only1_hosts_perc:.0f}%)"`** hosts own [only 1 entire property ("single-listing hosts")]{.underline}, and they are responsible for **`{python} f"{df_only1_perc:.0f}%"`** of listings.

-   **`{python} f"{df_morethan1_hosts:,} ({df_morethan1_hosts_perc:.0f}%)"`** hosts own [two or more entire properties ("multi-listing hosts")]{.underline}, and they disproportionately make up **`{python} f"{df_morethan1_perc:.0f}%"`** of listings.

-   **`{python} f"{df_morethan100_hosts:,} ({df_morethan100_hosts_perc:.2f}%)"`** hosts, [own over 100 entire properties;]{.underline} between them, they own **`{python} f"{df_morethan100_perc:.0f}%"`** of listings.

-   Notably, a larger proportion of properties belonging to multi-listings hosts were let out for 90 or more days ("frequent lets") **`{python} f"({df_morethan1_90more_perc:.0f}%)"`**, compared to properties belonging to single-listing hosts **`{python} f"({df_only1_90more_perc:.0f}%)"`**.

```{python}
#| lst-label: Figure 5 - Plot two panel figure with bar chart and bubble plot for hosts
#| echo: false
#| warning: false
fig, (ax1, ax2) = plt.subplots(
    ncols=2,
    figsize=(8, 5),
    gridspec_kw={'width_ratios': [4, 1]})

colors = ["skyblue", "tomato"]

# Horizontal bar chart (Left) on axis 1
ax1.barh(dist.index, dist["<90 days"], color=colors[0], label="< 90 days")
ax1.barh(dist.index, dist["≥90 days"],
         left=dist["<90 days"], color=colors[1], label="≥ 90 days")

# Add % labels
for i, (low, high) in enumerate(zip(dist["<90 days"], dist["≥90 days"])):
    ax1.text(low / 2, i, f"{low:.0f}%", va="center", ha="center", fontsize=12)
    ax1.text(low + high / 2, i, f"{high:.0f}%", va="center", ha="center", fontsize=12)

# Set labels  
ax1.set_xlabel("Percentage of Entire Home / Apartment Listings (%)", fontsize=12)
ax1.set_ylabel("No. of listings per unique host", fontsize=12)
ax1.set_xlim(0, 50.5)
ax1.tick_params(labelsize=12)
legend1 = ax1.legend(title="Occupancy Category", loc="upper right", fontsize=12)
legend1.get_frame().set_linewidth(0)
legend1.get_frame().set_facecolor("none")

# Bubble chart (Right) on axis 2
y_pos = np.arange(len(dist.index))
bubble_sizes = host_count.values
scaled_sizes = bubble_sizes / bubble_sizes.max() * 1500  

ax2.scatter(
    [0] * len(host_count),   
    y_pos,
    s=scaled_sizes,
    alpha=0.6,
    color="gray",
    edgecolor="black")

# Format bubble axis to match primary y-axis
ax2.set_yticks(y_pos)
ax2.set_yticklabels([])     # hide labels on bubble axis
ax2.set_xticks([])
ax2.set_xlim(-0.2, 0.6)
ax2.set_ylim(ax1.get_ylim())   # ensures identical vertical span
ax2.set_title("No. of Hosts", fontsize=12, pad=10)

# Add numeric labels beside bubbles
for y, (count, pct) in zip(y_pos, zip(host_count.values, host_pct.values)):
    ax2.text(
        0.22,               
        y,
        f"{count:,}\n({pct:.0f}%)",   
        va="center",
        ha="left",
        fontsize=11)

# Include a horizontal separator between "1" and "2" (single vs multi hosts) 
separator_y = 0.5

# Draw the separator on both panels
ax1.axhline(separator_y, color="black", linewidth=1, linestyle="--", alpha=0.8)
ax2.axhline(separator_y, color="black", linewidth=1, linestyle="--", alpha=0.8)

# Figure title 
fig.suptitle("Figure 5: A small number of hosts were responsible for a disporportionately \nlarge number of listings",
    x= 0.02, y = 0.95,
    fontsize=13, 
    fontweight="bold",
    ha="left")

# Figure Caption
fig.text(
    0.02, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.subplots_adjust(wspace=0.03)
plt.show()
```

Therefore, using the two-property threshold, **`{python} f"{df_morethan1_hosts_perc:.0f}%"`** of Airbnb hosts currently letting entire homes in London would be treated as "professional" landlords.

We test the robustness of this analysis in a London context by: 

-   Establishing the income earned from renting out entire homes on Airbnb. 

-   Comparing this with annual median income of Londoners[^9] ,£34,100, on the basis that a host earning this much could, in theory, treat this as their sole employment.

[^9]: The median income for London taxpayers was £34,100 [(London Datastore, 2023)](https://data.london.gov.uk/dataset/average-income-of-tax-payers-borough-2g1nq/)

```{python}
#| lst-label: Computing median earnings for single and multiple listing hosts over the last 365 days 
#| echo: false
#| warning: false
#| output: false
# Setting conditions for dataframe filters
single = df["host_size_group"] == "1"
multi  = df["host_size_group"] != "1"
less90   = df["occupancy_group"] == "<90 days"
more90   = df["occupancy_group"] == "≥90 days"

# Compute number of rows with 0 estimated revenue or NaN 
no_revenue = df["estimated_revenue_l365d"].isna() | (df["estimated_revenue_l365d"] == 0)
no_revenue_count = no_revenue.sum()
print(f"After excluding rows with 0 or NaN revenue: {no_revenue_count:,}")

# Median income for single listing hosts 
median_income_single_90d = round(df.loc[single & less90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)
median_income_single_90dmore = round(df.loc[single & more90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)

# median income for multiple listing hosts 
median_income_multi_90d = round(df.loc[multi & less90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)
median_income_multi_90dmore = round(df.loc[multi & more90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)

print(f"{median_income_single_90d:,.0f}", 
      f"{median_income_single_90dmore:,.0f}",
      f"{median_income_multi_90d:,.0f}",
      f"{median_income_multi_90dmore:,.0f}")
```

```{python}
#| echo: false
#| warning: false
#| output: false
# How many properties does a multi-listing host need to have to earn comparably to the median working adult? 
multi_listing_host_90d = 34100 / median_income_multi_90d  # 7 properties

# How does this change if the property was frequently rented out? 
multi_listing_host_90dmore = 34100 / median_income_multi_90dmore  # just 1 property
```

```{python}
#| lst-label: Computing median earnings for private room hosts over the last 365 days 
#| echo: false
#| warning: false
#| output: false
# Number of Listings that were private rooms 
private_rooms_2025 = df_2025[df_2025["room_type"] == "Private room"]
private_rooms_2025_n = len(private_rooms_2025)
private_rooms_2025_per = round(private_rooms_2025_n / len(df_2025) * 100, 0)

# Number of private rooms that had estimated occupancy > 90 days 
private_rooms_2025_90 = private_rooms_2025[private_rooms_2025['estimated_occupancy_l365d'] > 90]
private_rooms_2025_90_n = len(private_rooms_2025_90)
private_rooms_2025_90_per = round(private_rooms_2025_90_n / private_rooms_2025_n * 100, 2)

# Count the number of private rooms held by unique hosts
host_sizes_rooms = (
    private_rooms_2025
    .groupby("host_id")["id"]
    .count()
    .reset_index(name="listing_count"))

# Merging the total number of entire homes back to the listings dataframe
private_rooms_2025_df = private_rooms_2025.merge(host_sizes_rooms, on="host_id", how="left")

# Given the wide and long-tailed distribution, we visualise them in discrete buckets instead
# Using 1, 2, 10, 100, 100+ as discrete categories for a sense of scale 
bins = [0, 1, 2, 10, 100, float("inf")]
labels = ["1", "2", "3-10", "11-100", "100+"]

private_rooms_2025_df["host_size_group"] = pd.cut(
    private_rooms_2025_df["listing_count"],
    bins=bins,
    labels=labels,
    include_lowest=True)

# Further check for each bucket of listings, how many were let out for 90 or more days
private_rooms_2025_df["occupancy_group"] = private_rooms_2025_df["estimated_occupancy_l365d"].apply(
    lambda x: "<90 days" if x < 90 else "≥90 days")

# Total number of private room listings
total_private_rooms = len(private_rooms_2025_df)

# Count per host_size_group
host_group_counts_room = (
    private_rooms_2025_df["host_size_group"]
    .value_counts()
    .sort_index()
    .to_frame("count"))

# Add percentage column
host_group_counts_room["percentage"] = (
    host_group_counts_room["count"] / total_private_rooms * 100
).round(2)

# Filter to relevant rows (non-zero, non-null revenue)
private_rooms_clean = private_rooms_2025_df[
    (private_rooms_2025_df["estimated_revenue_l365d"] > 0) &
    private_rooms_2025_df["estimated_revenue_l365d"].notna()
]

home_private_types = {
    "Private room in home",
    "Private room in condo",
    "Private room in rental unit",
    "Private room in serviced apartment",
    "Private room in tiny home",
    "Private room in townhouse"
}

# Distinguish between private rooms in homes vs private rooms in commercial arrangements 
private_rooms_2025_df["private_room_category"] = private_rooms_2025_df["property_type"].apply(
    lambda x: "Private room (home)" if x in home_private_types else "Private room (commercial)"
)

# Count listings & compute median revenue by property_type
private_rooms_summary = (
    private_rooms_2025_df
    .groupby("private_room_category")
    .agg(
        count=("id", "count"),
        median_standard=("estimated_revenue_l365d",
                     lambda s: s[private_rooms_2025_df.loc[s.index, "occupancy_group"] == "<90 days"]
                     .replace(0, np.nan)
                     .median()),
        median_frequent=("estimated_revenue_l365d",
                     lambda s: s[private_rooms_2025_df.loc[s.index, "occupancy_group"] == "≥90 days"]
                     .replace(0, np.nan)
                     .median())
    )
)

median_home_standard = private_rooms_summary.loc["Private room (home)", "median_standard"]
median_home_frequent = private_rooms_summary.loc["Private room (home)", "median_frequent"]
```

Based on estimated revenues[^10],

[^10]: Estimated revenues obtained from Inside Airbnb, which is computed as a function of price and occupancy estimates. We focus on the median which provides a more accurate measure of central tendency for revenues, given the skew created by luxury homes.

-   The median single-listing host renting out their home for "standard lets" earned **£`{python} f"{median_income_single_90d:,.0f}"`** a year per property, while the median multi-listing host earned **£`{python} f"{median_income_multi_90d:,.0f}"`**.

-   The median single-listing "frequent lets" host earned **£`{python} f"{median_income_multi_90d:,.0f}"`**a year per property, while multi-listing "frequent lets" hosts earned **`{python} f"{median_income_multi_90dmore:,.0f}"`**.

```{python}
#| lst-label: Figure 6 - Distribution of revenue per property for standard lets and frequent lets hosts
#| echo: false
#| warning: false
#| output: false
# Define helper function to drop NaN and zeros
def clean(series):
    return series[(series > 0) & series.notna()]
    
# Re-ordered subsets
a = clean(df.loc[single & less90, "estimated_revenue_l365d"])   # Single <90
b = clean(df.loc[multi & less90,  "estimated_revenue_l365d"])   # Multi <90
c = clean(df.loc[single & more90, "estimated_revenue_l365d"])   # Single ≥90
d = clean(df.loc[multi & more90,  "estimated_revenue_l365d"])   # Multi ≥90

# Labels in new order
labels = ["Single-listing","Multi-listing","Single-listing","Multi-listing"]
xtick_labels = [
    f"Single-listing\n$(£{median_income_single_90d:,.0f})$",
    f"Multi-listing\n$(£{median_income_multi_90d:,.0f})$",
    f"Single-listing\n$(£{median_income_single_90dmore:,.0f})$",
    f"Multi-listing\n$(£{median_income_multi_90dmore:,.0f})$"
]

# Plot a series of four boxplots 
fig = plt.figure(figsize=(8, 5))

# Boxplot
bp = plt.boxplot(
    [a, b, c, d],
    tick_labels=labels,
    showfliers=False,
    patch_artist=True 
)
# Define colors for each box
colors = ["skyblue", "skyblue", "tomato", "tomato"]

# Apply the colours
for box, color in zip(bp['boxes'], colors):
    box.set_facecolor(color)
    box.set_edgecolor("black")

# Also colour medians, whiskers, and caps to match (optional but nicer)
for i, color in enumerate(colors):
    plt.setp(bp['medians'][i], color='black')
    plt.setp(bp['whiskers'][2*i:2*i+2], color='black')
    plt.setp(bp['caps'][2*i:2*i+2], color='black')

# Thicken median lines in boxplots
for median in bp['medians']:
    median.set_linewidth(2.2)

# Add horizontal dotted line for median household income (£34,100)
median_income = 34100

plt.axhline(
    y=median_income,
    color="black",
    linestyle="dotted",
    linewidth=1.3, 
    zorder=0)

# Add label on the right side of the line
plt.text(
    x= 0.6,              
    y=median_income * 1.05,   
    s="Median household \nincome (£34,100)",
    ha="left",
    va="bottom",
    fontsize=11)

top = ax.get_ylim()[1] * 1.00  

plt.text(1.5, top,
         "Standard Lets (<90 days)",
         ha="center",
         fontsize=12.5, fontweight="bold",
         color="skyblue")

plt.text(3.5, top,
         "Frequent Lets (≥90 days)",
         ha="center",
         fontsize=12.5, fontweight="bold",
         color="tomato")

# Adjust margin at bottom to fit the group labels
plt.subplots_adjust(bottom=0.2)

# Y-axis formatting with thousand separators
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ",")))
ax.set_ylim(0, 100000)

plt.xticks(
    ticks=[1, 2, 3, 4],
    labels=xtick_labels,
    fontsize=12
)
plt.yticks(fontsize=12)
plt.ylabel(" £ per property (Last 365 days)", fontsize = 11)

# Add title 
fig.suptitle("Figure 6: Median annual revenues per property for frequent lets were much \ncloser to median household incomes compared to standard lets",
    x= 0.02, y = 0.96,
    fontsize=13, 
    fontweight="bold",
    ha="left")

ax = plt.gca()
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

# Figure Caption
fig.text(
    0.03, -0.05,
    "Source: Listings from Insideairbnb.com\nNote: Figures in brackets refer to median annual revenue for each group",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()
```

Using the more conservative income from single-listing hosts on standard lets, **£`{python} f"{median_income_single_90d:,.0f}"`**, **`{python} f"owning {math.ceil(multi_listing_host_90d):,} properties"`** becomes the threshold to make more than the London median income.

However, with frequent lets, **`{python} f"owning {math.floor(multi_listing_host_90dmore):,} properties"`** is sufficient to make close to double the median income of London. Just **two properties** can generate nearly double the median income of London. Under this analysis:

-   Categorising any host owning two or more entire properties as a “professional” landlord is a defensible position.

-   The aide at the center of the current controversy would be classed as a "professional" landlord, which the Mayor should be mindful of when responding.

If the opposition’s proposal were implemented using the above definition of "professional" hosts, **`{python} f"{len(df_morethan1):,}"`** properties would be subject to increased council tax, constituting **`{python} f"{df_morethan1_perc:.0f}%"`** of all entire homes listings.

#### [Who are London's "professional" landlords?]{.underline}

```{python}
#| lst-label: Breakdown of profile of hosts (UK / Non UK)
#| echo: false
#| warning: false
#| output: false
# Use 'host_location' column in the dfs to compare whether hosts who own > 1 property are more likely to be based outside the UK. 
# Count the number of (UK/Non-UK) unique hosts with only 1 property 
only1prop_NA = df_only1.loc[df_only1.host_location.isna(), "host_id"].nunique() #5,258
only1prop_UK = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["host_id"].nunique() #20,538
only1prop_notUK = df_only1[~df_only1.host_location.str.contains(r"United Kingdom", regex = False, na=True)]["host_id"].nunique() #1,312
only1prop_UK_per = round(only1prop_UK / (only1prop_UK + only1prop_notUK + only1prop_NA) *100 , 1) 

# Count the number of (UK/Non-UK) unique hosts with more than 1 property 
propmorethan1_NA = df_morethan1.loc[df_morethan1.host_location.isna(), "host_id"].nunique() #1,460
propmorethan1_UK = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["host_id"].nunique() #3,369
propmorethan1_notUK = df_morethan1[~df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=True)]["host_id"].nunique() #137
propmorethan1_UK_per = round(propmorethan1_UK / (propmorethan1_UK + propmorethan1_notUK + propmorethan1_NA) *100 , 1) 

# Count the properties belonging to (UK/Non-UK) hosts with only 1 property
only1prop_NA_n = df_only1.loc[df_only1.host_location.isna(), "id"].nunique() #5,258
only1prop_UK_n = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["id"].nunique() #20,538
only1prop_notUK_n = df_only1[~df_only1.host_location.str.contains(r"United Kingdom", regex = False, na=True)]["id"].nunique() #1,312
only1prop_UK_n_per = round(only1prop_UK_n / (only1prop_UK_n + only1prop_notUK_n + only1prop_NA_n) *100 , 1) 

# Count the properties belonging to (UK/Non-UK) hosts with more than 1 property
propmorethan1_NA_n = df_morethan1.loc[df_morethan1.host_location.isna(), "id"].nunique() #8,875
propmorethan1_UK_n = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["id"].nunique() #20,456
propmorethan1_notUK_n = df_morethan1[~df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=True)]["id"].nunique() #1,115
propmorethan1_UK_n_per = round(propmorethan1_UK_n / (propmorethan1_UK_n + propmorethan1_notUK_n + only1prop_NA_n) *100 , 1) 

# Investigate hosts that own more than 100 properties to establish where they are based, how many properties each of them own and whether it can be established that they are individuals or companies
df_100plus = df[df["host_size_group"] == "100+"].copy()
df_100plus_grouped = df_100plus.groupby(['host_id', 'host_name', 'host_location']).size().reset_index(name = "number_of_properties").sort_values("number_of_properties", ascending=False) 
df_100plus_grouped_len = len(df_100plus_grouped) # 13 hosts
df_100plus_grouped_n = df_100plus_grouped['number_of_properties'].sum() # total of 2,472 properties

# Proportion of hosts that do not provide a host location 
host_NA = df.loc[df.host_location.isna(), "host_id"].nunique() / df["host_id"].nunique() *100
```

Most of both single **`{python} f"({only1prop_UK_per}%)"`** and multi-listing **`{python} f"({propmorethan1_UK_per}%)"`** hosts are UK-based. However, at least one of the hosts who owns over 100 properties is a company based abroad (Dubai). Three others have names suggesting that they are companies.

**`{python} f"({host_NA:.0f}%)"`** of hosts do not provide a host location. This provides strong impetus for the Mayor to engage Airbnb to improve host registration and enforcement.

## 3. Evaluating the opposition's proposal

#### [Is council tax an appropriate additional control?]{.underline}

The viability of the opposition’s proposal relies on generating a fiscally significant income stream to moderate disamenities arising from the high proportion of Airbnb properties in London. They suggest relying on the council tax system.

Council tax applies to a property’s domestic classification, based on its 1991 valuation. All properties in the same band currently pay the same rate of council tax, unless an exemption applies. It is therefore not a mechanism designed to assess current commercial use or income, making it a blunt instrument for targeted market intervention.

The opposition do not explain how the council tax system might be adjusted. Notwithstanding, we note that the recent 2025 Budget outlines plans to modify the council tax system from 2028 to apply a surcharge on luxury properties, using 2026 valuations[^11]. This suggests directional flexibility in the system than historically possible, and further work to “piggyback” on these changes could be explored.

[^11]: [Properties worth more than £2m in England face new mansion tax (BBC, 2025)](https://www.bbc.co.uk/news/articles/ce910z9jd3po)

To illustrate potential revenues from council tax adjustments, we calculated the estimated revenue increase based on a **2% - 10%** **premium** applied to the current council tax Band D rate[^12] for all properties operated by "professional" landlords.

[^12]: Band D was chosen as it sits in the middle of valuation bands and best represents a typical tax without unfairly skewing towards low or high property values. This is aligned with methodology used by councils for statutory tax base computations.

```{python}
#| lst-label: Simulation of revenue increase for council tax
#| echo: false
#| warning: false
#| output: false
# Set the parameters to download and retrieve the council tax file  
import os
# Set Data directory 
# Set Data directory 
datadir = "Data"
output_file = os.path.join(datadir, "counciltax_2025.csv")

# Create Data directory if missing
os.makedirs(datadir, exist_ok=True)

# Check if CSV already exists
if os.path.exists(output_file):
    print("File already exists:", output_file)
else:
    print("Downloading and processing dataset...")
    url = "https://data.london.gov.uk/download/expnl/59cc7c37-da8f-4158-bc47-491c3d167b05/council-tax-bands-borough.xlsx"

    # Download the Excel file
    response = requests.get(url)
    excel_path = os.path.join(datadir, "counciltax_tmp.xlsx")

    with open(excel_path, "wb") as f:
        f.write(response.content)

    # Read the sheet '2025-26'
    council_tax = pd.read_excel(excel_path, sheet_name="2025-26")
    council_tax = council_tax.drop(index=[0,1])

    # Keep and rename columns
    council_tax = council_tax[["Code", "Local authority", "Band D"]].rename(
        columns={
            "Code": "gss_code",
            "Local authority": "Local_Authority",
            "Band D": "Band_D"
        }
    )
  
    # Save the CSV
    council_tax.to_csv(output_file, index=False)
    print("Saved:", output_file)

    # Cleanup temporary Excel file
    os.remove(excel_path)
```

```{python}
#| lst-label: Simulation of revenue increase for council tax (2) 
#| echo: false
#| warning: false
#| output: false
# Add scenarios for a 2%, 5%, 10% increase in council tax for Band D 
council_tax = pd.read_csv(output_file)

council_tax["Band_D_2"] = council_tax["Band_D"] * 0.02
council_tax["Band_D_5"] = council_tax["Band_D"] * 0.05
council_tax["Band_D_10"] = council_tax["Band_D"] * 0.10

# Convert `df_morethan1` to GeoDataFrame
gdf_morethan1 = gpd.GeoDataFrame(
    df_morethan1,
    geometry=gpd.points_from_xy(df_morethan1.longitude, df_morethan1.latitude),
    crs='EPSG:4326'
    ).to_crs(epsg=27700) 

# Spatially join and include the Borough name 
gdf_morethan1 = gpd.sjoin(gdf_morethan1, bc[['name', 'gss_code', 'geometry']], how='left', predicate='within')

# Merge the council rate increases for each boro 
gdf_morethan1_tax = gdf_morethan1.merge(council_tax, on='gss_code', how='left')
gdf_morethan1_tax["area_type"] = gdf_morethan1_tax["name_right"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London"
)

# Compute sums for London-wide
total_2 = gdf_morethan1_tax["Band_D_2"].sum()
total_5 = gdf_morethan1_tax["Band_D_5"].sum()
total_10 = gdf_morethan1_tax["Band_D_10"].sum()

# Compute sums for Inner London only
inner_df = gdf_morethan1_tax[gdf_morethan1_tax["area_type"] == "Inner London"]

inner_2 = inner_df["Band_D_2"].sum()
inner_5 = inner_df["Band_D_5"].sum()
inner_10 = inner_df["Band_D_10"].sum()

# Build the summary table
summary_table = pd.DataFrame({
    "Premium Rate": ["2%", "5%", "10%"],
    "Revenue Increase (London Wide)": [total_2, total_5, total_10],
    "Revenue Increase (Inner London Only)": [inner_2, inner_5, inner_10]
})

cols = ["Revenue Increase (London Wide)", "Revenue Increase (Inner London Only)"]
summary_table[cols] = summary_table[cols].applymap(lambda x: f"£{x/1_000_000:.1f}m")
summary_table = summary_table.reset_index(drop=True)
```

```{python}
#| label: tbl-revenue-simulation
#| tbl-cap: "Modest increase in revenues across council tax increase scenarios"
summary_table
```

Source: London Datastore, Council tax charges and bands by borough\
Note: A conservative range of 2%, 5%, and 10% was chosen for simulation, given the requirement that councils proposing a \>5% increase must hold a successful local referendum.

The opposition proposal could yield between **`{python} f"£{total_2/1_000_000:.1f}m"`** and **`{python} f"£{total_10/1_000_000:.1f}m"`**. Inner London boroughs, the focus of much of this debate, account for **`{python} f"£{inner_2/1_000_000:.1f}m"`** to **`{python} f"£{inner_10/1_000_000:.1f}m"`** of this total.

The spatial concentration of potential revenue could allow the Mayor to justify the policy as one specifically taxing central areas, where commercial exploitation of housing stock is most acute, rather than a blanket tax rise across London ([Figure 8]{.underline}). 

However, the political viability and implementation logistics of this change remain to be determined:

-   A targeted surcharge affecting “professional” Airbnb hosts (in the manner of the Budget’s “Mansion Tax” surcharge) is likely to see far less pushback than a general increase in council tax for all properties.

-   Determining which bands should be affected is not straightforward, as properties of all types and values are available for short-term let on Airbnb and other platforms.

```{python}
#| lst-label: Simulation of revenue increase for council tax (2) 
#| echo: false
#| warning: false
# Plot spatial revenue distribution of increase in council tax under 5% scenario 
scenario = "Band_D_5"

# Aggregate revenue increase by borough
borough_revenue = (
    gdf_morethan1_tax
    .groupby("gss_code")[scenario]
    .sum()
    .reset_index()
    .rename(columns={scenario: "revenue_increase"})
)

# Merge with borough geometry
gdf_map = BORO.merge(borough_revenue, how="left",
                     left_on="ecode", right_on="gss_code")

# Jenks classifier (correct)
classifier = mapclassify.NaturalBreaks(gdf_map["revenue_increase"], k=5)
gdf_map["jenks_class"] = classifier.yb

# Nicely formatted numbers (k, M)
def fmt_num(x):
    if x >= 1_000_000:
        return f"{x/1_000_000:.1f}M"
    elif x >= 1_000:
        return f"{x/1_000:.0f}k"
    else:
        return f"{x:.0f}"

# Build labels
bin_edges = classifier.bins
labels = []
prev = round(gdf_map["revenue_increase"].min(), 1)

for b in bin_edges:
    labels.append(f"{fmt_num(prev)} – {fmt_num(b)}")
    prev = b

# Colours for classes
k = len(labels)
cmap = plt.cm.Reds
class_colors = [cmap(i / (k-1)) for i in range(k)]

# Define boundary for Inner London 
inner_boundary = BORO[BORO["name"].isin(inner_london)]

# Dissolve into a single outline
inner_boundary_dissolved = inner_boundary.dissolve()


# Plot figure 
fig, ax = plt.subplots(figsize=(8, 5))

# Plot each class manually (for discrete legend)
for i, color in enumerate(class_colors):
    gdf_map[gdf_map["jenks_class"] == i].plot(
        ax=ax,
        color=color,
        edgecolor="black",
        linewidth=0.6
    )

# Create custom legend
legend_handles = [
    Patch(facecolor=color, edgecolor='gray', linewidth=0.6, label=label)
    for color, label in zip(class_colors, labels)
]

legend = ax.legend(
    handles=legend_handles,
    title="Revenue Increase (£) - \n5% Scenario",
    title_fontsize=12,
    fontsize=12,
    loc="upper left",
    bbox_to_anchor=(0.85, 0.4)
)

legend.get_frame().set_linewidth(0)
legend.get_frame().set_facecolor("none")

# Title
fig.suptitle(
    "Figure 8: Spatial concentration of revenues allows option for justification\n"
    "of targeted measure for inner London boroughs",
    x=0.02, y=0.95,
    fontsize=13,
    fontweight="bold",
    ha="left"
)

# Plot Inner London boundary for visual focus 
inner_boundary_dissolved.boundary.plot(
    ax=ax,
    edgecolor="black",
    linewidth=3
)

# Remove axes
ax.set_axis_off()

# Caption
fig.text(
    0.03, 0,
    "Source: London Datastore, Council Tax by Borough and Bands \nNote: Inner London area is shown with thickened outline",
    ha="left", fontsize=11
)

plt.tight_layout()
plt.show()
```

#### [**Can we do better?**]{.underline}

Given that some of the existing Airbnb hosts are companies, and that a great deal more would be required to register as “professional” landlords if some form of the Mayor’s (and opposition’s) proposals were carried out, an alternative that relies on an existing regulatory infrastructure might be [business rates]{.underline}.

-   Business rates are triggered when a property is used for commercial purposes and are designed to identify commercial exploitation that should be liable to taxation.

-   The business rates requirement for short-term lets is that they must be registered once the threshold of, typically, 140 days of availability per year[^13] is reached. However, in practice there is room to improve enforcement, and implement tougher deterrent fines for errant landlords.

-   There is, therefore, a mismatch between the planning permission requirement and the business rates requirement in terms of when registration is required, which should be closed if this avenue were explored.

[^13]: [Business rates for self-catering and holiday let accommodation](https://www.gov.uk/introduction-to-business-rates/self-catering-and-holiday-let-accommodation) (Gov.uk, 2025)

Another option is to introduce a form of “[tourist tax]{.underline}”, a common approach in other countries. The Hackney Citizen estimates that such a measure could raise **£240 million a year** and, if implemented with care, would not necessarily reduce visitor numbers[^14]. This would be more targeted and easily administered, without relying on property valuations or enforcement against reluctant or elusive landlords.

[^14]: [What London ‘tourist tax’ would look like as Sadiq Khan issues verdict (Hackney Citizen, 2025)](https://www.hackneycitizen.co.uk/2025/11/19/london-tourist-tax-sadiq-khan-verdict/)

## 4. Weighing up the way ahead: Pros, cons and social mobility

While the opposition now agrees with the Mayor’s intent to curb the negative impacts of short-term lets in London, their proposal is blunt and scant on detail. It risks undermining London’s thriving tourism industry, which contributes at least **£10 billion a year** to London’s economy and supports around **7% of London’s jobs**[^15], while failing to distinguish between a small number of problematic commercial operators and the many residents who host legitimately. Moreover, London's short letting market is already more tightly regulated than many European equivalents, and we should be mindful of over-correction ([Figure 9]{.underline}).

[^15]: [Supporting London to continue as the world's best destination (Gov.uk, 2025)](https://www.london.gov.uk/who-we-are/what-london-assembly-does/questions-mayor/find-an-answer/supporting-london-continue-worlds-best-destination)

```{python}
#| lst-label: Computing a cities based comparison of current entire-homes ownership 
#| echo: false
#| warning: false
#| output: false
# Cities based comparison 
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/cities/'

# List of cities with inside airbnb data available
cities = ["amsterdam", "athens", "barcelona", "berlin", "budapest", "copenhagen", "florence",
    "lisbon", "london", "paris", "prague", "vienna"]

# Select set of columns needed
cols_select = [
    "id", "name", "host_id", "host_name",
    "latitude", "longitude", "room_type", "price",
    "minimum_nights", "number_of_reviews"]

# Define a function to load the data from github and convert it 
def load_city(city):
    url = f"{host}{ddir}{city}_listings.csv"
    df = pd.read_csv(url, usecols=lambda c: c in cols_select)

    # Remove NAs in id and host_id
    before = len(df)
    df = df.dropna(subset=["id", "host_id"])
    after = len(df)
    print(f"{city}: Removed {before - after} rows with NA id/host_id")
    
    # Convert data types
    cats  = ['room_type']
    money = ['price']
    ints  = ['id','host_id','minimum_nights','number_of_reviews']

    for c in cats:
        if c in df.columns:
            df[c] = df[c].astype('category')

    for m in money:
        if m in df.columns:
            df[m] = (
                df[m]
                .astype(str)
                .str.replace("$", "", regex=False)
                .str.replace(",", "", regex=False)
                .astype(float)
            )

    for i in ints:
        if i in df.columns:
            df[i] = df[i].astype("float").astype("Int64")

    return df

# Define a function to compute the distribution of host_sizes for each city 
def compute_distribution(df):

    # Entire homes only
    entire = df[df["room_type"] == "Entire home/apt"]

    if entire.empty:
        return pd.Series({"1":0, "2":0, "3-10":0, "11-100":0, "100+":0})

    # Count listings per host
    host_sizes = (
        entire.groupby("host_id")["id"]
        .count()
        .reset_index(name="listing_count")
    )

    # Merge
    df2 = entire.merge(host_sizes, on="host_id", how="left")

    # Bucket into host size groups
    bins   = [0, 1, 2, 10, 100, float("inf")]
    labels = ["1", "2", "3-10", "11-100", "100+"]

    df2["host_size_group"] = pd.cut(
        df2["listing_count"], bins=bins, labels=labels, include_lowest=True
    )

    # Compute distribution (%)
    dist = df2["host_size_group"].value_counts(normalize=True) * 100

    # Ensure missing groups appear as 0
    dist = dist.reindex(labels, fill_value=0)

    return dist

# Load all cities into a dictionary and convert to a dataframe 
dist_dict = {}

for city in cities:
    print("Processing:", city)
    df = load_city(city)
    dist = compute_distribution(df)
    dist_dict[city.capitalize()] = dist

dist_df = pd.DataFrame(dist_dict).T
dist_df
```

```{python}
#| lst-label: International Comparison Figure  
#| echo: false
#| warning: false
# Set color palette and groups and zip them together
palette = ["#7ba591", "#ffd45b", "#faa41b", "#f15b4c", "#cc222b"]
groups = ["1", "2", "3-10", "11-100", "100+"]
plt_colors = dict(zip(groups, palette))

# Make a working copy
dist_plot = dist_df.copy()
dist_plot = dist_plot.sort_values(by="1", ascending=False)

# Negative values for single-listing hosts (left side)
dist_plot["1"] = dist_plot["1"] * -1

# Set the plot for a horizontal bar chart 
fig, ax = plt.subplots(figsize=(8, 6))

ypos = range(len(dist_plot))

# Left side horizontal bar 
ax.barh(
    ypos,
    dist_plot["1"],
    color=plt_colors["1"],
    label="1"
)

# Right side horizontal stacked bar 
right_groups = ["2", "3-10", "11-100", "100+"]
left_offset = 0

for g in right_groups:
    ax.barh(
        ypos,
        dist_plot[g],
        left=left_offset,
        color=plt_colors[g],
        label=g
    )
    left_offset += dist_plot[g]

# Set grid to invisible but retain structure
ax.grid(color="none")
for spine in ax.spines.values():
    spine.set_color("none")
    spine.set_linewidth(0.0)

# Customized legend (under title, no border)
handles, labels_found = ax.get_legend_handles_labels()
unique = dict(zip(labels_found, handles))
legend_order = ["1", "2", "3-10", "11-100", "100+"]
legend_labels = ["1", "2", "3–10", "11–100", "100+"]
empty_handle = plt.Line2D([], [], linestyle="none")

ax.legend(
    [empty_handle] + [unique[k] for k in legend_order],
    ["Entire home listings per host:"] + ["1", "2", "3–10", "11–100", "100+"],
    fontsize=12,
    ncol=6,                   
    columnspacing=0.8,
    handletextpad=0.4,
    bbox_to_anchor=(0.5, 0.99),
    loc="lower center",
    frameon=False
)

# City labels on y-axis
ax.set_yticks(list(ypos))
ax.set_yticklabels(dist_plot.index, fontsize=12)
ax.tick_params(axis='y', length=0)

# Customize x-axis labels
ax.set_xlabel("")  # remove original
# Custom labels outside the plotting area (below x-axis)
ax.text(
    -20, -2, "Single-listing hosts",
    ha="right", va="top", fontsize=12
)

ax.text(
    20, -2, "Multi-listing hosts",
    ha="left", va="top", fontsize=12
)

# Thicken the zero line 
ax.axvline(0, color="black", linewidth=2)

# Set labels for total percentages on the left and right 
for i, city in enumerate(dist_plot.index):

    single = abs(dist_plot.loc[city, "1"])
    multi  = dist_plot.loc[city, right_groups].sum()

    # Left-side label
    ax.text(
        -single - 2.5, i,
        f"{single:.0f}",
        ha="right", va="center",
        fontsize=11
    )

    # Right-side label
    ax.text(
        multi + 2.5, i,
        f"{multi:.0f}",
        ha="left", va="center",
        fontsize=11
    )

# Clean X-tick labels (remove negative sign)
ax.tick_params(axis="x", length=0) 
ticks = ax.get_xticks()
ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"{abs(int(x))}%"))

# Expand the x-axis limits to provide spacing for labels 
ax.set_xlim(-105, 100)
x_pos = 105   

# Dotted horizontal line above London
london_index = list(dist_plot.index).index("London")
ax.axhline(london_index + 0.5, color="gray", linestyle=":", linewidth=1.5)

# Y positions: above and below the London divider
y_above = london_index - 2   # midpoint of upper section
y_below = london_index + 4   # midpoint of lower section

# Upper label (looser regulations)
ax.text(
    x_pos, y_above,
    "Generally stricter\nregulations",
    fontsize=11,
    rotation=90,
    rotation_mode="anchor",
    ha="center", va="center"
)

# Lower label (stricter regulations)
ax.text(
    x_pos, y_below,
    "Generally looser\nregulations",
    fontsize=11,
    rotation=90,
    rotation_mode="anchor",
    ha="center", va="center"
)

# Add title 
fig.suptitle("Figure 9: London's regulatory stance on short-term lets is already stricter \nthan many European counterparts and this is reflected in host listings",
    x= 0.02, y = 0.96,
    fontsize=13, 
    fontweight="bold",
    ha="left")

# Figure Caption
fig.text(
    0.03, -0.07,
    "Source: Cities data from Insideairbnb.com. Analysis includes hosts of entire homes/apartments only.\nNote: Barcelona is implementing a ban on short-term rentals from 2026, with the final phase to be \ncompleted by the end of 2028. Likewise, Budapest has committed to a ban from 2026.",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()

## Inspiration for the figure was taken from https://towardsdatascience.com/who-really-owns-the-airbnbs-youre-booking-marketing-perception-vs-data-analytics-reality-94407a32679c/
## ChatGPT was used to refine the plot layout and include the customized labels 
```

For ordinary Londoners, hosting often provides a modest but meaningful supplement to household income, helping them manage rising living costs and maintain financial stability.

Research from 2022 highlights that older women living alone and women with disabilities increasingly rely on hosting as a flexible lifeline in London[^16]. Indeed, for many just renting out a room, earnings are generally modest: the median annual revenue for a private room let under 90 days is **`{python} f"£{median_home_standard:,.0f}"`** rising to around **`{python} f"£{median_home_frequent:,.0f}"`** for frequent lets. 

[^16]: [AirBnB dependency: The women who rely on hosting to survive in London – ReMAP blog \~ Research, Media Arts, Play](https://reflect.ucl.ac.uk/remap-collaborative-blog/2022/06/27/airbnb-dependency-the-women-who-rely-on-hosting-to-survive-in-london/)

Introducing additional administrative complexity and costs through broad council tax increases would be a backward step in social mobility. While better-off “professional” hosts easily absorb the additional burden without necessarily releasing their properties onto the housing market, individual hosts may see the benefit of letting their property diminished to the point of exiting the market, to the detriment of their finances and security.

Our analysis has shown that the **key problems are concentrated among a subset of professional operators in specific boroughs**. A blanket approach such as raising council tax for all would therefore penalise responsible hosts while doing little to address true sources of pressure. **Instead, a targeted strategy, which could combine a tourist tax with a focus on ensuring that professional landlords are registered and paying business rates would be more proportionate, effective, and better aligned** with the Mayor’s aim of reducing harm, without compromising London’s vibrancy or the livelihoods of ordinary residents.  

Some short-term embarrassment stemming from the aide’s circumstances is unavoidable. To maintain public trust, the Mayor should commission an independent investigation, and take disciplinary action if wrongdoing is found. The opposition’s proposal is not a wholly bad development. Given similarities with existing Mayoral initiatives, this presents an opportunity for the Mayor’s office to ride on bipartisan support to implement more progressive tax changes. 

## References