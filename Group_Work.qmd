---
date: last-modified
title: The Pythoneers' Group Project
bibliography: fsds_group_work.bib
csl: harvard-cite-them-right.csl
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, The Pythoneers, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 16 December 2025 (Tuesday)

| Names          | Student Numbers | Contact Email                |
|----------------|-----------------|------------------------------|
| Alice Southall | 25120600        | alice.southall.25\@ucl.ac.uk |
| Benjamin Tee   | 25049107        | benjamin.tee.25\@ucl.ac.uk   |
| Bosco Choi     | 22040212        | bosco.choi.22\@ucl.ac.uk     |
| Owen Hughes    | 25197128        | owen.hughes.25\@ucl.ac.uk    |
| Tong, C.Y      | 25244321        | c.tong.25\@ucl.ac.uk         |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

# Briefing on Airbnb Growth Trends & Policy Implications

## **Executive Summary**

This report analyses Airbnb activity in London over the last five years, with a focus on 2025. Given time-sensitivity amidst the impending election, and a data-poor environment, we utilise appropriate data-driven approaches to provide insight into the following questions:

a.      Is Airbnb out of control in London?

b.      How many professional landlords are there?

c.      How many properties would be affected by the opposition’s proposal?

d.      What are the benefits and trade-offs with the opposition’s proposal?

e.      What are the Mayor's options to reframe the narrative to one about social mobility and housing opportunity?

### Key Findings

*(Insert findings in 2 column table)*

### Recommendations

*(Insert recommendations here)*

{{< pagebreak >}}

## 1. Is Airbnb ‘out of control’ in London?

Opposition critics have suggested that the growth and spread of Airbnbs in London has accelerated, with professional landlords rapidly accumulating properties, flouting rental restrictions to make a profit. The increase of short-term lets and its potential to damage the housing supply in London is a concern that the Mayor's office had raised [previously](https://www.london.gov.uk/mayor-demands-licencing-scheme-prevent-short-term-lets-damaging-housing-supply)[^1], and is one of the reasons that London's [90-day limit](https://www.london.gov.uk/programmes-strategies/housing-and-land/buying-and-owning-home/guidance-short-term-and-holiday-lets-london)[^2] was introduced. Indeed, the Mayor called for a registration system to be put in place as early as [2019](www.london.gov.uk/press-releases/mayoral/registration-system-for-short-term-letting-law)[^3]. This chapter provides a systematic analysis of the latest situation. 

[^1]: \[ @mayoroflondon2023 \]

[^2]: \[ @mayoroflondonn.d.\]

[^3]: \[ @mayoroflondon2019\]

```{python}
#| lst-label: Load common libraries and packages
#| echo: false
#| warning: false
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.lines import Line2D
```

```{python}
#| lst-label: Download the BibTeX and csl files into the same folder as this qmd if it has not been downloaded yet
#| echo: false
#| warning: false

from pathlib import Path
import requests

# Name of the BibTeX and csl files
bib_name = ["fsds_group_work.bib", "harvard-cite-them-right.csl"]

# Folder where the .qmd file lives
qmd_folder = Path(".").resolve()  

# Raw GitHub URL
url_bib = "https://raw.githubusercontent.com/BoscoChoi/CASA_FSDS_Project/refs/heads/main/fsds_group_work.bib"

for bib in bib_name:
    # Full path for the BibTeX/csl file
    bib_path = qmd_folder / bib

    # Download only if missing
    if not bib_path.exists():
        print(f"{bib_name} not found. Downloading to {bib_path} ...")
        response = requests.get(url_bib)
        response.raise_for_status()
        bib_path.write_bytes(response.content)
        print("Download complete!")
    else:
        print(f"{bib_name} already exists at {bib_path}")
```

```{python}
#| lst-label: Download Airbnb listings data
#| echo: false
#| warning: false
#| output: false
# Looking at the file structure in orca, 2024/2025 have one structure, 2022/2023 have one structure and 2021 has another structure. The following codes specify the urls. 
host = 'https://orca.casa.ucl.ac.uk'
city = 'London'

# Set up config file with specific patterns for each download year
config = {
    "2025": {"date": "20250615",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2024": {"date": "20240614",  "pattern": f"{host}/~jreades/data/{{date}}-{city}-listings.csv.gz"},
    "2023": {"date": "2023-09-06", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2022": {"date": "2022-09-10", "pattern": f"{host}/~jreades/data/{{date}}-listings.csv.gz"},
    "2021": {"date": "2021-10",    "pattern": f"{host}/~jreades/data/{city}-{{date}}-listings.csv.gz"}
}

# Subsetting columns of interest 
cols = ['id', 'listing_url', 'last_scraped', 'name', 
    'description', 'host_id', 'host_name', 'host_since', 
    'host_location', 'host_is_superhost', 
    'host_listings_count', 'host_total_listings_count', 
    'host_verifications', 'latitude', 'longitude', 
    'property_type', 'room_type', 'accommodates', 
    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 
    'amenities', 'price', 'minimum_nights', 'maximum_nights', 
    'availability_365', 'estimated_occupancy_l365d', 'estimated_revenue_l365d', 'number_of_reviews', 
    'first_review', 'last_review', 'review_scores_rating', 'reviews_per_month']

# Set up empty dictionary dfs 
dfs = {}

# Set up a loop to read in the data into the dictionary sequentially
for year, info in config.items():
    date    = info["date"]
    pattern = info["pattern"]
    url     = pattern.format(date=date)

    print("Loading:", year, url)

    # Read only the header (first row)
    header_cols = pd.read_csv(url, nrows=0).columns.tolist()

    # Keep only columns that actually exist in the file (Fix for 'estimated_occupancy_l365d' and 'estimated_revenue_l365d' which are only available for 2025)
    valid_cols = [c for c in cols if c in header_cols]
    
    dfs[year] = pd.read_csv(url, compression='gzip', low_memory=False, usecols= valid_cols)

    # Print shape of the dataframe just loaded
    print(f" → df_{year}: {dfs[year].shape[0]:,} rows × {dfs[year].shape[1]} columns\n")

print("Done!")
```

```{python}
#| lst-label: Cleaning the Airbnb listings data (2021-2025) 
#| echo: false
#| warning: false
#| output: false
# Evaluate the number of rows with NA in id and other columns 
na_summary = {}
for year, df in dfs.items():
    na_summary[year] = df.isna().sum()   

na_table = pd.DataFrame(na_summary).T  
na_table 

# Sensitivity Analysis: Count the number of rows with different thresholds of NA 
row_na_summary = {}
for year, df in dfs.items():

    na_per_row = df.isna().sum(axis=1)

    row_na_summary[year] = {
        "n1":  (na_per_row >= 1).sum(),
        "n2+": (na_per_row >= 2).sum(),
        "n3+": (na_per_row >= 3).sum(),
        "n4+": (na_per_row >= 4).sum(),
        "n5+": (na_per_row >= 5).sum(),
        "n6+": (na_per_row >= 6).sum(),
        "n7+": (na_per_row >= 7).sum(),
        "n8+": (na_per_row >= 8).sum(),
    }

# Convert to a DataFrame
na_summary = pd.DataFrame(row_na_summary).T
na_summary

# The na_summary shows that rows with more than six NAs comprise about 10,000 entries in 2024 and 2025. These are typically rows that do not have any reviews or do not have a price and data on bathrooms and bedrooms. 
```

```{python}
#| lst-label: Cleaning the data to remove NAs appropriately 
#| echo: false
#| warning: false
#| output: false
for year, df in dfs.items():

    # Removing rows with NA in id
    before = df.shape[0]
    df.drop(df[df['id'].isna()].index, inplace=True)

    # Dropping rows missing more than six NA values, which may be problematic - these rows tend to be those without reviews (4 columns) and three additional columns (e.g. bedrooms, bathrooms, price)
    cutoff = 6
    probs = df.isnull().sum(axis=1)
    problematic_rows = df.loc[probs > cutoff]
    df.drop(problematic_rows.index, inplace=True)

    # Print summary for reference
    after = df.shape[0]
    dropped = before - after

    print(f" → df_{year} now contains {after:,} rows. "
          f"(Dropped {dropped:,} rows due to missing data)")

    print("Done!")
```

```{python}
#| lst-label: Assign correct variable types to each column for efficiency in processing 
#| echo: false
#| warning: false
#| output: false
# Columns to convert
bools = ['host_is_superhost']
dates = ['last_scraped','host_since','first_review','last_review']
cats  = ['property_type','room_type']
money = ['price', 'estimated_revenue_l365d']
ints  = ['id','host_id','host_listings_count','host_total_listings_count',
         'accommodates','beds','minimum_nights','maximum_nights',
         'availability_365', 'estimated_occupancy_l365d']

# Loop through the dataframes in dfs for each year and convert columns to specific formats 
for year, df in dfs.items():

    # Boolean columns
    for b in bools:
        if b in df.columns:
            print(f"  Converting {b} → bool")
            df[b] = df[b].replace({'f': False, 't': True}).astype('bool')

    # Date columns
    for d in dates:
        if d in df.columns:
            print(f"  Converting {d} → datetime")
            df[d] = pd.to_datetime(df[d], errors='coerce')

    # Category columns
    for c in cats:
        if c in df.columns:
            print(f"  Converting {c} → category")
            df[c] = df[c].astype('category')

    # Money columns ($ → float)
    for m in money:
        if m in df.columns and df[m].dtype == object:
            print(f"  Cleaning {m} → float")
            df[m] = (
                df[m]
                .str.replace("$", "", regex=False)
                .str.replace(",", "", regex=False)
                .astype("float")
            )

    # Integer-like columns
    for i in ints:
        if i in df.columns:
            print(f"  Converting {i} → integer")
            try:
                df[i] = df[i].astype("float").astype("Int64")
            except ValueError:
                print(" Using UInt16 for compact storage")
                df[i] = df[i].astype("float").astype(pd.UInt16Dtype())

print("\nAll dataframes cleaned successfully!")
```

```{python}
#| lst-label: Load MSOA 2021 geopackage and BORO files from GitHub
#| echo: false
#| warning: false
#| output: false
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/'

BORO = gpd.read_file(f'{host}/{ddir}/LONDON_BOROUGH.gpkg').to_crs(epsg=27700)
MSOA = gpd.read_file(f'{host}/{ddir}/MSOA_DEC_2021_BOUNDARIES.gpkg').to_crs(epsg=27700)

# Clip MSOA Boundaries to those within London Boroughs and only retain Polygons and MultiPolygons
LONDON = gpd.GeoDataFrame(geometry=[BORO.dissolve().geometry.iloc[0]], crs= '27700')
MSOA = gpd.clip(MSOA, LONDON)
MSOA = MSOA[MSOA.geometry.type.isin(["Polygon", "MultiPolygon"])]

# Undertake an inner spatial join to retain the MSOA that fall within Greater London Boundaries 
MSOA = gpd.sjoin(MSOA, BORO[['name', 'ons_inner', 'sub_2011', 'geometry']], how='inner',predicate='intersects')

# Retain columns of interest and rename for better readability 
MSOA = MSOA[['MSOA21CD', 'MSOA21NM', 'geometry', 'name', 'ons_inner', 'sub_2011']].rename(
    columns={
        'MSOA21CD': 'ecode', 
        'MSOA21NM': 'MSOA_name', 
        'name' : 'BORO_name'}).dissolve(by='ecode' , as_index=False)
```

```{python}
#| lst-label: Key statistics for growth trend in listings 2021-2025
#| echo: false
#| warning: false
#| output: false
listings_2021 = len(dfs["2021"])
listings_2025 = len(dfs["2025"])
listings_pct_change = (listings_2025 - listings_2021) / listings_2021 * 100
listings_2022 = len(dfs["2022"])
listings_2023 = len(dfs["2023"])
listings_pct_change2223 = (listings_2023 - listings_2022) / listings_2022 * 100
```

```{python}
#| lst-label: Increase in Airbnb listings by Borough 
#| echo: false
#| warning: false
# First, we convert each dataframe in dfs into a gdfs with crs EPSG:27700
gdfs = {}
for year, df in dfs.items():
    gdfs[year] = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs='EPSG:4326'
    ).to_crs(epsg=27700)

# Next, we count the number of listings in each Borough for each year
boro_counts = {}

for year, gdf in gdfs.items():

    # Spatial join: attach Borough name to each point
    join = gpd.sjoin(
        gdf,
        BORO[['name', 'geometry']],
        how='left',
        predicate='within'
    )

    # Group by Borough name from the join
    counts = join.groupby('name_right').size()
    counts = counts.rename('listings')        
    counts.index.name = 'name'                

    # Merge onto Borough GeoDataFrame
    bc = BORO.merge(
        counts,
        left_on='name',
        right_index=True,
        how='left'
    )

    # Replace missing Boroughs with 0 listings
    bc['listings'] = bc['listings'].fillna(0).astype(int)
    boro_counts[year] = bc

# Combine all Borough counts into one table
df_boros = pd.DataFrame({
    year: boro_counts[year].set_index("name")["listings"]
    for year in ["2021", "2022", "2023", "2024", "2025"]
})
df_boros.head()

# Compile data for number of listings in 2021 and 2025 and percentage change in listings 
df_boros["x_2025"] = df_boros["2025"]
df_boros["abs_change"] = df_boros["2025"] - df_boros["2021"]
df_boros["pct_change"] = (df_boros["abs_change"] / df_boros["2021"].replace(0, pd.NA)) * 100

# Select top 5 Boroughs by absolute increase in listings
top5 = df_boros.nlargest(5, "abs_change")

# Extracting top 5 boros for summary 
top5_boros = top5.index.tolist()
top5_abs   = top5["abs_change"].tolist()
top5_pct   = top5["pct_change"].round(0).tolist()
top5_2025  = top5["2025"].tolist()
```

#### 1.1 What is the growth trend in Airbnb listings across London?

-   Since 2021, London has seen a dramatic increase in Airbnb listings[^4] , rising from **`{python} f"{listings_2021:,}"`** to **`{python} f"{listings_2025:,}"` listings,** a **`{python} f"{listings_pct_change:.0f}%"`** increase ([Figure 1.1]{.underline}).

-   The majority of these listings arose between **2022-2023 (↑`{python} f"{listings_pct_change2223:.0f}%"`**) with the most evident concentrations in new listings in central London boroughs.

-   Specifically, **`{python} f"{top5_boros[0]}"`**, **`{python} f"{top5_boros[1]}"`**, **`{python} f"{top5_boros[2]}"`**, **`{python} f"{top5_boros[3]}"`** and **`{python} f"{top5_boros[4]}"`** saw the highest increase in listings. **`{python} f"{top5_boros[0]}"`** alone added **`{python} f"{top5_abs[0]:,}"`** **listings**, a **`{python} f"{top5_pct[0]:.0f}%"`** increase in Airbnb listings within the borough.

[^4]: Data from ([http://insideairbnb.com](http://insideairbnb.com/)) which uses public information on rentals available for booking at specific months in each year. While owners have exercised care with data processing and cleaning, this dataset is the best publicly available estimate of Airbnb listings. We further process the data to remove listings with duplicate IDs, or more than six null fields, which typically reflects the fact that entries have no reviews or incomplete information.  

```{python}
#| lst-label: Figure 1.1 - Airbnb Listings by London Borough (2021–2025)
#| echo: false
#| warning: false
years = ["2021", "2022", "2023", "2024", "2025"]

# Set a consistent colour scale 
vmin = min(boro_counts[y]['listings'].min() for y in years)
vmax = max(boro_counts[y]['listings'].max() for y in years)

# Define the scale for mapping
sm = plt.cm.ScalarMappable(
    cmap="viridis",
    norm=plt.Normalize(vmin=vmin, vmax=vmax)
)
sm._A = []

# Set out the plot for Airbnb listings from 2021–2025
fig, axes = plt.subplots(2, 3, figsize=(8, 5), sharex=True, sharey=True)
axes = axes.ravel()

# Plot each map (5 years → fill first 5 axes)
for ax, year in zip(axes[:5], years):
    
    boro_counts[year].plot(
        ax=ax,
        column="listings",
        cmap="viridis",
        edgecolor="black",
        linewidth=0.5,
        vmin=vmin, vmax=vmax,
        legend=False
    )
    
    ax.set_title(
        f"{year}\n{int(boro_counts[year]['listings'].sum()):,} listings",
        fontsize=13,
        pad=5
    )
    ax.axis("off")
    ax.set_aspect("equal")

# Set Colourbar 
axes[5].axis("off")
cax = axes[5].inset_axes([0.15, 0.25, 0.67, 0.10])
cbar = fig.colorbar(
    sm,
    cax=cax,
    orientation="horizontal"
)
cbar.set_label("Number of Listings", fontsize=12)
cbar.ax.tick_params(labelsize=10)    

# Add main title
fig.suptitle(
    "Figure 1.1: Airbnb Listings by London Borough (2021–2025)",
    fontsize=14,
    y= 0.98)

# Caption
fig.text(
    0.5, -0.02,
    "Source: Listings data from Insideairbnb.com, Borough boundaries from data.london.gov.uk",
    ha="center",
    fontsize=11)

# Reduce whitespace between rows 
plt.subplots_adjust(
    top= 0.88,
    bottom= 0.01,
    hspace=0.05,  
    wspace=0.02)

plt.show()
### ChatGPT5.1 was used to refine the plot layout and the inclusion of the colourbar in the figure, and to size the positioning of the chart. 
```

#### 1.2 How does this compare to changes in the overall housing stock?

```{python}
#| lst-label: Download data from Valuation Office Agency on Housing Stock 
#| echo: false
#| warning: false
#| output: false
# Set the parameters to retrieve the downloaded data from LSOA 
host = 'https://raw.githubusercontent.com/benjamintee/'
years = [2020, 2021, 2022, 2023, 2024]

df_stock = pd.concat(    [pd.read_csv(f'{host}/CASA_FSDS_Project/main/Data/CTSOP1_1_{y}_03_31.csv').assign(Year=y)
     for y in years],
    ignore_index=True)

df_stock.info()

# Filter for LA data and ecodes that fall within Greater London 
df_stock_LA = df_stock[
    (df_stock['geography'] == 'LAUA') &
    (df_stock['ecode'].str.startswith('E090'))
]

# Filter and select MSOAs within Greater London 
"""
This block first extracts names of London Boroughs, then filters the df_stock to select only the MSOAs with name strings that contain any of the London Borough names, hence forming a dataframe of property stock by MSOA in Greater London only
"""

#list of London Boroughs
london_boroughs = df_stock_LA.area_name.unique().tolist()

# Build regex pattern — escape special chars and join with |
pattern = "|".join([rf"\b{b.replace(' ', r'\s+')}\b" for b in london_boroughs])

# Filter for MSOA data and ecodes that fall within Greater London 
df_stock_MSOA = df_stock[
    (df_stock['geography'] == 'MSOA') &
    (df_stock['area_name'].str.contains(pattern, case=False, na=False, regex=True))
]
```

```{python}
#| lst-label: Create a stock_table detailing the total number of properties in London each year 
#| echo: false
#| warning: false
stock_table = (
    df_stock_MSOA
    .groupby("Year")["all_properties"]
    .sum()
    .sort_index())

pct_change = stock_table.pct_change() * 100

summary_stock = pd.DataFrame({
    "total_stock": stock_table,
    "pct_change_yoy": pct_change})

stock_2020 = summary_stock.loc[2020, "total_stock"]
stock_2024 = summary_stock.loc[2024, "total_stock"]
stock_growth = (summary_stock.loc[2024, "total_stock"] - summary_stock.loc[2020, "total_stock"]) / summary_stock.loc[2020, "total_stock"]*100
```

```{python}
#| lst-label: Change in stock and listings at the Borough level
#| echo: false
#| warning: false
df_stock_LA_wide = (
    df_stock_LA
    .pivot_table(
        index=["area_name", "ecode"],
        columns="Year",
        values="all_properties")
    .reset_index()).rename(columns={"area_name": "name"})

df_stock_LA_wide["stock_change_2020_2024"] = df_stock_LA_wide[2024] - df_stock_LA_wide[2020]
df_stock_LA_wide["pct_change_2020_2024"] = (df_stock_LA_wide["stock_change_2020_2024"] / df_stock_LA_wide[2020]) * 100

df_listings_LA_wide = df_boros.reset_index().rename(columns={"index": "name", "abs_change": "listings_change"})

df_change_LA = df_stock_LA_wide.merge(
    df_listings_LA_wide,
    on="name",
    how="left")

# Dataframe of change in listings, stock and listing as percentage of housing stock
df_change_LA = df_change_LA[["name", "stock_change_2020_2024", "listings_change"]]
df_change_LA["listings_stock_perc"] = df_change_LA["listings_change"] / df_change_LA["stock_change_2020_2024"]

df_change_LA_sorted = df_change_LA.sort_values("listings_stock_perc", ascending=False)
largest_ratio_name = df_change_LA_sorted.iloc[0]["name"]
largest_ratio_value = round(df_change_LA_sorted.iloc[0]["listings_stock_perc"])
```

-   In comparison with the **`{python} f"{listings_pct_change:.0f}%"`** **increase** in Airbnb listings, London housing stock increases lag significantly behind. Between 2020 and 2024, properties in London grew from **`{python} f"{stock_2020 / 1000000:.2f}M"`** to **`{python} f"{stock_2024 / 1000000:.2f}M"`** constituting a mere **`{python} f"{stock_growth:.1f}%"`** **increase** in housing stock. Amidst a tight housing market, the faster increase in properties for short-term rental relative to housing stock exacerbates pressures on housing prices and rents [@todd2021]).
-   Unsurprisingly, when comparing the ratio of listings increases and housing stock increases at the Borough level, central London Boroughs display more accelerated rates of listings increases compared to housing stock increases than peripheral Boroughs. In particular, **`{python} f"{largest_ratio_name}"`** saw **`{python} f"{largest_ratio_value} times"`** more new Airbnb listings than new housing stock between 2020 and 2024.

#### 1.3 What is the situation at the Borough- and MSOA-levels in 2025?

```{python}
#| lst-label: Aggregation of listings to Boroughs by count
#| echo: false
#| warning: false
gdf_BORO = gpd.sjoin(gdf, BORO[['gss_code', 'geometry']], how='left', predicate='within')
BORO = BORO.merge(gdf_BORO.groupby('gss_code').size().rename('listings2025'), on='gss_code', how='left')
BORO['listings2025'] = BORO['listings2025'].fillna(0).astype(int)

# Filter the total stock data for 2024
df_stock_BORO2025 = df_stock_LA[df_stock_LA['Year'] == 2024]

# Rename BORO columns
BORO = BORO.rename(columns={'gss_code': 'ecode'}).dissolve(by='ecode', as_index=False)

# Merge in the data on the count of all_properties to Borough by gss_code
BORO = BORO.merge(df_stock_BORO2025[['ecode','all_properties']], on='ecode', how='left')
BORO = BORO.rename(columns={'all_properties': 'allprop2024'})

# Count listings as a proportion of total housing stock
BORO['listing_prop2025'] = (BORO['listings2025'] / BORO['allprop2024']) * 100
BORO['listing_prop2025'] = BORO['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)

# Define Boroughs in Inner London
inner_london = {
    "Camden", "City of London", "Greenwich", "Hackney", "Hammersmith and Fulham",
    "Islington", "Kensington and Chelsea", "Lambeth", "Lewisham",
    "Southwark", "Tower Hamlets", "Wandsworth", "Westminster"}

# Average listing percent of stock across Boroughs
BORO_mean = round(BORO["listing_prop2025"].mean(),2)

# Number of Boroughs exceeding average 
BORO_above_mean_n = (BORO["listing_prop2025"] > BORO_mean).sum()

# Number of Inner London Boroughs greater than mean 
BORO_above_mean = BORO.loc[BORO["listing_prop2025"] > BORO_mean, "name"].tolist()
BORO_inner_above_mean_n = sum(b in inner_london for b in BORO_above_mean)

# Top Two Boros 
BORO_sorted = BORO.sort_values("listing_prop2025", ascending=False)
top1_listingprop_name  = BORO_sorted.iloc[0]["name"]
top1_listingprop_value = BORO_sorted.iloc[0]["listing_prop2025"]
top2_listingprop_name  = BORO_sorted.iloc[1]["name"]
top2_listingprop_value = BORO_sorted.iloc[1]["listing_prop2025"]
```

-   Across London Boroughs in 2025, the average number of listings as a percent total housing stock was **`{python} f"{BORO_mean:.2f}%"`**, though **`{python} f"{BORO_above_mean_n} Boroughs"`** exceed this average. All **`{python} f"{BORO_above_mean_n} Boroughs"`** are located in Central London with **`{python} f"{top1_listingprop_name}"`** and **`{python} f"{top2_listingprop_name}"`** having close to **`{python} f"{top1_listingprop_value:.0f}%"`** of total housing stock listed as an Airbnb (Figure 1.2).

```{python}
#| lst-label: Figure 1.2 - Visualising the percentage of listings of housing stock in 2025
#| echo: false
#| warning: false
# Prepare the dataframe
df_bar = BORO[["name", "listing_prop2025"]].copy()

# Calculate overall average
overall_mean = df_bar["listing_prop2025"].mean()

# Create Inner/Outer labels
df_bar["area_type"] = df_bar["name"].apply(
    lambda x: "Inner London" if x in inner_london else "Outer London")

# Sort by the measure
df_bar = df_bar.sort_values("listing_prop2025", ascending=True)

# Colour map
color_map = {
    "Inner London": "tomato",
    "Outer London": "skyblue"}
colors = df_bar["area_type"].map(color_map)

# Plot the figure 
fig, ax = plt.subplots(figsize=(8, 6))

ax.bar(
    df_bar["name"],
    df_bar["listing_prop2025"],
    color=colors)

# Add average line 
ax.axhline(
    overall_mean,
    color="grey",
    linestyle="--",
    linewidth=1.2,
    label=f"Average: {overall_mean:.2f}%")

# Label average line
ax.text(
    x= -0.1,              
    y=overall_mean + overall_mean*0.02,  
    s=f"Average: {overall_mean:.2f}%",
    fontsize=12,
    ha="left",
    va="bottom")

# Axis labels and title
ax.set_xticks(range(len(df_bar)))
ax.set_xticklabels(df_bar["name"], rotation=90, ha="right", fontsize=12)
ax.tick_params(axis="y", labelsize=12)
ax.set_ylabel("Listings as percentage of Housing Stock (%)", fontsize=12)
ax.set_title(
    "Figure 1.2: Airbnb Listings as a Proportion of Housing Stock by Borough (2025)",
    fontsize=14,
    pad=15)

# Legend
handles = [
    plt.Rectangle((0,0),1,1, color="tomato"),
    plt.Rectangle((0,0),1,1, color="skyblue"),
]
ax.legend(handles, ["Inner London", "Outer London"], fontsize=14)

# Caption Text
plt.figtext(
    0.01, -0.05,
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\n"
    "Note: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used.",
    ha="left",
    fontsize=11)

plt.tight_layout()
plt.show()
```

-   At the MSOA-level, it becomes clear that there are pockets of concentration of listings relative to housing stock in these Boroughs, with **up to 1 in 8 homes (12.5%)** listed for Airbnb ([Figure 1.3]{.underline}). This high proportion could be driving negative sentiments in some parts of London that Airbnb could be out of control.

```{python}
#| lst-label: Aggregation of listings to MSOA level 
#| echo: false
#| warning: false
# Filter the total stock data for 2024
df_stock_MSOA2025 = df_stock_MSOA[df_stock_MSOA['Year'] == 2024]

# Merge in the data on the count of all_properties to MSOA by ecode
MSOA = MSOA.merge(df_stock_MSOA2025[['ecode', 'all_properties']], on='ecode', how='left')
MSOA = MSOA.rename(columns={'all_properties': 'allprop2024'})

# Count Airbnb listings inside each MSOA polygon
gdf_MSOA = gpd.sjoin(gdfs["2025"], MSOA[['ecode', 'geometry']], how='left', predicate='within')
MSOA = MSOA.merge(gdf_MSOA.groupby('ecode').size().rename('listings2025'), on='ecode', how='left')
MSOA['listings2025'] = MSOA['listings2025'].fillna(0).astype(int)

# Count listings as a proportion of total housing stock
MSOA['listing_prop2025'] = (MSOA['listings2025'] / MSOA['allprop2024']) * 100
MSOA['listing_prop2025'] = MSOA['listing_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)
```

```{python}
#| lst-label: Figure 1.3 - Airbnb Listings as a Proportion of Housing Stock (MSOA, 2025)", Jenks breaks
#| echo: false
#| warning: false
import mapclassify
from matplotlib.patches import Patch

def MSOA_map_plotter (column:str, Legend_title:str, Map_title:str, Notes:str):

    # Jenks classification
    classifier = mapclassify.NaturalBreaks(MSOA[column], k=5)
    MSOA[f"Jenks_{column}"] = classifier.yb  # 0..k-1

    # Prepare class boundaries for labels
    bin_edges = [round(b, 1) for b in classifier.bins]  
    labels = []
    prev = round(MSOA[column].min(), 1)

    for b in bin_edges:
        labels.append(f"{prev} – {round(b, 1)}")
        prev = round(b, 1)

    # Assign colours
    k = len(bin_edges)
    cmap = plt.cm.viridis
    colors = [cmap(i / (k-1)) for i in range(k)]

    # Create figure
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))

    # Plot each class separately for categorical legend
    for i, color in enumerate(colors):
        MSOA[MSOA[f"Jenks_{column}"] == i].plot(
            ax=ax,
            color=color,
            edgecolor="gray",
            linewidth=0.35)

    # Plot Borough boundaries on top
    BORO.boundary.plot(ax=ax, edgecolor='black', linewidth=1.0)

    # Create custom legend with class boundaries
    legend_handles = [Patch(facecolor=color, edgecolor='gray', label=label)
                      for color, label in zip(colors, labels)]
    ax.legend(
        handles=legend_handles,
        title=Legend_title,
        title_fontsize=12,
        fontsize=12,
        loc="upper left",
        bbox_to_anchor=(0.85,0.4))

    # Titles and annotations
    ax.set_title(Map_title, fontsize=14, pad=15)
    ax.axis("off")
    fig.text(
        0.03, -0.02,
        Notes,
        ha='left',
        fontsize=11)

    plt.tight_layout()
    plt.show()

# Create Figure 1.3 to show distribution of listings as proportion of housing stock 
MSOA_map_plotter (
    'listing_prop2025',
    "Listings as Proportion\n of Housing Stock (%)",
    "Figure 1.3: Airbnb Listings as a Proportion of Housing Stock (MSOA, 2025)",
    "Source: Listings data from Insideairbnb.com, Housing Stock from UK Valuation Office Agency.\nNote: As housing stock data for 2025 was unavailable, the latest available data for 2024 was used."
)
```

#### 1.4 How many listings potentially breach the current 90-day limit for rentals?

```{python}
#| lst-label: Compute the number of properties and % in each category in 2025 
#| echo: false
#| warning: false
df_2025 = dfs['2025'] 
type_summary_2025 = (
    df_2025["room_type"]
    .value_counts()
    .to_frame("count")
    .assign(percentage=lambda x: (x["count"] / x["count"].sum() * 100).round(1)))

# Number of Listings that were entire homes/apartments 
entire_homes_2025 = df_2025[df_2025["room_type"] == "Entire home/apt"]
entire_homes_2025_n = len(entire_homes_2025)
entire_homes_2025_per = round(entire_homes_2025_n / len(df_2025) * 100, 0)

# Number of entire homes that had estimated occupancy > 90 days 
entire_homes_2025_90 = entire_homes_2025[entire_homes_2025['estimated_occupancy_l365d'] > 90]
entire_homes_2025_90_n = len(entire_homes_2025_90)
entire_homes_2025_90_per = round(entire_homes_2025_90_n / entire_homes_2025_n * 100, 2)
```

```{python}
#| lst-label: Average percentages of entire homes / apartments let out for more than 90 days by Borough 
#| echo: false
#| warning: false
# Convert `entire_homes_2025` to GeoDataFrame
entire_homes_2025_gdf = gpd.GeoDataFrame(
    entire_homes_2025,
    geometry=gpd.points_from_xy(entire_homes_2025.longitude, entire_homes_2025.latitude),
    crs='EPSG:4326'
    ).to_crs(epsg=27700) 

# Spatially join and include the Borough name 
entire_homes_2025_mapped = gpd.sjoin(entire_homes_2025_gdf, bc[['name', 'geometry']], how='left', predicate='within')

# Summarise spatial findings 
summary = (
    entire_homes_2025_mapped
    .groupby('name_right')
    .agg(
        total_listings=('estimated_occupancy_l365d', 'count'),
        high_occupancy=('estimated_occupancy_l365d', lambda x: (x > 90).sum())
    )
    .assign(
        pct_high_occupancy=lambda df: (df['high_occupancy'] / df['total_listings'] * 100).round(1)
    )
    .sort_values(by='pct_high_occupancy', ascending=False)
)

top3 = summary.head(3)
top3_names = top3.index.tolist()
top3_pcts = top3['pct_high_occupancy'].tolist()
```

-   Since 2015, London planning rules state that entire homes / apartments let on platforms such as Airbnb should not be used for short-term letting for more than **90 nights per calendar year**, without planning permission obtained. This prevents permanent housing from being converted into de facto holiday accommodation. Given its negative impact, we thus focused our analysis on entire homes / apartments.
-   Based on occupancy estimates[^5] ,**`{python} f"{entire_homes_2025_90_n:,}"`** entire homes / apartments were used for 90 or more nights per year ("frequent lets"). This represented **more than 2 in 10 of all entire homes listings**, and **1 in 8 of all listings in London**.
-   Spatially, the pattern is consistent with earlier trends, with Inner London Boroughs showing the highest rates of intense use. **`{python} f"{top3_names[0]}, {top3_names[1]} and {top3_names[2]}"`** had **`{python} f"{top3_pcts[0]:.0f}%, {top3_pcts[1]:.0f}% and {top3_pcts[2]:.0f}%"`** of entire home listings on frequent lets.
-   Notwithstanding the sensitivity of assumptions used by Inside Airbnb[^6], we expect this to be a conservative estimate, given reported likelihood of multiple listings of the same property, which some owners have used to circumvent the 90 day rule[^7]. These high numbers, notwithstanding that some may have sought planning permissions, suggest misuse of Airbnbs in inner London, fanning the perception that it is out of control.

[^5]: Occupancy estimates are obtained from Inside Airbnb data. First, a review rate is used to compute reviews to estimated bookings. Thereafter, the average length of stay for London was multiplied by the estimated bookings for each listing giving the occupancy rate (out of 365 days). To estimate potential breaches, we look at the subset of listings that are "entire homes / apartments", and the occupancy rate, which estimates how many nights the listing was booked in the last 365 days, providing a useful proxy for annual use, where raw daily calendar data are unavailable.

[^6]: The occupancy model assumed (a) a review rate of 50% and (b) average length of stay of 3.1 days for London. This is conservative given that Airbnb previously cited an average review rate of 78%, and, community forums have cited a range of 33% and 85% for various properties.

[^7]: BBC journalists found about 1,300 listings had reused identical images - such as the same furniture, rooms and decor - from other supposedly unique listing. <https://www.bbc.co.uk/news/articles/cvg96rz9061o>

```{python}
#| lst-label: Figure 1.4 - Distribution of Estimated Occupancy (Entire Homes/Apt) in 2025
#| echo: false
#| warning: false
data = entire_homes_2025["estimated_occupancy_l365d"]

# Define bins every 30 days up to 271
bins = np.arange(0, 271, 30)
thresholds = [90, 120, 150, 180]

# Summary table data
summary_rows = []
for t in thresholds:
    count_t = (data > t).sum()
    pct_t   = round(count_t / entire_homes_2025_n * 100)
    summary_rows.append([t, count_t, pct_t])

summary_df = pd.DataFrame(summary_rows, columns=["Days", "Count", "% total"])

# Plot histogram with summary table at top 
from matplotlib.ticker import FuncFormatter
fig, ax = plt.subplots(figsize=(8, 5))

counts, edges, patches = ax.hist(
    data,
    bins=bins,
    edgecolor="black",
    color="skyblue",
    alpha=0.8
)

# Colour bins > 90 days
for i, patch in enumerate(patches):
    mid = (edges[i] + edges[i+1]) / 2
    if mid > 90:
        patch.set_facecolor("tomato")

# Add vertical line at 90 days
ax.axvline(90, color="black", linestyle="--", linewidth=1.2)

# Titles and labels
ax.set_title(
    "Figure 1.4: Distribution of Estimated Occupancy (Entire Homes/Apt) in 2025",
    fontsize=14,
    pad=10
)
ax.set_xlabel("Estimated Occupancy (days per year)", fontsize=12)
ax.tick_params(axis="y", labelsize=12)
ax.set_ylabel("Number of Listings - Entire Homes / Apts", fontsize=12)
ax.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"{int(x):,}"))

fig.text(
    0.03, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

# X-axis ticks
ax.set_xticks([30, 60, 90, 120, 150, 180, 210, 240, 270])

# Summary Table 
table = plt.table(
    cellText=summary_df.values,
    colLabels=summary_df.columns,
    loc='upper right',
    cellLoc='center',
    colColours=["lightgray"] * 3,
    bbox=[0.68, 0.45, 0.31, 0.35] 
)
table.set_fontsize(11)
table.scale(1.0, 1.3)

# Legend for coloured bars 
legend_handles = [
    plt.Rectangle((0, 0), 1, 1, color="skyblue", label="<90 days"),
    plt.Rectangle((0, 0), 1, 1, color="tomato", label="≥90 days")
]
ax.legend(handles=legend_handles, fontsize=12, loc="upper right")

# Grid lines
ax.grid(axis='y', linestyle='--', alpha=0.4)

plt.tight_layout()
plt.show()
```

#### 1.5 Taken together, what is the extent of the problem and possible impact on communities?

*\[AS note: just to flag that previous Mayoral press releases may be a good source of high level commentary about the impact, and be quite helpful in the context of 'advising' the Mayor now. e.g. https://www.london.gov.uk/mayor-demands-licencing-scheme-prevent-short-term-lets-damaging-housing-supply\]*

*\[AS note 2: we could perhaps refer to measures taken in other jurisdictions here and the reasons (basically competition for housing) but we should also talk about the positives i.e. money in Londoners' pockets for renting out their spare rooms/tourist income etc. https://www.bbc.co.uk/travel/article/20240701-what-does-a-world-without-airbnb-look-like; https://hbr.org/2024/02/what-does-banning-short-term-rentals-really-accomplish \]*

## **2. How many professional landlords are there?**

#### 2.1 How might we define a professional landlord?

The opposition proposes that "professional" landlords' properties should be subject to increased council tax. Leaving aside the question of how this might be administered, a separate issue is how the opposition would determine which landlords are classed as "professional".

In this report a "professional" landlord is defined as one:

-   whose income from their Airbnb properties, in an economically ideal circumstance, is greater than the London median income. This is on the basis that the meaning of the word "professional" is as a counterpart to "amateur". In this context, "professional" suggests that the landlord could be relying on Airbnb for their living; that is, to provide the income that they would otherwise be earning through employment.

-   whose properties are residential, rather than commercial. This is based on, first, the opposition's reference to Council Tax (which is not paid by commercial landlords; instead they pay business rates) and, second, the context for the opposition's proposal: the letting of three "homes" (including one council-owned property) on Airbnb - the likely argument on the opposition's part being that letting these homes on Airbnb means that they are not available either for private rental (or council tenants) or sale to buyers on the property market.

Our approach is consistent with relevant literature investigating how Airbnb listings affect local rent and property value [@todd2021; @cox2016] which suggest that a professional landlord can be a company or individual that operates [2 or more]{.underline} entire home / apartment listings (i.e. "multi-listing owners"). With this, the next two chapters focus on multi-listing owners, with an emphasis on the extent of "frequent lets" vs "standard lets".

#### 2.2 How many hosts have listings for two or more entire properties (homes / apartments)?

```{python}
#| lst-label: Calculate statistics for hosts 
#| echo: false
#| warning: false
host_sizes = (
    entire_homes_2025
    .groupby("host_id")["id"]
    .count()
    .reset_index(name="listing_count"))

# Merging the total number of entire homes back to the listings dataframe
df = entire_homes_2025.merge(host_sizes, on="host_id", how="left")

# Given the wide and long-tailed distribution, we visualise them in discrete buckets instead
# Using 1, 2, 10, 100, 100+ as discrete categories for a sense of scale 
bins = [0, 1, 2, 10, 100, float("inf")]
labels = ["1", "2", "3-10", "11-100", "100+"]

df["host_size_group"] = pd.cut(
    df["listing_count"],
    bins=bins,
    labels=labels,
    include_lowest=True
)
# Further check for each bucket of listings, how many were let out for 90 or more days
df["occupancy_group"] = df["estimated_occupancy_l365d"].apply(
    lambda x: "<90 days" if x < 90 else "≥90 days")

# Compute % of listings in each bucket
table = df.groupby(["host_size_group", "occupancy_group"]).size().unstack(fill_value=0)
dist = table / table.sum().sum() * 100
labels = ["1", "2", "3-10", "11-100", "100+"]
dist = dist.reindex(labels)

# Computing number of hosts for each category 
host_count = (
    host_sizes
    .assign(host_size_group=pd.cut(
        host_sizes["listing_count"],
        bins=bins,
        labels=labels,
        include_lowest=True))
    .groupby("host_size_group")["host_id"]
    .nunique()
    .reindex(labels))

# Compute the % of hosts in each category 
host_pct = host_count / host_count.sum() * 100

# Computing the numnber of hosts in each group and percentage 
df_only1 = df[df["host_size_group"] == "1"].copy()
df_only1_perc = round(len(df_only1) / len(df) *100, 0)
df_only1_hosts = df_only1["host_id"].nunique()
df_only1_hosts_perc = round(df_only1_hosts / df["host_id"].nunique() * 100, 0)

df_morethan1 = df[df["host_size_group"] != "1"].copy()
df_morethan1_perc = round(len(df_morethan1) / len(df) *100, 0)
df_morethan1_hosts = df_morethan1["host_id"].nunique()
df_morethan1_hosts_perc = round(df_morethan1_hosts / df["host_id"].nunique() * 100, 0)

df_5ormore = df[df["listing_count"] >= 5].copy() #for calculations based on a 5+ property threshold
df_5ormore_perc = round(len(df_5ormore) / len(df) *100)
df_5ormore_hosts = df_morethan1["host_id"].nunique()
df_5ormore_hosts_perc = round(df_5ormore_hosts / df["host_id"].nunique() * 100, 0)

df_morethan100 = df[df["host_size_group"] == "100+"].copy()
df_morethan100_perc = round(len(df_morethan100) / len(df) *100, 0)
df_morethan100_hosts = df_morethan100["host_id"].nunique()
df_morethan100_hosts_perc = round(df_morethan100_hosts / df["host_id"].nunique() * 100, 2)
```

An analysis of host listings[^8] shows that a **small number of hosts are responsible for a disproportionately large number of listings** ([Figure 2.1)]{.underline}. Previous studies have found that such "multi-listing hosts" are significant drivers of increase in rent and Airbnb's profit in London[^9].

[^8]: We assume that hosts either [own]{.underline} the entire home / apartment that they list, or have [written permission]{.underline} from owners to list on Airbnb.

[^9]: Todd et al. and Cox and Slee BT: Need to cite this properly and reference the key finding.

-   **`{python} f"{df_only1_hosts:,} ({df_only1_hosts_perc:.0f}%)"`** hosts own [only 1 entire property (i.e. single-listing hosts)]{.underline}, and they are responsible for **`{python} f"{df_only1_perc:.0f}%"`** of listings.

-   **`{python} f"{df_morethan1_hosts:,} ({df_morethan1_hosts_perc:.0f}%)"`** hosts own [two or more properties (i.e. multi-listing hosts)]{.underline}, and they disproportionately make up **`{python} f"{df_morethan1_perc:.0f}%"`** of listings.

-   A small number, **`{python} f"{df_morethan100_hosts:,} ({df_morethan100_hosts_perc:.2f}%)"`** of hosts, [own over 100 properties]{.underline} but made up **`{python} f"{df_morethan100_perc:.0f}%"`** of listings.

-   Out of the total entire home / apartment listings, a larger proportion of properties belonging to multi-listings hosts were let out for 90 or more days (14%), compared to properties belonging to single-listing hosts (7%). *\[AS note: should we mention again that they may well have obtained planning permission?\]*

```{python}
#| lst-label: Figure 2.1 Plot two panel figure with bar chart and bubble plot for hosts
#| echo: false
#| warning: false
fig, (ax1, ax2) = plt.subplots(
    ncols=2,
    figsize=(8, 5),
    gridspec_kw={'width_ratios': [4, 1]})

colors = ["skyblue", "tomato"]

# Horizontal bar chart (Left) on axis 1
ax1.barh(dist.index, dist["<90 days"], color=colors[0], label="< 90 days")
ax1.barh(dist.index, dist["≥90 days"],
         left=dist["<90 days"], color=colors[1], label="≥ 90 days")

# Add % labels
for i, (low, high) in enumerate(zip(dist["<90 days"], dist["≥90 days"])):
    ax1.text(low / 2, i, f"{low:.0f}%", va="center", ha="center", fontsize=12)
    ax1.text(low + high / 2, i, f"{high:.0f}%", va="center", ha="center", fontsize=12)

# Titles and labels  
fig.suptitle("Figure 2.1 Distribution of Listings by Host Portfolio Size \nand Estimated Occupancy (2025)",
              fontsize=14, y=0.94)
ax1.set_xlabel("Percentage of Entire Home / Apartment Listings (%)", fontsize=12)
ax1.set_ylabel("Number of listings per unique host", fontsize=12)
ax1.set_xlim(0, 50.5)
ax1.legend(title="Occupancy Category", loc="upper right", fontsize=12)
ax1.tick_params(labelsize=12)

# Bubble chart (Right) on axis 2
y_pos = np.arange(len(dist.index))
bubble_sizes = host_count.values
scaled_sizes = bubble_sizes / bubble_sizes.max() * 1500  

ax2.scatter(
    [0] * len(host_count),   
    y_pos,
    s=scaled_sizes,
    alpha=0.6,
    color="gray",
    edgecolor="black")

# Format bubble axis to match primary y-axis
ax2.set_yticks(y_pos)
ax2.set_yticklabels([])     # hide labels on bubble axis
ax2.set_xticks([])
ax2.set_xlim(-0.2, 0.6)
ax2.set_ylim(ax1.get_ylim())   # ensures identical vertical span
ax2.set_title("Number of Hosts", fontsize=12, pad=10)

# Add numeric labels beside bubbles
for y, (count, pct) in zip(y_pos, zip(host_count.values, host_pct.values)):
    ax2.text(
        0.22,               # x-position (adjust for spacing)
        y,
        f"{count:,}\n({pct:.0f}%)",   
        va="center",
        ha="left",
        fontsize=11
    )

# Include a horizontal separator between "1" and "2" (single vs multi hosts) 
separator_y = 0.5

# Draw the separator on both panels
ax1.axhline(separator_y, color="black", linewidth=1, linestyle="--", alpha=0.8)
ax2.axhline(separator_y, color="black", linewidth=1, linestyle="--", alpha=0.8)

# Figure Caption
fig.text(
    0.03, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.subplots_adjust(wspace=0.03)
plt.show()
```

```{python}
#| lst-label: Figure 2.2 - Percentage of entire home listings that are operated by professional hosts 
#| echo: false
#| warning: false

#left join professional host and 90days count to entire_homes_2025_gdf
entire_homes_2025_gdf = (
    entire_homes_2025_gdf
.merge(df[["id","listing_count","host_size_group","occupancy_group"]],on="id",how="left"))

# Count 2025 entire home listings operated by professional hosts inside each MSOA polygon
gdf_entire_prof_25_MSOA = gpd.sjoin(entire_homes_2025_gdf[~ (entire_homes_2025_gdf.host_size_group=="1")], MSOA[['ecode', 'geometry']], how='left', predicate='within')
MSOA = MSOA.merge(gdf_entire_prof_25_MSOA.groupby('ecode').size().rename('prof_entire_listings_2025'), on='ecode', how='left')
MSOA['prof_entire_listings_2025'] = MSOA['prof_entire_listings_2025'].fillna(0).astype(int)

# Count 2025 entire home listings operated by professionals as a proportion of total listings
MSOA['entire_prof_prop2025'] = (MSOA['prof_entire_listings_2025'] / MSOA['listings2025']) * 100
MSOA['entire_prof_prop2025'] = MSOA['entire_prof_prop2025'].replace([np.inf, -np.inf], np.nan).fillna(0)

# Map professionally operated entire home listings / total listings by MSOA (Jenks)
MSOA_map_plotter (
    "entire_prof_prop2025",
    "Proportion of listings that \n are entire homes operated \n by multi-listing owners(%)",
    "Figure 2.2: Proportion of Airbnb Listings that are \n entire homes operated by multi-listing owners (MSOA, 2025)",
    "Source: Listings data from Insideairbnb.com"
)
```

#### 2.3 What is the average income from these properties, and how does it compare with median / average incomes in London?

```{python}
#| lst-label: Computing median earnings for single and multiple listing hosts over the last 365 days 
#| echo: false
#| warning: false
#| output: false
# Setting conditions for dataframe filters
single = df["host_size_group"] == "1"
multi  = df["host_size_group"] != "1"
less90   = df["occupancy_group"] == "<90 days"
more90   = df["occupancy_group"] == "≥90 days"

# Compute number of rows with 0 estimated revenue or NaN 
no_revenue = df["estimated_revenue_l365d"].isna() | (df["estimated_revenue_l365d"] == 0)
no_revenue_count = no_revenue.sum()
print(f"After excluding rows with 0 or NaN revenue: {no_revenue_count:,}")

# Median income for single listing hosts 
median_income_single_90d = round(df.loc[single & less90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)
median_income_single_90dmore = round(df.loc[single & more90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)

# median income for multiple listing hosts 
median_income_multi_90d = round(df.loc[multi & less90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)
median_income_multi_90dmore = round(df.loc[multi & more90 & ~no_revenue, "estimated_revenue_l365d"].median(),0)

print(f"{median_income_single_90d:,.0f}", 
      f"{median_income_single_90dmore:,.0f}",
      f"{median_income_multi_90d:,.0f}",
      f"{median_income_multi_90dmore:,.0f}")
```

```{python}
#| echo: false
#| warning: false
#| output: false
# How many properties does a multi-listing host need to have to earn comparably to the median working adult? 
multi_listing_host_90d = 34100 / median_income_multi_90d  # 7 properties

# How does this change if the property was frequently rented out? 
multi_listing_host_90dmore = 34100 / median_income_multi_90dmore  # just 1 property
```

```{python}
# Number of Listings that were private rooms 
private_rooms_2025 = df_2025[df_2025["room_type"] == "Private room"]
private_rooms_2025_n = len(private_rooms_2025)
private_rooms_2025_per = round(private_rooms_2025_n / len(df_2025) * 100, 0)

# Number of private rooms that had estimated occupancy > 90 days 
private_rooms_2025_90 = private_rooms_2025[private_rooms_2025['estimated_occupancy_l365d'] > 90]
private_rooms_2025_90_n = len(private_rooms_2025_90)
private_rooms_2025_90_per = round(private_rooms_2025_90_n / private_rooms_2025_n * 100, 2)

# Count the number of private rooms held by unique hosts
host_sizes_rooms = (
    private_rooms_2025
    .groupby("host_id")["id"]
    .count()
    .reset_index(name="listing_count"))

# Merging the total number of entire homes back to the listings dataframe
private_rooms_2025_df = private_rooms_2025.merge(host_sizes_rooms, on="host_id", how="left")

# Given the wide and long-tailed distribution, we visualise them in discrete buckets instead
# Using 1, 2, 10, 100, 100+ as discrete categories for a sense of scale 
bins = [0, 1, 2, 10, 100, float("inf")]
labels = ["1", "2", "3-10", "11-100", "100+"]

private_rooms_2025_df["host_size_group"] = pd.cut(
    private_rooms_2025_df["listing_count"],
    bins=bins,
    labels=labels,
    include_lowest=True
)

# Further check for each bucket of listings, how many were let out for 90 or more days
private_rooms_2025_df["occupancy_group"] = private_rooms_2025_df["estimated_occupancy_l365d"].apply(
    lambda x: "<90 days" if x < 90 else "≥90 days")

# Total number of private room listings
total_private_rooms = len(private_rooms_2025_df)

# Count per host_size_group
host_group_counts_room = (
    private_rooms_2025_df["host_size_group"]
    .value_counts()
    .sort_index()
    .to_frame("count")
)

# Add percentage column
host_group_counts_room["percentage"] = (
    host_group_counts_room["count"] / total_private_rooms * 100
).round(2)

# Filter to relevant rows (non-zero, non-null revenue)
private_rooms_clean = private_rooms_2025_df[
    (private_rooms_2025_df["estimated_revenue_l365d"] > 0) &
    private_rooms_2025_df["estimated_revenue_l365d"].notna()
]

# Count listings & compute median revenue by property_type
private_rooms_summary = (
    private_rooms_2025_df
    .groupby("property_type")
    .agg(
        count=("id", "count"),
        median_standard=("estimated_revenue_l365d",
                     lambda s: s[private_rooms_2025_df.loc[s.index, "occupancy_group"] == "<90 days"]
                     .replace(0, np.nan)
                     .median()),
        median_frequent=("estimated_revenue_l365d",
                     lambda s: s[private_rooms_2025_df.loc[s.index, "occupancy_group"] == "≥90 days"]
                     .replace(0, np.nan)
                     .median())
    )
)

private_rooms_summary = private_rooms_summary[
    (private_rooms_summary["median_standard"].notna()) &
    (private_rooms_summary["median_standard"] != 0) & 
    (private_rooms_summary["median_frequent"].notna()) &
    (private_rooms_summary["median_frequent"] != 0) 
]
```

Based on estimated revenues from Inside Airbnb data[^10],

[^10]: Initially, mean income of frequently rented listings and all listings was taken direnctly from the Inside Airbnb. This assumed that our definition of a frequenly listed property (one listed fro \>90 days) aligned with the equivalent definition from Inside Airbnb. Though, the average income per listing was eventualy compiled using the same listing dataset from earlier analyses. This aligns closer with aspects of our proposal.

-   The median single-listing host renting out their home for 'standard lets' (i.e. less than 90 days) earned **£`{python} f"{median_income_single_90d:,.0f}"`**a year per property, while the median multi-listing host earned **£`{python} f"{median_income_multi_90d:,.0f}"`**.

-   This rises much further when we look at 'frequent lets' (i.e. occupancy 90 days or more). The median single-listing host earned **£`{python} f"{median_income_multi_90d:,.0f}"`**a year per property, while multi-listing hosts earned **`{python} f"{median_income_multi_90dmore:,.0f}"`** .

-   Using the more conservative income from single-listing hosts on standard lets, **£`{python} f"{median_income_single_90d:,.0f}"`**, \*\*owning seven properties\*\* becomes the threshold to make more than the London median income. In contrast, \*\*owning two properties\*\* is sufficient to make close to double the median income of London when letting for 90 days or more. Given this range, one can definitively suggest that owning between \*\*2 and 7\*\* listings is sufficient to make Airbnb a source of primary income. This assertion that under ideal circumstances just 2 properties can generate nearly double the median income of London further suggests that a professional landlord and multi-listing host (when considering entire homes and apartments) are synonymous.

```{python}
#| lst-label: Figure 2.3 - Distribution of revenue per property for standard lets and frequent lets hosts
#| echo: false
#| warning: false
#| output: false
# Define helper function to drop NaN and zeros
def clean(series):
    return series[(series > 0) & series.notna()]
    
# Re-ordered subsets
a = clean(df.loc[single & less90, "estimated_revenue_l365d"])   # Single <90
b = clean(df.loc[multi & less90,  "estimated_revenue_l365d"])   # Multi <90
c = clean(df.loc[single & more90, "estimated_revenue_l365d"])   # Single ≥90
d = clean(df.loc[multi & more90,  "estimated_revenue_l365d"])   # Multi ≥90

# Labels in new order
labels = ["Single-listing","Multi-listing","Single-listing","Multi-listing"]

# Prepare data table for medians
table_data = [
    ["Single (Standard)", f"£{median_income_single_90d:,.0f}"],
    ["Multi (Standard)",  f"£{median_income_multi_90d:,.0f}"],
    ["Single (Frequent)", f"£{median_income_single_90dmore:,.0f}"],
    ["Multi (Frequent)",  f"£{median_income_multi_90dmore:,.0f}"]
]

# Plot a series of four boxplots 
fig = plt.figure(figsize=(8, 5))

# Boxplot
bp = plt.boxplot(
    [a, b, c, d],
    labels=labels,
    showfliers=False,
    patch_artist=True 
)
# Define colors for each box
colors = ["skyblue", "skyblue", "tomato", "tomato"]

# Apply the colours
for box, color in zip(bp['boxes'], colors):
    box.set_facecolor(color)
    box.set_edgecolor("black")

# Also colour medians, whiskers, and caps to match (optional but nicer)
for i, color in enumerate(colors):
    plt.setp(bp['medians'][i], color='black')
    plt.setp(bp['whiskers'][2*i:2*i+2], color='black')
    plt.setp(bp['caps'][2*i:2*i+2], color='black')

# Add dotted vertical separator between (b) and (c)
plt.axvline(x=2.5, color="grey", linestyle="--", linewidth=1)

# Add group labels
plt.text(1.5, plt.ylim()[0] - (plt.ylim()[1] * 0.1), "Standard Lets (<90 days)",
         ha="center", va="top", fontsize=12)

plt.text(3.5, plt.ylim()[0] - (plt.ylim()[1] * 0.1), "Frequent Lets (≥90 days)",
         ha="center", va="top", fontsize=12)

# Adjust margin at bottom to fit the group labels
plt.subplots_adjust(bottom=0.2)

# Y-axis formatting with thousand separators
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ",")))

plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.ylabel("Estimated Revenue per property (Last 365 days)", fontsize = 12)
plt.title("Figure 2.3 Distribution of revenue per property by host portfolio size \nand occupancy group (2025)", fontsize = 14)

ax = plt.gca()

# Add table in top-left corner
table = plt.table(
    cellText=table_data,
    colLabels=["Group", "Median"],
    colColours=["lightgray"] * 2,
    colLoc="left",
    cellLoc="left",
    loc="upper left",
    bbox=[0.01, 0.68, 0.42, 0.30]   # (x, y, width, height)
)

# Set column widths to 65%, 35%
for row in range(len(table_data) + 1):  # include header
    table[row, 0].set_width(0.65)
    table[row, 1].set_width(0.35)

# Set font sizes in table
table.auto_set_font_size(False)
table.set_fontsize(11)

# Figure Caption
fig.text(
    0.03, -0.02,
    "Source: Listings from Insideairbnb.com",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()
```

#### 2.4 What is the profile of these hosts?

```{python}
#| lst-label: Breakdown of profile of hosts (UK / Non UK)
#| echo: false
#| warning: false
#| output: false
# Use 'host_location' column in the dfs to compare whether hosts who own > 1 property are more likely to be based outside the UK. 
# Count the countries (UK/Non-UK) of unique hosts with only 1 property 
only1prop_NA = df_only1.loc[df_only1.host_location.isna(), "host_id"].nunique() #5,258
only1prop_UK = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["host_id"].nunique() #20,538
only1prop_notUK = df_only1[~df_only1.host_location.str.contains(r"United Kingdom", regex = False, na=True)]["host_id"].nunique() #1,312
only1prop_UK_per = round(only1prop_UK / (only1prop_UK + only1prop_notUK) *100 , 1) #94.0%

# Count the countries (UK/Non-UK) of unique hosts with more than 1 property 
propmorethan1_NA = df_morethan1.loc[df_morethan1.host_location.isna(), "host_id"].nunique() #1,460
propmorethan1_UK = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["host_id"].nunique() #3,369
propmorethan1_notUK = df_morethan1[~df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=True)]["host_id"].nunique() #137
propmorethan1_UK_per = round(propmorethan1_UK / (propmorethan1_UK + propmorethan1_notUK) *100 , 1) #96.1%

# Count the properties belonging to (UK/Non-UK) hosts with only 1 property
only1prop_NA_n = df_only1.loc[df_only1.host_location.isna(), "id"].nunique() #5,258
only1prop_UK_n = df_only1[df_only1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["id"].nunique() #20,538
only1prop_notUK_n = df_only1[~df_only1.host_location.str.contains(r"United Kingdom", regex = False, na=True)]["id"].nunique() #1,312
only1prop_UK_n_per = round(only1prop_UK_n / (only1prop_UK_n + only1prop_notUK_n) *100 , 1) #94.0%

# Count the properties belonging to (UK/Non-UK) hosts with more than 1 property
propmorethan1_NA_n = df_morethan1.loc[df_morethan1.host_location.isna(), "id"].nunique() #8,875
propmorethan1_UK_n = df_morethan1[df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=False)]["id"].nunique() #20,456
propmorethan1_notUK_n = df_morethan1[~df_morethan1.host_location.str.contains(r'United Kingdom', regex = False, na=True)]["id"].nunique() #1,115
propmorethan1_UK_n_per = round(propmorethan1_UK_n / (propmorethan1_UK_n + propmorethan1_notUK_n) *100 , 1) #94.8%

# Investigate hosts that own more than 100 properties to establish where they are based, how many properties each of them own and whether it can be established that they are individuals or companies
df_100plus = df[df["host_size_group"] == "100+"].copy()
df_100plus_grouped = df_100plus.groupby(['host_id', 'host_name', 'host_location']).size().reset_index(name = "number_of_properties").sort_values("number_of_properties", ascending=False) 
df_100plus_grouped_len = len(df_100plus_grouped) # 13 hosts
df_100plus_grouped_n = df_100plus_grouped['number_of_properties'].sum() # total of 2,472 properties
```

Among hosts whose origin is known, the vast majority are UK-based, suggesting that any drawbacks of increased Aribnb presence in London are locally initiated and perpetuated. Notwithstanding, the host owning the most properties is a company based in Dubai, while three other hosts have names suggesting that they are companies.

This raises potential concerns with the opposition's proposal of raising council tax as these companies may already be properly registered as companies and complying with London business tax. Given the mayor's history of working with Airbnb for several years and Airbnb's largely cooperative history in London, targeting all professional landlords with business taxes may be more consistent with business precedent.

Additionally, a substantial number of listings do not provide a host origin. This potentially contradicts the proposition in the above paragraph that drawbacks of Airbnb presence is locally sourced. Despite this, a host origin does not affect the feasibility of imposing business taxes onto multi-listing hosts as they apply irrespective of host origin.

```{python}
#| echo: false
#| warning: false
# Figure 2.4 Plot a bar charts to visualise this data 
data = {
    "Group": ["Single-listing hosts"] * 3 + ["Multi-listing hosts"] * 3,
    "Country": ["UK", "Non-UK", "Unknown"] * 2,
    "Count": [
        only1prop_UK,
        only1prop_notUK,
        only1prop_NA,
        propmorethan1_UK,
        propmorethan1_notUK,
        propmorethan1_NA
    ]
}
df_country = pd.DataFrame(data)

# Compute percentages within each group
df_country["Percent"] = (
    df_country.groupby("Group")["Count"]
    .transform(lambda x: x / x.sum() * 100)
    .round(0))

# Prepare data
groups = ["Single-listing hosts", "Multi-listing hosts"]
countries = ["UK", "Non-UK", "Unknown"]
colors = {"UK": "#6BAED6", "Non-UK": "#FD8D3C", "Unknown": "#D3D3D3"}

df_percent = df_country.pivot(index="Group", columns="Country", values="Percent").loc[groups]
df_counts  = df_country.pivot(index="Group", columns="Country", values="Count").loc[groups]

# SAMPLE SIZES
group_totals = df_counts.sum(axis=1)

fig, ax = plt.subplots(figsize=(8, 5))

x = np.arange(len(groups)) * 0.7 # bring bars closer together
bar_width = 0.5        # reduce the thickness of the bars for readability 
bottom = np.zeros(len(groups))

for country in countries:
    pct_vals = df_percent[country].values
    count_vals = df_counts[country].values

    # Stacked % bars
    ax.bar(
        x, pct_vals, width=bar_width, bottom=bottom,
        color=colors[country], edgecolor="black", label=country)

    # Set labels inside bar: "XX% (X,XXX)"
    for i, (pct, count) in enumerate(zip(pct_vals, count_vals)):
        if pct > 0:
            ax.text(
                x[i],
                bottom[i] + pct_vals[i] / 2,
                f"{pct:.0f}% ({count:,})",
                ha="center", va="center",
                fontsize=13)

    bottom += pct_vals

# X-axis labels
ax.set_xticks(x)
ax.set_xticklabels(groups, fontsize=13)

# Include total number of hosts in each category at the bottom 
for i, total in enumerate(group_totals):
    ax.text(
        x[i], -8.5,                 
        f"(n = {total:,})",
        ha="center", va="top", fontsize=12)

# Y-axis and title
ax.set_ylabel("Percentage of Hosts (%)", fontsize=12)
ax.set_ylim(0, 103)
fig.suptitle(
    "Figure 2.4: Profile of Single- and Multi-listing Hosts by Country (2025)",
    fontsize=14, y=0.97)

# Legend *outside on the RIGHT*
ax.legend(
    title="Country Category",
    fontsize=12,
    title_fontsize=12,
    bbox_to_anchor=(1.04, 0.5),  
    loc="center left",
    borderaxespad=0)

# Figure Caption
fig.text(
    0.03, -0.08,
    "Source: Listings from Insideairbnb.com \nNote: Above analysis includes hosts of entire homes/apartments only.\nMulti-listing hosts refer to hosts that list more than one entire home/apartment.",
    ha='left',
    fontsize=11)

# Adjust spacing so legend doesn't overlap
plt.subplots_adjust(right=0.80, bottom=0.20, top=0.90)

plt.tight_layout()
plt.show()
```

## **3. How many properties would be affected by the opposition’s proposal?**

## **4. What are the likely pros and cons of the opposition’s proposal (for the Mayor, for residents, and for the city)?**

-   Not straightforward. *\[AS note: However, we note that the recent Budget proposed changes to the Council Tax system in order to introduce the so-called "Mansion Tax"; reviewing the methods used to introduce this new tax may be instructive in determining a mechanism for introducing an Airbnb Council Tax surcharge.\]*

-   Systems-view needed.

    -   Any council tax adjustments will require updating of 1991 values, which would be (i) administratively difficult to implement (i.e. enforcement of registration), (b) have a wide-ranging impact on all residents. In particular, residents whose properties move into a higher value bracket will need to pay more tax (even if they do not rent out properties).

    -   Council tax increases will reduce margins for landlords who may pass on tax increase to renters. Differentiated impact depending on whether they have deeper pockets, others may raise prices or exit the market. Also likely to see more shift towards more frequent lets (i.e. greater than 90 days per year).

        -   Reduction in Airbnb listings. Some near-term increase in housing supply if unprofitable listings are converted to homes for sale. But longer-term likely to see more listings move to frequent lets.

        -   Higher prices of rental listings may see reduction in tourist attractiveness

        -   Overall, London would be seen to be taking a stricter stance on short-term lets, but overall impact of council tax increases is broad-based and sweeping.

        -   Room for more progressive taxation measure targeting business owners, and those who do frequent lets (i.e. \>140 days reduce to 120 or 90 days).

+---------------------------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Stakeholder Group         | Benefits (Pros)                                                                                            | Drawbacks (Cons)                                                                                                                                                 |
+===========================+============================================================================================================+==================================================================================================================================================================+
| Mayor                     | -   Seen to be taking action against errant listings and tackling disamenities from frequent lets.         | -   Politically unfavorable tax increases in longer-term, loss of votes from residents (who do not let out their homes) and commercial landlords                 |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           | -   Politically expedient (i.e. near-term vote share increase) to be seen to be moving on a 'problem area' | -   Difficult to convince council managers to implement system changes to allow for registration of multi-listing owners. May require funding or see gridlock.   |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           |                                                                                                            | -   Limited impact on housing supply increase and could instead see more shifting towards frequent lets.                                                         |
+---------------------------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Residents (Landlords)     | -   For those able to remain in the market, may have an opportunity to raise rent prices.                  | -   Higher taxes for multiple-listing owners.                                                                                                                    |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           |                                                                                                            | -   For those without deeper pockets, may be compelled to increase the occupation frequency of properties.                                                       |
+---------------------------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Residents (Non-Landlords) | -   Reduced disamenities                                                                                   | -   Higher council taxes                                                                                                                                         |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           | -   Some increase in housing supply                                                                        |                                                                                                                                                                  |
+---------------------------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| City of London            | -   Overall curtailing of negative impact of Airbnb in some pockets of London                              | -   Impact on tourism attractiveness from higher rental prices for business / leisure travelers - (a) reduced numbers, (b) reduced stay duration per individual. |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           |                                                                                                            | -   May be perceived to be less business friendly, especially to liberal market/capitalist disruptors like Airbnb.                                               |
|                           |                                                                                                            |                                                                                                                                                                  |
|                           |                                                                                                            | -   Limited impact on housing supply.                                                                                                                            |
+---------------------------+------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+

## **5. Can the story be reframed as a positive one about social mobility or housing opportunity?**

-   Airbnb largely operating legitimately. Supports tourism and encourages local stay. For locals, it helps to provide additional source of income.

-   But there are small number of hosts who are profiting off Airbnb and aren’t compliant. This has seen listings boom in certain areas, with disamenities to some. Additional competition raises housing prices and limiting young people from accessing these flats.

-   Main issue with housing shortage and this government will continue to focus on that instead of mindlessly raising the tax burden for everyone. Advise the mayor to double down on his plans to expand housing supply in brownfield and greyfield areas.

```{python}
# Bonus: Working on a cities based comparison 
host = 'https://raw.githubusercontent.com/benjamintee/'
ddir = 'CASA_FSDS_Project/main/Data/cities/'

# List of cities with inside airbnb data available
cities = ["amsterdam", "athens", "barcelona", "berlin", "budapest", "copenhagen", "florence",
    "lisbon", "london", "paris", "prague", "vienna"]

# Select set of columns needed
cols_select = [
    "id", "name", "host_id", "host_name",
    "latitude", "longitude", "room_type", "price",
    "minimum_nights", "number_of_reviews"]

# Define a function to load the data from github and convert it 
def load_city(city):
    url = f"{host}{ddir}{city}_listings.csv"
    df = pd.read_csv(url, usecols=lambda c: c in cols_select)

    # Remove NAs in id and host_id
    before = len(df)
    df = df.dropna(subset=["id", "host_id"])
    after = len(df)
    print(f"{city}: Removed {before - after} rows with NA id/host_id")
    
    # Convert data types
    cats  = ['room_type']
    money = ['price']
    ints  = ['id','host_id','minimum_nights','number_of_reviews']

    for c in cats:
        if c in df.columns:
            df[c] = df[c].astype('category')

    for m in money:
        if m in df.columns:
            df[m] = (
                df[m]
                .astype(str)
                .str.replace("$", "", regex=False)
                .str.replace(",", "", regex=False)
                .astype(float)
            )

    for i in ints:
        if i in df.columns:
            df[i] = df[i].astype("float").astype("Int64")

    return df

# Define a function to compute the distribution of host_sizes for each city 
def compute_distribution(df):

    # Entire homes only
    entire = df[df["room_type"] == "Entire home/apt"]

    if entire.empty:
        return pd.Series({"1":0, "2":0, "3-10":0, "11-100":0, "100+":0})

    # Count listings per host
    host_sizes = (
        entire.groupby("host_id")["id"]
        .count()
        .reset_index(name="listing_count")
    )

    # Merge
    df2 = entire.merge(host_sizes, on="host_id", how="left")

    # Bucket into host size groups
    bins   = [0, 1, 2, 10, 100, float("inf")]
    labels = ["1", "2", "3-10", "11-100", "100+"]

    df2["host_size_group"] = pd.cut(
        df2["listing_count"], bins=bins, labels=labels, include_lowest=True
    )

    # Compute distribution (%)
    dist = df2["host_size_group"].value_counts(normalize=True) * 100

    # Ensure missing groups appear as 0
    dist = dist.reindex(labels, fill_value=0)

    return dist

# Load all cities into a dictionary and convert to a dataframe 
dist_dict = {}

for city in cities:
    print("Processing:", city)
    df = load_city(city)
    dist = compute_distribution(df)
    dist_dict[city.capitalize()] = dist

dist_df = pd.DataFrame(dist_dict).T
dist_df
```

```{python}
# Set color palette and groups and zip them together
palette = ["#7ba591", "#ffd45b", "#faa41b", "#f15b4c", "#cc222b"]
groups = ["1", "2", "3-10", "11-100", "100+"]
colors = dict(zip(groups, palette))

# Make a working copy
dist_plot = dist_df.copy()
dist_plot = dist_plot.sort_values(by="1", ascending=False)

# Negative values for single-listing hosts (left side)
dist_plot["1"] = dist_plot["1"] * -1

# Set the plot for a horizontal bar chart 
fig, ax = plt.subplots(figsize=(8, 6))

ypos = range(len(dist_plot))

# Left side horizontal bar 
ax.barh(
    ypos,
    dist_plot["1"],
    color=colors["1"],
    label="1"
)

# Right side horizontal stacked bar 
right_groups = ["2", "3-10", "11-100", "100+"]
left_offset = 0

for g in right_groups:
    ax.barh(
        ypos,
        dist_plot[g],
        left=left_offset,
        color=colors[g],
        label=g
    )
    left_offset += dist_plot[g]

# Set grid to invisible but retain structure
ax.grid(color="none")
for spine in ax.spines.values():
    spine.set_color("none")
    spine.set_linewidth(0.0)

# Plot title
plt.title(
    "Figure X.X - % of Listings by Single/Multi-listing hosts in European cities (2025)",
    fontsize=13,
    pad= 32
)

# Customized legend (under title, no border)
handles, labels_found = ax.get_legend_handles_labels()
unique = dict(zip(labels_found, handles))
legend_order = ["1", "2", "3-10", "11-100", "100+"]
legend_labels = ["1", "2", "3–10", "11–100", "100+"]
empty_handle = plt.Line2D([], [], linestyle="none")

ax.legend(
    [empty_handle] + [unique[k] for k in legend_order],
    ["Entire home listings per host:"] + ["1", "2", "3–10", "11–100", "100+"],
    fontsize=12,
    ncol=6,                   
    columnspacing=0.8,
    handletextpad=0.4,
    bbox_to_anchor=(0.5, 0.99),
    loc="lower center",
    frameon=False
)

# City labels on y-axis
ax.set_yticks(list(ypos))
ax.set_yticklabels(dist_plot.index, fontsize=12)
ax.tick_params(axis='y', length=0)

# Customize x-axis labels
ax.set_xlabel("")  # remove original
# Custom labels outside the plotting area (below x-axis)
ax.text(
    -20, -2, "Single-listing hosts",
    ha="right", va="top", fontsize=12
)

ax.text(
    20, -2, "Multi-listing hosts",
    ha="left", va="top", fontsize=12
)

# Thicken the zero line 
ax.axvline(0, color="black", linewidth=2)

# Set labels for total percentages on the left and right 
for i, city in enumerate(dist_plot.index):

    single = abs(dist_plot.loc[city, "1"])
    multi  = dist_plot.loc[city, right_groups].sum()

    # Left-side label
    ax.text(
        -single - 2.5, i,
        f"{single:.0f}",
        ha="right", va="center",
        fontsize=11
    )

    # Right-side label
    ax.text(
        multi + 2.5, i,
        f"{multi:.0f}",
        ha="left", va="center",
        fontsize=11
    )

# CLEAN X-TICK LABELS (remove negative signs)
ax.tick_params(axis="x", length=0) 
ticks = ax.get_xticks()
ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"{abs(int(x))}%"))

# Expand the x-axis limits to provide spacing for labels 
ax.set_xlim(-105, 100)
x_pos = 105   

# Dotted horizontal line above London
london_index = list(dist_plot.index).index("London")
ax.axhline(london_index + 0.5, color="gray", linestyle=":", linewidth=1.5)

# Y positions: above and below the London divider
y_above = london_index - 2   # midpoint of upper section
y_below = london_index + 4   # midpoint of lower section

# Upper label (looser regulations)
ax.text(
    x_pos, y_above,
    "Generally stricter\nregulations",
    fontsize=11,
    rotation=90,
    rotation_mode="anchor",
    ha="center", va="center"
)

# Lower label (stricter regulations)
ax.text(
    x_pos, y_below,
    "Generally looser\nregulations",
    fontsize=11,
    rotation=90,
    rotation_mode="anchor",
    ha="center", va="center"
)

# Figure Caption
fig.text(
    0.03, -0.07,
    "Source: Cities data from Insideairbnb.com. Analysis includes hosts of entire homes/apartments only.\nNote: Barcelona is implementing a ban on short-term rentals from 2026, with the final phase to be \ncompleted by the end of 2028. Likewise, Budapest has committed to a ban from 2026.",
    ha='left',
    fontsize=11)

plt.tight_layout()
plt.show()

## Inspiration for the figure was taken from https://towardsdatascience.com/who-really-owns-the-airbnbs-youre-booking-marketing-perception-vs-data-analytics-reality-94407a32679c/
## ChatGPT was used to refine the plot layout and include the customized labels 
```

## References